[["index.html", "Learning R for Mass Spectrometrists Preface", " Learning R for Mass Spectrometrists Jeff Jones, Heath Patterson, Ryan Benz 2024-06-02 Preface Who are the authors: Jeff Jones (PhD), Senior Scientist Proteomics, Division of Physics, Mathematics and Astronomy, Caltech, California USA Heath Patterson (PhD), Director of Spatial Biology Bioinformatics, Aspect Analytics, Genk, Belgium Ryan Benz (PhD), Director Data Science, Seer Bio, Redwood City, California USA Who is the audience? This book is aimed at absolute beginners in R and programming in general. The topics covered are designed to be straightforward and easy to follow, and by the end of the book, readers should be able to develop analytical processes for their own research, provide means for others to accomplish analyses, and extend their skills with more advanced literature. Although no previous knowledge of R is required, some experience with data and statistical analysis is recommended. Reading and understanding the chapters and exercises should provide the skills necessary for basic data analysis and prepare readers for more advanced concepts and skills. What Is Covered? The purpose of this book is to provide you with a comprehensive guide to the R programming language, as well as to teach you how to use RStudio, tidy data, the tidyverse ecosystem, and ggplot2. By the end of this book, you will have a solid foundation in R and the skills necessary to conduct data analysis and visualization. Our goals for this book are: Learn the fundamentals of the R programming language, including variables, data types, functions, and control structures. You will also learn how to write and execute basic R scripts. Learn to use the RStudio integrated development environment (IDE), including how to navigate the interface, create projects, and install packages. Learn about tidy data: what it is, why it’s important for data analysis, and how to use the tidyr package to transform data into a tidy format. Learn the basics of the tidyverse ecosystem of R packages, including dplyr, tidyr, and ggplot2, and how they can streamline the data analysis process. Learn to create data visualizations using the ggplot2 R package, including how to customize plots, add themes and colors, and create complex visualizations. What Is Not Covered? This book is designed to provide you with a comprehensive introduction to R programming. While we cover a variety of essential topics, we don’t cover everything. For instance, we do not go into depth on statistical analysis, probability, regression, machine learning, or any other advanced analytical topics. We also do not cover constructing R packages, documentation, markdown, or any other advanced R programming topic. Why create another book on R? This book serves as the foundation for the Introduction to R course at the annual conference for the American Society for Mass Spectrometry (ASMS). It’s a permanent, expandable, and revisable reference document created by the authors. It evolved from a presentation used as instructional material in the “Getting Started with R” short course. This tome provides greater depth of coverage on various topics and records instructors’ nuanced approach to R. The book is a living document, improved based on feedback. When was this book developed? The initiative that resulted in the creation of the R Book started in 2017, when a group of experts in the field came together to offer a series of workshops at the annual ASMS conference. These workshops were aimed at teaching attendees how to effectively use R, a programming language used in statistical computing and graphics. Year after year, the workshops drew a significant number of attendees, with between 200 and 300 people participating for three consecutive years. Building on the success of the workshops, the presenters decided to offer a more formal short course in 2020, which was a remote year due to the COVID-19 pandemic. The course proved to be very popular and was well-received by participants for 2020 (Virtual), 2021 (Philadelphia, PA), and 2022 (Minneapolis, MN). Following the positive response to the short course, in 2023, it was decided to convert all of the teaching materials and resources used in the workshops and course into a formal book. This book, the R Book, was created with the aim of providing a comprehensive and accessible guide to using R, and has since become a valuable resource for students, researchers, and professionals alike. Where can the book be accessed? The book is available online here. Additionally, this book and it’s contents are covered at the annual American Society for Mass Spectrometry (ASMS), typically the first week in June the weekend prior to the scientific meeting. "],["acknowledgements.html", "Acknowledgements", " Acknowledgements A special thanks to the American Society of Mass Spectrometry for allowing us to provide an annual basic R introductory course. "],["contents.html", "Contents", " Contents This book is organized in a manner such that each chapter builds upon the previous. There are three main sections to this book, where the first section covers the introduction to R and the Integrated Development Environment (IDE) called R Studio. The middle section is the most extensive, basically covering the R programming language its nuances and the main data science packages that will cover the majority of your day-to-day analyses. These chapters all have example exercises to work on that should help sharpen your skills. The last section covers mass spectrometry specific data and some packages you can use to work with that data. This section on mass spectrometry is not meant to be comprehensive, but rather a sample showcase for what is possible. "],["introduction-to-r.html", "Chapter 1 Introduction to R Why choose it? What you can do with it? The R Learning Curve Learning to code 1.1 Alternatives 1.2 Resources", " Chapter 1 Introduction to R Before we get started, this book contains some basic cues to help facilitate your understanding of the current topic. At the end of this chapter you should be able to Understand why R is a good choice for data analysis. Realize that you have just started the learning curve and all your efforts hence forth are worth it. Know where to find additional educational resources. Why choose it? In recent years, R has gained a lot of popularity among data scientists and analysts. The reason for this is simple: R is a language that is specifically designed for working with data. While other programming languages like C/C++, Java, and Python are general purpose languages that can be used in any domain, R is geared towards data analysis and manipulation. Because R is designed for working with data, it has several features that make it easier to work with large datasets. For instance, R has several built-in data structures that allow users to organize and manipulate data in a variety of ways. Additionally, R has a wide range of libraries and packages that can be used to perform specific tasks like data visualization, statistical analysis, and machine learning. Another reason why R is so popular among data scientists is that it is an open-source language. This means that anyone can contribute to its development, and there is a vast community of users and developers working together to improve the language and its capabilities. Despite its many advantages, R does have a few limitations. For example, it is not as fast as some other programming languages, and it can be difficult for beginners to learn. However, there are many resources available online to help users learn R, and once they get the hang of it, they will find that it is a powerful tool for data analysis and visualization. Overall, R is an excellent language for anyone who wants to work with data. Its specialized features and wide range of capabilities make it a top choice for data scientists and analysts everywhere. The Stack Overflow blog post The Impressive Growth of R by David Robinson, discusses the growth and popularity of the programming language R. The post highlights the increase in R’s usage on Stack Overflow, as well as the growing interest in R from various industries. We found in a previous post that Python has a solid claim to being the fastest-growing programming language in terms of Stack Overflow visits. The same analysis showed that the R programming language has shown remarkable growth in the last five years as well. In fact, R is growing at a similar rate to Python… The post provides an overview of R’s history, its advantages and disadvantages, and its current position in the programming world. The author notes that R’s popularity is due to its ability to handle large datasets, its flexibility for data analysis and in increase in popularity of data science and the growing number of companies using R for data analysis. Overall, the post concludes that R’s growth and popularity are likely to continue in the future, as more industries recognize the value of data analysis and turn to R as a solution. What you can do with it? The potential of what you can achieve with R is vast and ultimately depends on the level of dedication you have towards learning and expanding your skill set. By utilizing R, you can analyze data through various methods such as reading and plotting data, constructing analysis pipelines, prototyping new algorithms, and even writing your analysis code into shareable packages. With these abilities, you can not only perform data analysis, but also create a more efficient and reproducible workflow. The more you learn and experiment with R, the more you can discover and unlock its full potential. NOTES Some helpful explanatory notes and tips appear as a block quote. R can be a fast, nimble, forgiving scripting language with lots of ready-made tools and resources (CRAN, Github, Bioconductor). The R Learning Curve The learning curve for R 10+ years ago was difficult as there where fewer R resources, it was less mature with not a lot of interest. Additionally, there were fewer people in the community and data science wasn’t “a thing” yet. Figure 1.1: R learning curve past The R programming language is still challenging but worth it. With the introduction of packages encompassed in the tidyverse there are more high-quality resources, mature utilization with well documented explanations and examples. Currently there is lots of current interest in R with a large community of users and developers. Additionally, the data science “revolution has pushed R to develop and evolve, become more user-centric. Figure 1.2: R learning curve present Learning to code When it comes to learning a programming language, it can be daunting to know where to start. However, the first step to learning any programming language is to understand its syntax. Syntax refers to the set of rules and symbols that make up structurally correct code. Without proper syntax, even the smallest of errors can result in code that doesn’t run. These errors could be as simple as a typo, an incorrect name, missing spaces or too many spaces, or even wrong brackets. Syntax errors can be frustrating, especially for beginners, but it’s important to hang in there and start simple. It’s best to begin by trying to understand very simple cases first, before building and expanding on them. This approach will help you to get a better grip on the basics of the language and will help you to avoid becoming overwhelmed. If you’re learning R, there are many resources available to help you get started. You could start by reading through the R Book, which provides a comprehensive guide to the R programming language. Alternatively, there are many online tutorials available, which can help to break down complex concepts into more manageable pieces. In short, when learning R, it’s important to remember that syntax is key. By taking the time to understand the syntax rules, you can avoid frustrating syntax errors and build a solid foundation for your future coding endeavors 1.1 Alternatives When it comes to data science, R is a popular programming language among statisticians and data analysts. However, there are several data science alternatives to R that are also gaining popularity. One of the most popular alternatives to R is Python. Python is a general-purpose programming language that has a wide range of libraries and frameworks for data science. It is known for its simplicity, readability, and versatility. Python’s libraries such as NumPy, Pandas, and Scikit-Learn are widely used in data science for tasks such as data cleaning, data analysis, and machine learning. Another alternative to R is Julia, a new programming language that is designed specifically for scientific computing and numerical analysis. Julia is known for its speed and efficiency, making it a great choice for data analysis and modeling. Julia also has a growing package ecosystem with libraries such as DataFrames.jl and Flux.jl that are specifically designed for data science. Matlab is another alternative to R that is widely used in the scientific community. Matlab is known for its extensive numerical computing capabilities and its strong visualization features. It is commonly used in fields such as engineering, physics, and finance for data analysis and modeling. In conclusion, while R is a popular language for data science, it is not the only option available. Python, Julia, and Matlab are all viable alternatives with their own strengths and weaknesses. It is important to consider the specific needs of your project and choose the programming language that best suits your requirements. Did you know, that while R on its own is a powerful scripting language, some analytical tasks might require the use of other programming languages such as Python, C++ or Rust. Luckily, R provides different packages that allow us to use these languages within R code. These packages provide a seamless integration between these languages and R, allowing you to leverage the strengths of each language to perform complex tasks. The reticulate package enables the integration of Python code in R. This package allows you to import Python modules and functions directly into R and also allows you to call Python functions from R code. This is especially useful when you need to use Python’s machine learning libraries such as TensorFlow or PyTorch, which are not yet available in R. Similarly, the Rcpp package provides a smooth integration between R and C++. With this package, you can easily write C++ functions and use them directly in your R code. This is useful when you need to perform computationally-intensive tasks, such as simulations or optimization, that require the speed of C++. Finally, the extendr package provides an interface between R and Rust, allowing you to use Rust functions in R code and vice versa. Rust is a relatively new programming language that provides a balance between performance and safety. It is especially useful when you need to develop high-performance and low-level code, such as in systems programming or hardware development. 1.2 Resources When working with R, it is important to understand the basics and terms so that you can ask the right questions when seeking help. In the next two sections, we will provide an overview of these concepts to ensure that you have a solid foundation. It is worth noting that while googling your issue can be a great starting point, it is also important to seek out additional resources to help you solve your problem. For instance, you might consider joining an R community or forum where you can ask questions and receive feedback from other users. Additionally, many universities and organizations offer R workshops or training programs that can help you build your skills and knowledge. By taking advantage of these resources, you can develop a deeper understanding of R and become more confident in your ability to use it for data analysis and visualization. 1.2.1 Online In addition to Googling to find how to do something in R, there are several online resources available for individuals learning R programming and needing assistance with concepts or coding issues. These resources include CRAN, Bioconductor, RStudio Community, R-bloggers, and Stack Overflow. Each of these resources offers different benefits, such as packages, forums, blogs, and Q&amp;A communities, to help R users. Locating available packages (pre-built algorithms) The Comprehensive R Archive Network: cran.r-project.org/ Bioconductor: bioconductor.org/ RforMassSpectrometry: rformassspectrometry.org/ Ways to ask for help, or find answers to a similar question The RStudio Community: community.rstudio.com/ R Community Explorer: r-community.org/rstudio/ R bloggers: r-bloggers.com/ Stack Overflow: stackoverflow.com/ More ways to search and find what you are looking for R-Seek: rseek.org/ R-universe: r-universe.dev/search/ Cheat-sheets Tidyverse: www.tidyverse.org/ reading data: readr.tidyverse.org/ manipulating data :dplyr.tidyverse.org/ arranging table data: tidyr.tidyverse.org/ visualizing data ggplot2.tidyverse.org/ working with strings: stringr.tidyverse.org/ working with date and time: lubridate.tidyverse.org/ 1.2.2 In Print R books in print are becoming increasingly popular due to the growing demand for R programming and data analysis. These books offer several benefits that make them an excellent resource for anyone who wants to learn R programming or improve their data analysis skills. One of the key advantages of R books in print is that they are easy to read and navigate. The authors of these books take into consideration that not everyone who reads their books is an expert in programming. They use simple language and examples to explain concepts from the basics, making it easy for readers to understand. They can be used as a quick reference guide when working on a project or when facing a programming challenge. R books in print are cost-effective. While online resources are free, they are not always reliable, and it can be time-consuming to find the information you need. R books in print, on the other hand, are written and edited by experts who have years of experience in the field making them a reliable source of information. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data 1st Edition by Garrett Grolemund, Hadley Wickham Learn how to use R to turn raw data into insight, knowledge, and understanding. This book introduces you to R, RStudio, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for readers with no previous programming experience, R for Data Science is designed to get you doing data science as quickly as possible. Use R!, a collection of 67 print books. The Use R! collection of print books is a series of books aimed at helping people learn and use the R programming language. The books in this series cover a wide range of topics related to R, including data analysis, statistical modeling, and data visualization. Each book in the collection is written by a different author or group of authors, and provides a unique perspective on how to use R for different tasks. The books are targeted at a range of audiences, from beginners who are just starting to learn R, to more advanced users who are looking to expand their skills and knowledge. Some of the popular books in the Use R! collection include: An Introduction to R by Venables and Smith: This book provides a comprehensive introduction to the R programming language, covering topics such as data types, control structures, and functions. Data Manipulation with R by Spector: This book covers how to use R to manipulate data, including topics such as data cleaning, merging, and reshaping. ggplot2: Elegant Graphics for Data Analysis by Wickham: This book provides an in-depth introduction to the ggplot2 package in R, which is used for creating high-quality data visualizations. Applied Regression Analysis by Fox: This book covers how to use R to perform regression analysis, including topics such as linear regression, logistic regression, and mixed-effects models. In summary, the Use R! collection of print books is a valuable resource for anyone looking to learn or improve their skills in the R programming language. With a wide range of topics and authors, there is something for everyone in this collection. 1.2.3 Organizations Many R organizations offer individuals an opportunity for training and support, networking, access to resources, and help building a reputation. These benefits make R organizations a valuable resource to consider for both individuals and organizations using R for statistical computing and graphics. R User Group (RUG) RUGs are a relaxed and friendly way to broaden your contacts, scope and understanding of R. R User Groups R User Groups are communities of people who are interested in using R, a programming language and software environment for statistical computing and graphics. These groups are formed to provide a platform for individuals to learn, share knowledge, and collaborate on projects related to R programming. R User Groups usually meet on a regular basis, either virtually or in person, and organize events such as talks, workshops, and hackathons. These events are designed to provide members with opportunities to improve their skills, network with like-minded individuals, and work on projects that are of mutual interest. R User Groups are open to anyone who is interested in using R, regardless of their level of expertise. Members can range from beginners who are just starting to learn R, to experienced professionals who use R on a daily basis. This diversity of membership allows for a rich exchange of ideas and perspectives on the use of R in various fields, such as data science, finance, and healthcare. Joining an R User Group can be a great way to stay up-to-date with the latest developments in R programming, as well as to learn from and collaborate with other members. Many R User Groups also have online forums or discussion boards where members can ask questions, share resources, and seek feedback on their work. Where to find a RUG R Community Another list of RUGs R Specific Confrences The R Confrence The R Conference currently takes place in New York, Washington D.C., and soon Dublin, Ireland. They were created to foster the local R communities and serve as gathering places for people to learn from their peers. The R Conference hosts one of the most elite gatherings of data scientists and data professionals who come together to explore, share, inspire and to promote the growth of open source ideals. D4 Confrence Innovation and Entrepreneurship in Data, Design, Development and Discovery The D4 conference exists to bring creative communities together and to bolster the exchange of ideas. Data professionals, software developers, and other creatives can meet and collaborate. R Education at Confrences ASMS American Society for Mass Spectrometry The American Society for Mass Spectrometry (ASMS) was created in 1969 to promote and share knowledge of mass spectrometry. Membership includes over 8,500 scientists from academia, industry, and government labs. Members focus on technique and instrument advancements, as well as research in various sciences. ASMS offers several short-courses (1 or 2-day) covering a myriad of topics, including using R for data analysis. MSACL Mass Spectrometry &amp; Advances in the Clinical Lab MSACL aims to advance mass spectrometry and other advanced technologies in clinical laboratory medicine through education and training of practitioners, physicians, and other healthcare professionals. They also support the development of new technologies for diagnosis, treatment, and prognosis of clinical disorders. MSACL offers resources through their Learning Center on several topics, including using R in clinical data analysis. May Institute Computation and statistics for mass spectrometry and proteomics The event features keynotes, tutorials, and hands-on sessions led by proteomics experts and authors of methods and computational tools. We will take an in-depth look at case studies focused on the design and analysis of quantitative mass spectrometry-based experiments. At the end of each day, the speakers will be available online for additional exercises and Q&amp;A sessions, if there is interest. "],["installation.html", "Chapter 2 Installation 2.1 R interpreter 2.2 R Studio IDE 2.3 R Packages 2.4 For this Book", " Chapter 2 Installation In the scope of this book, there are three main components that need to be installed, and periodically updated: The R interpreter - the software that understands math and plotting RStudio IDE - the software that makes it easy write code and visualize data R Packages - bits of R code that perform specalized operations In this book we will be utilizing the RStudio integrated development environment (IDE) to interact with R. Two separate components are required for this - the R interpreter and the RStudio IDE. Both are required as the RStudio IDE only provides an interface for the R interpreter, which reads the code and does all the mathematical operations. The R interpreter can be used alone, interacting through the command line (eg. Windows CMD, MacOS and Linux Terminal), a plain text editor or another IDE such as Xcode, VSCode, Eclipse, Notepad++, etc. Rstudio provides a comprehensive, R specific environment, with auto-complete, code syntax highlighting, in-editor function definitions along with package management and plot visualizations. At the end of this chapter you should be able to Install R, RStudio and a few R packages Understand the major components for working with R. 2.1 R interpreter The underlying “engine” for R programming language can be downloaded from The R Project for Statistical Computing. R is an open-source implementation of the S statistical computing language originally developed at Bell Laboratories. Both langauges contain a variety of statistical and graphical techniques, however, R has been continually extended by professional, academic and amateur contributors and remains the most active today. With the advent of open-source sharing platforms such as GitHub, R has become increasingly popular among data scientists because of its ease of use and flexibility in handling complex analyses on large datasets. Additionally, one of R’s strengths is the ease with which well-designed publication-quality plots can be produced. Steps Navigate to The R Project Click on CRAN under Download, left-hand side Click on https://cloud.r-project.org/ under 0-Cloud This will take you to the globally nearest up-to-date repository Click on Download for ... and choose the OS compatible with your device Windows OS Click on base Binaries for base distribution. This is what you want … Click on “Download R-4.x.x for Windows” This prompt you to save R-4.x.x-win.exe file Locate and Double Click on the R-4.x.x-win.exe file. In the popup select your language Figure 2.1: Windows10 R Installer Click “OK” Click “Next” 3 times, accept defaults for Startup options Click “Next” 3 more times Install should be complete Figure 2.2: Windows10 R Installer MacOS For an Intel CPU: click R-4.x.x.pgk to download For an M1 CPU: click R-4.x.x-arm64.pkg to download After downloading, double-click the installer and follow the instructions Figure 2.3: Mac R Installer 2.2 R Studio IDE RStudio, prior to 2023, was an independent software provider for the ever-popular RStudio products, which included both the desktop and server based IDEs, along with the RShiny applications and servers that facilitate easy-to-build interactive web applications straight from R, and deployed on the web. The last chapter in this book will explore the tidyproteomics package which also has a Shiny web application. RStudio announced at the beginning of 2023 a soft pivot to Posit, which essentially is a rebranding of the RStudio company to encompass a larger data science audience, one that also provides integration with the Python programming language inside the RStudio IDE. The most trusted IDE for open source data science “RStudio is an integrated development environment (IDE) for R and Python. It includes a console, syntax-highlighting editor that supports direct code execution, and tools for plotting, history, debugging, and workspace management. RStudio is available in open source and commercial editions and runs on the desktop (Windows, Mac, and Linux).” — www.posit.co (Jan 2023) Steps Navigate to posit.co, alternatively rstudio.com redirects to the Posit website. Click Download RStudio in the menu top right Select RStudio Desktop Click Download RStudio skip 1: Install R Click Download RStudio Desktop for .. Windows OS Click on “Download RStudio Desktop for Windows” This prompt you to save RStudio-2023.x.x-xxx.exe file Locate and Double Click on the RStudio-2023.x.x-xxx.exe file. Click “Next” 3 times (accepting the defaults) Install should be complete Figure 2.4: Windows10 RStudio IDE Install MacOS Opening the .dmg file shows the archive that can be copied into the Applications folder simply by click-dragging the application onto the Applications folder shortcut. Figure 2.5: MacOS RStudio IDE Install IDE Layout The RStudio Integrated Development Environment (IDE) is a powerful tool that can make your data analysis and coding tasks more manageable. One of the key features of the RStudio IDE is that it consists of four individual panes, each containing parts of the total environment. This makes it easier for you to navigate your coding and analysis tasks. For example, while creating and viewing a plot, you can have the text editor and console open and organized. This way, you can easily see how the code you are writing is impacting the plot you are creating. Having everything in one place can also help reduce the clutter on your desktop, as you don’t need to have multiple applications open at the same time. Overall, the RStudio IDE is an excellent option for anyone looking to streamline their coding and data analysis workflows. By taking advantage of its various features, you can make your work more efficient and enjoyable. Figure 2.6: RStudio IDE in the default layout The Editor Tabs: All Open Files The Editor is a tool that allows you to write R code with ease. It is essentially a text editor, but with the added benefit of having knowledge of R. This means that it can automatically color different parts of your code based on their function. This can be a huge time saver, as it makes it easier to read and understand your code. For example, comments in R code start with a hash (#) symbol. In the Editor, these comments are colored light green, making them easy to spot. Similarly, operators like the plus sign (+) and the assignment operator (&lt;-) are colored light blue. This makes it easy to identify where these operators are being used in your code. Variables are an important part of any programming language, and R is no exception. In the Editor, variables are colored black. This makes it easy to distinguish variable names from other parts of your code. Finally, quoted text (also known as strings) are colored purple. This makes it easy to identify where strings are being used in your code. In summary, the Editor is a powerful tool that can help you write R code more efficiently. By automatically coloring different parts of your code, it makes it easier to read and understand. Whether you are a beginner or an experienced R programmer, the Editor can help you write better code in less time. Figure 2.7: RStudio IDE syntax highlighting The Editor also has the ability to suggest available variables and functions. In the image provided, the editor suggests using the mean() function to calculate the average of a collection of values. A pop-up with a description accompanies the suggestion. This feature occurs after typing in the first three letters of anything, and the editor will try to guess what you want to type next. This is a helpful tool that can save you time and effort when writing R code. Figure 2.8: RStudio IDE auto complete Files and Plots Tabs: Files, Plots, Packages, Viewer, and Presentation When you’re working in RStudio, your workflow is made simple with the various tabs and features available. For instance, the script that you’re currently working on is saved to the current project and can be accessed via the Files tab located on the top right-hand side of the pane. This tab provides an overview of all the files in the working directory, and you can easily navigate between them. If you need to open another file, you can do so by clicking on the File menu or by using the shortcut key. When you open a new file, it will create a new tab in the Editor pane, which allows you to switch between open files. This feature is super helpful when you’re working on multiple files simultaneously. Another useful tab located in the same pane is the Plots tab. This tab provides a quick way to view any active plots instantly. You don’t need to export your plots or save them separately. Instead, you can view them right within RStudio. This is where RStudio truly shines, as it brings together editing and visualization in one application. Figure 2.9: RStudio IDE plot window The Console Tabs: Console, Terminal, and Background Jobs In the RStudio IDE, the Console pane is where lines of code are executed from the editor. It is a vital component of the RStudio interface that allows users to interact with R in real-time. The Console pane is not only where code is run, but it is also where users can view output and error messages. Additionally, the Console pane provides users access to the computer’s terminal. This feature allows users to execute commands outside of the R environment, such as navigating files and directories or installing packages. Overall, the Console pane is an essential tool for any RStudio user and should be utilized to its full potential. Environment Tabs: Environment, History, Connections, and Tutorial When you’re working on a project in R, it’s essential to keep track of the variables and functions that you’re using in your current session. The Environment tab, located at the top left of the RStudio interface, provides a concise summary of in-memory variables and functions that were created locally, as opposed to functions that were loaded from a package. This summary can be useful for new-comers to R because it allows you to quickly see what objects you are currently working with, without having to remember each or manually check. By having a clear overview of your current session, you can avoid mistakes or errors that might arise from using the wrong object or function. Overall, the Environment tab is a helpful feature of RStudio that can save you time and frustration. If you’re new to R or just starting to use RStudio, make sure to keep an eye on the Environment tab and make use of its features as often as possible. As you become more versed in RStudio this tab may become less relevant. Figure 2.10: RStudio IDE environment window Usage Run from the editor (recommended) Type in the code in the Editor (top-left pane) Put editor cursor anywhere on that line Press Ctrl/CMD+Enter. Multiple lines: highlight multiple lines then press Ctrl/CMD+Enter #### Run from the onsole (occasionally) Type code into Console (bottom-left) after the ‘&gt;’ Press Enter. Multiple lines, not advised, but copy and paste multiple lines into console then press Enter. 2.3 R Packages R packages are a powerful tool in the R programming language that allow you to easily use code written by others in your own projects. They can save a lot of time and effort in the development of your own code, as they often provide new functions to deal with specific problems. For example, the popular ggplot2 package provides a variety of functions to help you create beautiful visualizations, while the mzR package allows you to read mass spectrometry data files with ease. Additionally, the twitteR package is a great tool for accessing Twitter data and conducting analysis. 2.3.1 Exploring It’s worth noting that packages can be written by anyone, which means that their quality can vary widely. While there are many high-quality packages available, it’s important to be wary of randomly coming across packages on the internet. To ensure that you’re working with trustworthy code, it’s a good idea to stick with well-established and frequently updated packages from reputable sources such as the CRAN (The Comprehensive R Archive Network) and Bioconductor repositories. By doing so, you can ensure that your code is reliable, efficient, and secure. CRAN cran.r-project.org Bioconductor bioconductor.org GitHub github.com In addition to using established packages, it’s also possible to create your own packages in R. This is a great way to share your own code with others and make it accessible to a wider audience. When creating a package, it’s important to follow a set of best practices to ensure that your code is well-documented, easy to use, and compatible with other packages. This includes providing clear and concise documentation, including examples and tutorials, and following established coding conventions. Another important consideration when working with R packages is version control. It’s essential to keep track of the versions of the packages you’re using, as updates can sometimes break existing code. By using a tool like Git or GitHub, you can easily manage different versions of your code and keep track of changes over time. This can be especially useful when collaborating with others on a project. Overall, R packages are an essential tool for anyone working with R. By using established packages and following best practices when creating your own, you can ensure that your code is efficient, reliable, and easy to use. And by using version control, you can keep track of changes over time and collaborate effectively with others. 2.3.2 Installing When working with R, it is important to understand how to install packages. R packages are collections of functions, data, and documentation that extend the capabilities of R. Most R packages have binary versions available for direct installation with no additional steps required. Binary packages are pre-compiled and ready-to-use packages that are platform-specific. They can be installed with the install.packages() function in R. Follow the examples below to install all the required packages used in this book. Jump to the following section if you run into any issues. Use the copy-paste button in the top-right of each code block. CRAN # this installs all of the packages in the tidyverse collection install.packages(&#39;tidyverse&#39;) Bioconductor # do this once to install the Bioconductor Package Manager install.packages(&quot;BiocManager&quot;) # this installs the mzR package BiocManager::install(c(&quot;mzR&quot;, &quot;xcms&quot;, &quot;MSstats&quot;, &quot;MSnbase&quot;)) GitHub # do this once to install the devtools package install.packages(&quot;devtools&quot;) # this installs the tidyproteomics package install_github(&quot;jeffsocal/tidyproteomics&quot;) There maybe several additional packages to install including additional operating system level installs. Go to the tidyproteomics webpage for additional installation help. Gotchas However, there are cases where a binary version of a package may not be available. This could be because the package is new or has just been updated. In such cases, the package may need to be compiled before it can be installed. Compiling a package involves converting the source code into machine-readable code that can be executed. To compile R packages, you’ll need to have the necessary programs and libraries installed on your computer. For Windows, you’ll need to install RTools, which provides the necessary tools for package compilation. For Mac, you’ll need to install Command Line Tools. Once these tools are installed, you can use them to compile packages that are not available as binaries. However, it’s worth noting that package compilation can sometimes fail for various reasons. This can be frustrating, especially if you’re new to R. Therefore, it is generally recommended to stick with using binary packages whenever possible. Binary packages are more stable and easier to install, making them the preferred option for most users. In summary, when working with R, it’s important to understand how to install packages. Most packages have binary versions available for direct installation, but there may be cases where you need to compile a package yourself. While package compilation can be useful in some cases, it can also be frustrating and time-consuming. Therefore, it’s generally recommended to stick with using binary packages whenever possible. 2.4 For this Book tidyverse The Tidyverse R package is a collection of data manipulation and visualization packages for the R programming language. It includes popular packages such as dplyr, ggplot2, and tidyr, among others. The Tidyverse R package is a powerful and versatile tool for data analysis in R. It includes a collection of data manipulation and visualization packages designed to work seamlessly together, making it easy to analyze and visualize data in R. This package is cover in more detail in 6 Tidyverse, and 8.2 GGplot2. library(tidyverse) ## ── Attaching core tidyverse packages ───────────────────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.1 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.2 ## ── Conflicts ─────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors The readr package provides a versatile means of reading data from various formats, such as comma-separated (CSV) and tab-separated (TSV) delimitated flat files. In addition to its versatility, the readr package is also known for its speed and efficiency. It is designed to be faster than the base R functions for reading in data, making it an ideal choice for working with large datasets. tbl &lt;- &quot;./data/table_peptide_fragmnets.csv&quot; %&gt;% read_csv() ## Rows: 14 Columns: 7 ## ── Column specification ───────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): ion, seq, pair, type ## dbl (3): mz, z, pos ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. The tibble package embodies a modern, flexible take on the data table, making it a powerful tool for data analysis in R. This package includes a suite of functions that allow you to easily manipulate and reshape data. It also has a printing method that makes it easy to view and explore data, even when dealing with large datasets. Additionally, tibble objects are designed to work seamlessly with other Tidyverse packages, such as dplyr and tidyr, making it easy to switch between packages and maintain a consistent syntax. print(tbl) ## # A tibble: 14 × 7 ## ion mz z seq pair pos type ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 b1+ 98.1 1 P p01 1 b ## 2 y1+ 148. 1 E p06 1 y ## 3 b2+ 227. 1 PE p02 2 b ## 4 y2+ 263. 1 DE p05 2 y ## 5 b3+ 324. 1 PEP p03 3 b ## 6 y3+ 376. 1 IDE p04 3 y ## 7 MH++ 401. 2 PEPTIDE p00 NA precursor ## 8 b4+ 425. 1 PEPT p04 4 b ## 9 y4+ 477. 1 TIDE p03 4 y ## 10 b5+ 538. 1 PEPTI p05 5 b ## 11 y5+ 574. 1 PTIDE p02 5 y ## 12 b6+ 653. 1 PEPTID p06 6 b ## 13 y6+ 703. 1 EPTIDE p01 6 y ## 14 MH+ 800. 1 PEPTIDE p00 NA precursor The readxl package is a complement to readr providing a means to read Excel files, both legacy .xls and the current xml-based .xlsx. It is capable of reading many different types of data, including dates, times, and various numeric formats. The package also provides options for specifying sheet names, selecting specific columns and rows, and handling missing values. The dplyr package is widely known and used among data scientists and analysts for its interface that allows for easy and efficient data manipulation in tibbles. Providing a set of “verbs” that are designed to solve common tasks in data transformations and summaries, such as filtering, arranging, and summarizing data, all designed to work seamlessly with other Tidyverse packages making it easy to switch between packages and maintain a consistent syntax. One of the key benefits of the dplyr package is its ease of use, making it perfect for beginners and advanced users alike. It is widely used in the R community and is a valuable tool for anyone working with R and data tables. tbl %&gt;% filter(type != &#39;precursor&#39;) %&gt;% group_by(type) %&gt;% summarise( num_ions = n(), avg_mass = mean(mz) ) ## # A tibble: 2 × 3 ## type num_ions avg_mass ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 b 6 378. ## 2 y 6 424. The tidyr package contains a set of data table transformations, including pivotting rows to columns, splitting a single column into multiple ones, and tidying or cleaning up data tables for a more usable structure. These transformations are essential for dealing with real-world data tables, which are often messy and irregular. By using tidyr, you can quickly and easily manipulate data tables to extract the information you need and prepare them for further analysis. tbl %&gt;% filter(type == &#39;precursor&#39;) %&gt;% pivot_wider(z, names_from = &#39;type&#39;, values_from = &#39;mz&#39;) ## Warning: Specifying the `id_cols` argument by position was deprecated in tidyr 1.3.0. ## ℹ Please explicitly name `id_cols`, like `id_cols = z`. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ## # A tibble: 2 × 2 ## z precursor ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 401. ## 2 1 800. The ggplot2 package stands out as the most advanced and comprehensive package for transforming tabulated data into meaningful and informative graphics. With its wide range of visualization tools, this package allows you to create expressive and compelling graphics that not only look great but also convey detailed information in a clear and concise manner. Apart from other visualization tools, ggplot2 takes a layered approach to creating graphics, allowing for the additive layering of additional data, labels, legends, and annotations, which helps to provide a more comprehensive view of your analysis. tbl %&gt;% mutate(int = rnorm(n(), mean = 1e5, sd=5e4), relative_int = int/max(int) * 100) %&gt;% ggplot(aes(mz, relative_int, color=type)) + geom_segment(aes(xend = mz, yend = 0)) + labs(title = &quot;Simulated MS/MS Spectrum&quot;) + theme_classic() One of the key benefits of using the Tidyverse is the standardization of syntax and functions across each package. This means that once you learn the basics of one package, you can easily switch to another package and be confident in your ability to use it. This makes it easier to create reproducible code and improves the efficiency of your data analysis. The Tidyverse is widely used in the R community and is a valuable tool for any data scientist or analyst working with R. It is especially useful for those who need to manipulate and visualize data quickly and efficiently, without sacrificing accuracy. Whether you are new to R or an experienced user, the Tidyverse is a must-have tool in your data analysis toolkit. MS Packages This book, while providing a beginners level guide to R programming, also introduces several mass spectrometry-specific packages in many of the code examples. While these examples may only touch on some of their functions, the last chapter is dedicated to a more formal, albeit not comprehensive introduction to many of these packages. For example the 10.4.3 mzR package, which enables users to read and process mass spectrometry data, as well as the 10.4.4 xcms package, which is used for preprocessing and feature detection. Additionally, the book introduces the 10.4.2 MSnbase package, which provides a framework for quantitative and qualitative analysis of mass spectrometry data, and the 10.5.1 MSstats package, which is used for statistical analysis of quantitative proteomics experiments. Lastly, the book covers the tidyproteomics package, which provides a collection of tools for analyzing post-analysis quantitative proteomics data using a framework similar to the tidyverse. "],["rstudio-basics.html", "Chapter 3 RStudio Basics 3.1 R vs R Studo 3.2 RStudio Basic Interactions 3.3 Hot Keys 3.4 Create, Run, Save Scripts 3.5 Running Code 3.6 R Notebooks vs R scripts 3.7 R Studio Projects Exercises", " Chapter 3 RStudio Basics 3.1 R vs R Studo R and RStudio are related and have become somewhat synonymous because the popular of RStudio for developing R code. However, it is important to understand the difference. R: R is a programming language and software environment specifically designed for statistical computing and graphics. It provides a wide variety of statistical and graphical techniques, including linear and nonlinear modeling, time-series analysis, classification, clustering, and more. R is open-source and is maintained by a community of statisticians and data scientists. RStudio: RStudio, on the other hand, is an integrated development environment (IDE) for R. It provides a user-friendly interface for writing, running, and debugging R code. RStudio offers features like syntax highlighting, code completion, built-in plotting, debugging tools, and package management. It also includes features for managing projects, organizing files, and integrating with version control systems like Git. In essence, R is the language itself, while RStudio is a tool that makes working with R more efficient and convenient by providing an intuitive interface and additional features for development and analysis. 3.2 RStudio Basic Interactions Script Editor: The central pane is the script editor, where you can write and edit your R code. This is where you’ll spend most of your time writing your scripts, functions, and analyses. Console: The bottom-left pane is the console, which is where R code is executed. You can directly type commands into the console and see the results immediately. It’s also where error messages and warnings are displayed. Environment/History: The top-right pane typically displays your current R environment, including objects (variables, data frames, functions, etc.) that are currently loaded into memory. It also shows your command history. File Viewer/Plots/Packages/Help: The bottom-right pane is a multi-functional pane that can display various tabs such as Files (for navigating your file system), Plots (for displaying plots generated by R), Packages (for managing installed R packages), and Help (for accessing documentation and help files). 3.2.1 Other Areas Toolbar: The toolbar at the top provides quick access to common actions such as saving scripts, running code, debugging, and accessing project-related functions. Tabs and Panes: RStudio allows you to customize the layout by rearranging or closing panes and tabs according to your preferences. You can also split panes horizontally or vertically to view multiple files or outputs simultaneously. Project Management: RStudio has built-in support for managing projects, which helps organize your work by providing a dedicated workspace with its own working directory, environment, and settings. These are the basic interactions in RStudio, but as you become more familiar with the environment, you’ll discover additional features and functionalities that can enhance your workflow and productivity. 3.3 Hot Keys To find RStudio hotkeys, you can navigate to the “Tools” menu in RStudio and find Keyboard Shortcuts Help (or alt+shift+k or opt+shift+k). You can view and customize the existing keyboard shortcuts with the Modify Keyboard Shortcuts... option. Here are some very important keyboard shortcuts to know: Run Current Line or Selection: Windows: Ctrl + Enter macOS: Command + Enter Run Current Script: Windows: Ctrl + Shift + S macOS: Command + Shift + S Comment/Uncomment Current Line or Selection: Windows: Ctrl + Shift + C macOS: Command + Shift + C Insert Assignment Operator: Windows: Alt + - macOS: Option + - Clear Console: Windows: Ctrl + L macOS: Command + L Interrupt R Process: Windows: Ctrl + Break macOS: Command + Period Open File: Windows: Ctrl + O macOS: Command + O Save File: Windows: Ctrl + S macOS: Command + S Find: Windows: Ctrl + F macOS: Command + F Close Current Tab: Windows: Ctrl + W macOS: Command + W 3.4 Create, Run, Save Scripts As scripts are the business of RStudio, it is important to know how to deal with them and incorporate them in your developments. Creating a New Script: To create a new script in RStudio, you can go to the “File” menu at the top left corner. From there, select “New File” and then “R Script.” Alternatively, you can use the shortcut “Ctrl + Shift + N” on Windows or “Command + Shift + N” on macOS to create a new script directly. Writing and Editing Code: Once you’ve opened a new R script, you can start writing your R code in the script editor pane, which is the central pane in RStudio. You can type or paste your R code directly into this pane. RStudio provides syntax highlighting to make your code more readable. You can also insert comments in your script using the “#” symbol. Running Code: To run specific lines of code or a selection in your script, you can either: Place your cursor on the line(s) you want to run and press “Ctrl + Enter” on Windows or “Command + Enter” on macOS. Select the lines you want to run and then click the “Run” button in the toolbar at the top of the script editor pane. Alternatively, you can run the entire script by clicking the “Source” button in the toolbar, or by using the shortcut “Ctrl + Shift + S” on Windows or “Command + Shift + S” on macOS. Saving Scripts: To save your script, you can go to the “File” menu and select “Save” or “Save As” to specify a new file name or location. You can also use the shortcut “Ctrl + S” on Windows or “Command + S” on macOS to quickly save your changes. RStudio will save your script with a “.R” extension by default. By following these steps, you can create, run, and save scripts in RStudio efficiently. This workflow is essential for developing and executing R code for data analysis, statistical modeling, and other tasks. 3.5 Running Code 3.5.1 R Notebooks Running code as you develop R code can give you immediate feedback about whether the code actually runs, can also print something to the console that is useful in understanding your data analysis, or can generate new data objects for other acts. Running R code in RStudio can be done in three main ways: line-by-line execution, running a block of lines, or executing an entire script. Line-by-Line Execution: In RStudio, you can execute R code line-by-line by placing your cursor on the line you want to run and then pressing “Ctrl + Enter” (Windows) or “Command + Enter” (Mac). This method is useful for debugging or testing small sections of code, as you can see the output or any errors generated by each line immediately. Running a Block of Lines: If you want to execute a block of consecutive lines, you can select the lines you want to run in the script editor. After selecting the lines, you can press “Ctrl + Enter” (Windows) or “Command + Enter” (Mac) to execute the entire selected block at once. This method is convenient when you want to test or run a specific section of your code without executing the entire script. Running an Entire Script: To execute an entire R script in RStudio, you have a couple of options: You can click the “Source” button in the toolbar at the top of the script editor pane. Alternatively, you can use the keyboard shortcut “Ctrl + Shift + S” (Windows) or “Command + Shift + S” (Mac) to run the entire script. Running the entire script is useful when you want to execute all the code in your script file, such as when reproducing an analysis or generating final results. By utilizing these methods, you can effectively run R code in RStudio, whether it’s for debugging, testing, or executing complete scripts for data analysis or other tasks. Each method offers flexibility and efficiency in different scenarios, depending on your workflow and needs. 3.6 R Notebooks vs R scripts 3.6.1 R Notebooks Notebooks are a great way that has come to prominence in recent years to develop R or other language code. R Notebooks are a powerful tool in RStudio that allow you to combine narrative text, code, and the output of that code (such as plots or tables) into a single, interactive document. They are particularly useful for reproducible research, data analysis, and sharing insights with others. Here’s a breakdown of their use: Creating an R Notebook: To create a new R Notebook in RStudio, you can go to the “File” menu, choose “New File,” and then select “R Notebook.” Alternatively, you can use the keyboard shortcut “Ctrl + Shift + N” (Windows) or “Command + Shift + N” (Mac) and choose “R Notebook.” Structure of an R Notebook: R Notebooks have a similar structure to R scripts, with the addition of Markdown cells for narrative text. Markdown cells allow you to write formatted text, including headings, lists, links, and more, using simple syntax. You can intersperse Markdown cells with code chunks, where you write and execute R code. Writing Narrative Text: Markdown cells in R Notebooks are where you can provide explanations, interpretations, and context for your analysis. You can use Markdown syntax to format text, create headings, include images, and even write mathematical equations using LaTeX syntax. Writing and Executing R Code: Code chunks in R Notebooks are denoted by three backticks followed by “r” in curly braces: {r}. Within code chunks, you can write and execute R code just like you would in a regular R script. Code chunks can be run individually or all at once, and the output (such as plots, tables, or printed results) is displayed directly below each code chunk. Reproducibility: R Notebooks promote reproducibility by capturing both the code and its output in a single document. Others can easily reproduce your analysis by running the notebook, ensuring transparency and accountability in research and data analysis. Exporting and Sharing: R Notebooks can be exported to various formats, including HTML, PDF, and Word, allowing you to share your work with collaborators or stakeholders. You can also publish R Notebooks directly to platforms like RStudio Connect or R Markdown websites for broader dissemination. Overall, R Notebooks provide a seamless way to integrate code, text, and output in a single document, facilitating reproducible research and effective communication of data analysis findings. They are a valuable tool for anyone working with R for data analysis, research, or reporting. 3.6.2 R Scripts R scripts by contrast are JUST CODE elements where descriptive text can be added as comments. Comments are added with at least 1 # before code like below. # comments go like this # do a basic addition 2 + 2 3.7 R Studio Projects RStudio gives us a lot of tools to help beginners and experts develop better AND faster. Project are how RStudio organizes your work. Think of project as singular goal oriented collection. There are no rules but some basic organizational tips should help simplify your project. 3.7.1 Creating Creating a new project is very forgiving, you can create a new directory with a project name, or create a project out of an existing directory. Either 1. Click on the drop down in the top right 2. OR: Under the menu item select File &gt; New Project In the New Project Wizard select New Directory &gt; New Project, enter the name of the project and click Create Project. Figure 3.1: RStudio Create New Project Figure 3.2: RStudio Create New Project 3.7.2 Organizing Organizing your RStudio project is an important task that can help you work more efficiently and avoid confusion when working on a project. Good organization will help you come back to your code at a later date as well as make it possible in some smaller projects to share entirely the data and code, making it possible for others to run the code and modify it. Typical project directory structure your_project |- 001_import_data.R |- 002_tidy_data.R |- 003_analysis_01.R |- 004_analysis_02.R |- data |- tidy_table.csv |- main_file.xlsx |- plots |- plot_xy_scatter.png |- plot_xy_regression.png |- results |- predicted_output_20230531.csv Data It’s a good idea to keep your data files separate from your R scripts and other files. If the data is small enough (at your disgression what is small), it can be helpful to store the data in /data/ folder in the project itself! You can then also use subfolders to organize different types of data or data from different sources with the /data/ folder. You can create and navigate folders in your RStudio project directly in RStudio! Figure 3.3: RStudio Create New Folder Scripts One of the best ways to manage scripts is tocreate scripts for different tasks: Break your project into separate scripts that perform different tasks, such as data cleaning, data analysis, and data visualization. This can make it easier to modify and update specific parts of your project without affecting other parts. Results Like data, results may be best saved in a specific results folder. Keeping track of date and time of results can also be smart. Tables Tables can be exported as .csv or other formats depending on your use case and stored in results. Always keep track of date and time the results were created. One issue that can arise is a new version of the result was created by a new process in the code but that is not indicated in the file name, so it is hard to be certain the results are the latest. Plots Similar to tables, plot files could be stored in a sub folder. Exercises Create a new R Studio Project and name it 001_new_project. Create a new R script, add your name and date at the top as comments. Create an R script file, write something (anything), and save it as 01_test. Create a new R script, add your name and date at the top as comments. Locate your new project on your PC. "],["r-syntax.html", "Chapter 4 R Syntax 4.1 Reserved Words 4.2 Types 4.3 Flow-Control Exercises", " Chapter 4 R Syntax Welcome to the R Book! In this chapter, we will explore the basics of R, a powerful programming language used for statistical computing and graphics. At its most fundamental level, R is a calculator capable of performing simple, and complex, mathematical operations. It can read and write data to and from files, manipulate the data, calculate summaries and plot visual representations of the data. Essentially, it is a programmatic version of a spreadsheet program. However, R is much more than just a calculator. It is also a platform for conducting complex analyses, statistical evaluations, predictive inferencing, and machine learning. With R, you can explore and visualize data in a variety of ways, perform advanced statistical analyses, and build predictive models. In this chapter, we will start by examining the simplest operations of R. We will cover basic arithmetic, working with variables, and creating basic plots. By the end of this chapter, you will have a solid understanding of the fundamentals of R and be ready to tackle more complex topics. So, let’s get started! At the end of this chapter you should be able to Understand R’s syntax, variables, operators and functions. Create and edit a project in RStudio. 4.1 Reserved Words As we begin our journey, it’s important to keep in mind that there are certain reserved words that carry a special meaning and cannot be used as identifiers. These words have been set aside by the R programming language, and using them as variable names or function names could lead to errors in your code. Therefore, before we dive too deeply into our R programming endeavors, let’s take a moment to familiarize ourselves with these reserved words. This will help us avoid potential issues down the road and ensure that our code runs smoothly. # to read more about them type ?reserved Word Use if, else flow control, part of the if-then-else statement for, repeat, while, break, next flow control, part of the for-loop statement function basis for defining new algorithms TRUE, FALSE Boolean logic values NULL an undefined value Inf , -Inf an infinite value (eg. 1/0 ) NaN ‘not a number’ NA a missing value indicator A Null results when a value is missing and could be a string or a numeric, where as a NA results when a known value, such as in a column of numbers, is missing. 4.2 Types Welcome to the R Book! Whether you’re just starting out or a seasoned pro, understanding the different components of R code is essential for writing high-quality, efficient R programs. In this section, we’ll take a deep dive into the various components of R code that you should be familiar with. R input is composed of typed characters that represent different parts of a process or mathematical operation. These characters come together to form what we call R code. It’s important to note that R code is not just a random collection of characters - each character serves a specific purpose and contributes to the larger structure of the code. As such, understanding the different components of R code is key to writing effective and efficient R programs. So, what are these different components of R code? Below, we’ve provided some examples to help you get started: 4.2.1 comments # this is an important note 4.2.2 strings \"letters\" or \"numbers\" in quotes 4.2.3 numbers 1 integers or 1.000002 floats 4.2.4 operators +, -, /, *, … 4.2.5 variables var &lt;- 2 containers for information 4.2.6 statements == exactly the same, != not the same 4.2.7 functions add(x, y) complex code in a convenient wrapper By understanding these different components of R code, you’ll be well on your way to writing effective and efficient R programs. So let’s dive in and get started! # adding two numbers here and storing it as a variable four &lt;- 2 + 2 # using the function &#39;cat&#39; to print out my variable along with some text cat(&quot;my number is &quot;, four) ## my number is 4 R does not have an line ending character such as ; in java, PHP or C++ 4.2.1 Comments Comments are essential parts of the code you will write. They help explain why you are taking a certain approach to the problem, either for you to remember at a later time or for a colleague. Comments in other coding languages, including R package development, can become quite expressive, representing parts and structures to a larger documentation effort. Here, however, comments are just simple text that gets ignored by the R interpreter. You can put anything you want in comments. oops, not a comment # This is a comment # and here a comment tag is used to ignore legitimate R code # four &lt;- 2 + 2 four &lt;- 2 * 2 4.2.2 Strings Strings are essentially a sequence of characters, consisting of letters or numbers. They are commonly used in programming languages and are used to represent text-based data. A string can be as simple as a single character, such as “A”, or it can be a longer sequence of characters such as “Hello, World!”. Strings are often used to store data that requires text manipulation, such as usernames, passwords, and email addresses. In contrast to words, which are made up of a specific combination of letters to represent a linguistic term, strings do not follow any specific rules of composition and can be a random or semi-random sequence of characters. # a string can be a word, this is a string variable three &lt;- 1 + 2 # or an abbreviation, this is a variable (thr) representing the string &quot;three&quot; thr &lt;- &quot;three&quot; # a mass spec reference peptide &lt;- &quot;QWERTK&quot; # or an abbreviated variable pep &lt;- &quot;QWERTK&quot; When working with R programming language, it is essential to note that strings play a crucial role in the syntax used. Strings, which define text characters, are used to represent data in R, and they must be enclosed in quotes. Failure to do so will result in the interpreter assuming that you are referring to a variable that is not enclosed in quotes. For instance, in the example above, the peptide variable contains the string of letters representing the peptide amino acid sequence \"QWERTK\". However, it is essential to note that there are no strict rules for how strings and variables are composed, except that variables cannot start with a number. # permitted b4 &lt;- 1 + 3 # not permitted 4b &lt;- 1 + 3. ## Error: unexpected symbol in &quot;4b&quot; There are however, conventions that you can follow when constructing variable names that aid in the readability of the code and convey information about the contents. This is especially useful in long code blocks, or, when the code becomes more complex and divested across several files. For example: # a string containing a peptide sequence str_pep &lt;- &quot;QWERTK&quot; # a data table of m/z values and their identifications tbl_mz_ids &lt;- read_csv(&quot;somefile.csv&quot;) To learn more about and follow specific conventions, explore the following resources: Hadley Wickham’s Style Guide Google’s style Guide The tidyverse style guide 4.2.3 Numbers Numbers are the foundation upon which all data analysis is built. Without numbers, we would not be able to perform calculations, identify patterns, or draw conclusions from our data. In the programming language R, there are two main types of numbers: integers and floats. An integer is a whole number with no decimal places, while a float is a number with decimal places. Understanding the difference between these two types of numbers is essential for accurate numerical analysis. In R, integers are represented as whole numbers, such as 1, 2, 3, and so on, while floats are represented with a decimal point, such as 1.5, 2.75, and so on. It is important to note that integers occupy less space in memory than floats, which can be a consideration when working with large datasets. This means that when possible, it is generally better to use integers over floats in R, as they are more efficient and can improve the overall performance of your code. # integers 1, 12345, -17, 0 Numbers are the foundation upon which all data analysis is built. Without numbers, we would not be able to perform calculations, identify patterns, or draw conclusions from our data. In the programming language R, there are two main types of numbers: integers and floats. An integer is a whole number with no decimal places, while a float is a number with decimal places. In most programming languages, including R, integers are represented as whole numbers, such as 1, 2, 3, and so on, while floats are represented with a decimal point, such as 1.5, 2.75, and so on. It is essential to understand the difference between these two types of numbers for accurate numerical analysis. While integers can only represent whole numbers, floats can represent fractions and decimals. Thus, if you need to represent a number that is not a whole number, you should use a float. Moreover, it is important to note that integers occupy less space in memory than floats. This can be a consideration when working with large datasets, especially when the whole number is enough to represent the data. Therefore, when possible, it is generally better to use integers over floats in R, as they are more efficient and can improve the overall performance of your code. # floats significand &lt;- 12345 exponent &lt;- -3 base &lt;- 10 # 12.345 = 12345 * 10^-3 significand * base ^ exponent 4.2.4 Operators Operators are fundamental components of programming that enable us to manipulate and process various data types. They are symbols that perform a specific action on one or more operands, which could be numeric values, variables, or even strings. Most commonly these symbols allow us to perform basic arithmetic operations such as addition, subtraction, multiplication, and division on numeric values, as well as more complex mathematical operations like exponentiation and modulus. In addition to numeric values, operators can also manipulate string variables. For instance, we can use concatenation operators to join two or more strings together, which is particularly useful when working with text data. By utilizing operators, we can perform powerful operations that allow us to build complex programs and applications that can handle large amounts of data. Operators play a crucial role in programming, as they allow us to manipulate data in a way that would be difficult or impossible to achieve otherwise. At their very basic, operators allow you to perform calculations .. 1 + 2 ## [1] 3 1 / 2 ## [1] 0.5 .. assign values to string variables .. myvar &lt;- 1 .. and compare values. 1 == myvar ## [1] TRUE 2 != myvar + myvar ## [1] FALSE Here is a table summarizing of some common operators in R. Operator Name Description Example &lt;- assignmnet assigns numerics and functions to variables x &lt;- 1 x now has the value of 1 + addition adds two numbers 1 + 2 = 3 - subtraction subtracts two numbers 1 - 2 = -1 * multplication multiplies two numbers 1 * 2 = 2 / division divides two numbers 1 / 2 = 0.5 ^ power or exponent raises one number to the power of the other 1 ^ 2 = 1 = equals also an assignment operator x = 1 x now has the value of 1 == double equals performs a comparison (exactly equal) 1 == 1 = TRUE != not equals performs a negative comparison (not equal) 1 != 2 = TRUE %% modulus provides the remainder after division 5 %% 2 = 1 Remember order of operations (PEMDAS): Parentheses, Exponents, Multiplication and Division (from left to right), Addition and Subtraction (from left to right). 4.2.5 Variables In programming, variables are essential elements used to store information that can in essence vary. They come in handy when we need to manipulate or retrieve the information stored in them. Variables can be thought of as containers that can store any kind of information, such as letters, words, numbers, or text strings. They are flexible enough to hold different types of data, and we can use them to store all sorts of information. One of the most significant advantages of using variables is that we can refer to them repeatedly to retrieve the information stored in them. We can also manipulate the information stored in them with an operation or replace it with an assignment. Variables are a powerful tool in programming that allows us to store and retrieve information, manipulate it, and perform various operations on it. # create two viables and assign values to each var_a &lt;- 1 var_b &lt;- 3.14 var_a + var_b ## [1] 4.14 R even has some intrinsic variables that come in handy, like pi. pi ## [1] 3.141593 In R it is easy to overwrite existing variables, either initialized by R or created by you, causing error and confusion. pi &lt;- 9.876543 pi ## [1] 9.876543 4.2.6 Statements Using a comparison operator, you can make logical comparisons called statements. Operator Description Example | an either or comparison, TRUE if both are true FALSE if one is false. 1 == 1 | 1 != 2 = TRUE 1 == 1 | 1 == 2 = FALSE &amp; a comparison where both must be TRUE 1 == 1 &amp; 1 != 2 = TRUE 1 == 1 &amp; 1 != 2 = FALSE There are also the double operators || and &amp;&amp;, these are intended to work as flow control operators and stop at the first condition met. In the most recent versions of R, the double operators will error out if a vector is applied. 4.2.7 Functions In programming, a function is a type of operator that performs a specific task and can accept additional information or parameters. Functions in the R programming language are fundamental building blocks used to encapsulate and execute a sequence of statements. They allow for modular, reusable, and efficient code development. Functions in R can perform a wide range of tasks, from simple operations like adding two numbers to complex data analyses and visualizations. The structure and behavior of functions in R are designed to support both built-in functions provided by R itself and user-defined functions created by programmers to meet specific needs. Functions in R can do a wide range of tasks, such as perform a simple calculation and return a single variable. or a vector of variables. Functions can be used to clean, subset, merge, and transform data frames or lists. They can perform statistical modeling and analysis as well as create simple plots and complex, multi-layered graphics. Moreover, R empowers users to define their own functions, allowing for the encapsulation of complex or repetitive tasks into single, reusable commands, enhancing the language’s flexibility and efficiency. A function in R is defined using the function keyword, followed by a set of parentheses that can contain any arguments (parameters) the function requires, and a body enclosed in curly braces {} that contains the code to be executed. Here’s the basic syntax: add &lt;- function(a,b) { a + b } add(1,2) ## [1] 3 Note that this function requires the inputs for a and b as denoted in the parameters brackets (). The function then transfers those inputs into the main body of the function that performs the operation inside the curly brackets {}. And while this single line function is compact and concise, it does not define default values, check any of the inputs, or explicitly return a value. And as a consequence we can get an error that can be confusing. add(1,&#39;two&#39;) ## Error in a + b: non-numeric argument to binary operator add(1) ## Error in add(1): argument &quot;b&quot; is missing, with no default The following is a more robust function that can be reused at a later date that adds some readability to the process and explicitly returns, with the return() function, the intended value. Note, that we used some functions, is.numeric(), stop(), and paste0 inside our function. add &lt;- function( a = 1, b = 2 ) { if(!is.numeric(a)) { stop(&#39;the first value is not a number&#39;) } if(!is.numeric(b)) { stop(paste0(&#39;the second value &quot;&#39;, b, &#39;&quot; is not a number&#39;)) } answer &lt;- a + b return(answer) } add(1,2) ## [1] 3 add(1) ## [1] 3 add(1,&#39;two&#39;) ## Error in add(1, &quot;two&quot;): the second value &quot;two&quot; is not a number 4.3 Flow-Control Sometimes in the course of data analysis we have to make decisions or branch decisions based on what is contained within the data. The logic of how this done within the programming language is called flow control. More generally, flow control is an essential aspect of programming that allows you to control the order in which statements and functions are executed. 4.3.1 For Loop In R, a loop is a programming construct that allows you to execute a block of code repeatedly. Loops are used when you want to perform a set of instructions repeatedly, such as when you want to iterate over a set of data and perform a particular operation on each element. There are several types of loops available in R, including the for loop, the while loop, and the repeat loop. The for loop is the most commonly used loop in R. It is used to iterate over a sequence of values, such as a vector or a list, and perform a particular operation on each element of the sequence. The basic syntax of a for loop in R is as follows: for (var in sequence) { # code to be executed } Let’s look at a very simple for loop: # Create a vector of numbers numbers &lt;- c(1, 2, 3, 4, 5) # Iterate over the vector using a for loop for (num in numbers) { print(num) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 This code first creates some example data, numbers 1 to 5 increasing by 1, then we go every number and print the value. Translating the synatx to english we can say for every num in numbers vector, print the num. 4.3.2 If-Else When we have data that needs to be treated based on a condition of the data, we have a branching decision. In this case, our flow control is an If-Else statement. In plain english, if a condition is met, we do something, else we do something else. The brackets between these statements determine what is done. Let’s take the previous example and print if the number is even or odd. You can see already that in how we formulate the programming question we already see the syntax. We say “if the number is even…” if is our flow control # Create a vector of numbers numbers &lt;- c(1, 2, 3, 4, 5) # Iterate over the vector using a for loop for (num in numbers) { if (num %% 2 == 0) { print(paste(num, &quot;is even&quot;)) } else { print(paste(num, &quot;is odd&quot;)) } } ## [1] &quot;1 is odd&quot; ## [1] &quot;2 is even&quot; ## [1] &quot;3 is odd&quot; ## [1] &quot;4 is even&quot; ## [1] &quot;5 is odd&quot; In this example, we’re using the %% operator to determine whether each number in the vector is even or odd. If the number is even, the code inside the if block is executed and the number is printed along with the message “is even”. If the number is odd, the code inside the else block is executed and the number is printed along with the message “is odd”. `paste is a handy function for combining Exercises Create a new R Studio Project and name it 002_basics. Create a new R script, add your name and date at the top as comments. # Your Name # YYYY-MM-DD # Institution # # Description Calculate the sum of 2 and 3. ## [1] 5 Evaluate if 0.5 is equal to 1 divided by 2. ## [1] TRUE Define a variable that is is 98.6 degrees in Fahrenheit. Construct an if-else statement to determine if the temperature indicates a fever (temperature greater than or equal to 100). If it does print “The temperature indicates a fever.” If the temperature is less than 100, print “The temperature does not indicate a fever.” ## [1] &quot;The temperature indicates a fever.&quot; Create a function to test if a temperature is a fever called fever_checker the prints above, but can be reused fever_checker(101) ## [1] &quot;The temperature indicates a fever.&quot; fever_checker(98.6) ## [1] &quot;The temperature does not indicate a fever.&quot; Use similar logic to print if a temperature is the homeostatic range for human beings (97.7–99.5). homeostatic_range_check(96.0) ## [1] &quot;Outside homeostatic range!&quot; homeostatic_range_check(100) ## [1] &quot;Outside homeostatic range!&quot; Advanced exercise… add TRUE / FALSE returns to the functions and create a function that combines both called temperature_check. That gives us info on the what our temperature means (i.e., is it homeostatic, is it a fever, are we possibly dead (temperature way too low??) "],["r-objects.html", "Chapter 5 R Objects 5.1 Variable 5.2 Vector 5.3 List 5.4 Matrix 5.5 Data Frame 5.6 Data Table 5.7 Tibbles 5.8 Managing Objects Exercises More Exercises", " Chapter 5 R Objects The R programming environment includes four basic types of data structures that increase in complexity: variable, vector, matrix, and list. Additionally there is the data.frame while and independent data structure, it is essentially derived from the matrix. At the end of this chapter you should be able to Understand the 5 most common data structures. Understand the data structure lineage. Access given subsets of a multi-variable data object. This book introduced variables briefly in 8.2.1. Here, we will expand on that introduction. At its simplest, a variable can be thought of as a container that holds only a single thing, like a single stick of gum. A vector is an ordered, finite collection of variables, like a pack of gum. A matrix consists of columns of equally-sized vectors, similar to a vending machine for several flavors of gum packs. Mentally, you can think of them as a point, a line, and a square, respectively. Figure 5.1: R main data structures 5.1 Variable Again, a variable is the most basic information container, capable of holding only a single numeric or string value. a &lt;- 1 5.2 Vector A vector is simply a collection of variables of all the same type. In other programming languages these are called arrays, and can be more permissive allowing for different types of values to be stored together. In R this is not permitted, as vectors can only contain either numbers or strings. If a vector contains a single string value, this “spoils” the numbers in the vector, thus making them all strings. # permitted a &lt;- c(1, 2, 3) a ## [1] 1 2 3 # the numerical values of 1 and 3 are lost, and now only represented as strings b &lt;- c(1, &#39;two&#39;, 3) b ## [1] &quot;1&quot; &quot;two&quot; &quot;3&quot; Vectors can be composed through various methods, either by concatenation with the c() function, as seen above, or using the range operator :. Note that the concatenation method allows for the non-sequential construction of variables, while the range operator constructs a vector of all sequential integers between the two values. 1:3 ## [1] 1 2 3 There are also a handful of pre-populated vectors and functions for constructing patters. # all upper case letters LETTERS ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; &quot;K&quot; &quot;L&quot; &quot;M&quot; &quot;N&quot; &quot;O&quot; &quot;P&quot; &quot;Q&quot; &quot;R&quot; &quot;S&quot; &quot;T&quot; &quot;U&quot; &quot;V&quot; ## [23] &quot;W&quot; &quot;X&quot; &quot;Y&quot; &quot;Z&quot; # all lower case letters letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; &quot;v&quot; ## [23] &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; # a repetitive vector of the letter &quot;a&quot; rep(&#39;a&#39;, 5) ## [1] &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; # a repetitive vector of a previous vector rep(b, 2) ## [1] &quot;1&quot; &quot;two&quot; &quot;3&quot; &quot;1&quot; &quot;two&quot; &quot;3&quot; # a sequence of integers between two values, in this case reverse order seq(10, 5) ## [1] 10 9 8 7 6 5 # same as above 10:5 ## [1] 10 9 8 7 6 5 While variables don’t require a referencing scheme, because they only contain a single value, vectors need to have some kind of referencing scheme, shown in 5.1 as x[9] and illustrated in the following example. The use of an integer vector to sub-select another vector based on position. R abides by the 1:N positional referencing, where as other programming languages refer to the first vector or array position as 0. A good topic for a lively discussion with a computer scientist. x &lt;- LETTERS # 3rd letter in the alphabet x[3] ## [1] &quot;C&quot; # the 9th, 10th, 11th and 12th letters in the alphabet x[9:12] ## [1] &quot;I&quot; &quot;J&quot; &quot;K&quot; &quot;L&quot; # the 1st, 5th, 10th letters in the alphabet x[c(1,5,10)] ## [1] &quot;A&quot; &quot;E&quot; &quot;J&quot; Numerical vectors can be operated on simultaneously, using the same conventions as variables, imparting convenient utlity to calculating on collections of values. x &lt;- 1:10 x / 10 ## [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 In addition, there are facile ways to extract information using a coonditional statement … x &lt;- 1:10 / 10 x &lt; .5 ## [1] TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE … the which() function returns the integer reference positions for the condition x &lt; 0.5 … which(x &lt; .5) ## [1] 1 2 3 4 … and since the output of that function is a vector, we can use it to reference the original vector to extract the elements in the vector that satisfy our condition x &lt; 0.5. x[which(x &lt; .5)] ## [1] 0.1 0.2 0.3 0.4 5.3 List In R programming, a ‘list’ is a powerful and flexible collection of objects of different types. It can contain vectors, matrices, data frames, and even other lists, making it an extremely versatile tool in data analysis, modeling, and visualization. With its ability to store multiple data types, a list can be used to represent complex structures such as a database table, where each column can be a vector or a matrix. Furthermore, a list can be used to store multiple models for model comparison, or to store a set of parameters for a simulation study. In addition to its flexibility, a list is also efficient, as it allows for fast and easy data retrieval. It can be used to store large datasets, and its hierarchical structure makes it easy to navigate and manipulate. Here’s an example of how to create a list in R: # create a list my_list &lt;- list(name = &quot;Janie R Programmer&quot;, age = 32, salary = 100000, interests = c(&quot;coding&quot;, &quot;reading&quot;, &quot;traveling&quot;)) print(my_list) ## $name ## [1] &quot;Janie R Programmer&quot; ## ## $age ## [1] 32 ## ## $salary ## [1] 1e+05 ## ## $interests ## [1] &quot;coding&quot; &quot;reading&quot; &quot;traveling&quot; In the above code, we have created a list ‘my_list’ with four elements, each having a different data type. The first element ‘name’ is a character vector, the second element ‘age’ is a numeric value, the third element ‘salary’ is also a numeric value, and the fourth element ‘interests’ is a character vector. We can access the elements of a list using the dollar sign ‘$’ or double brackets ‘[[]]’. For example: # accessing elements of a list print(my_list$name) ## [1] &quot;Janie R Programmer&quot; print(my_list[[&quot;salary&quot;]]) ## [1] 1e+05 Lists are also useful for storing and manipulating complex data structures such as data frames and tibbles. 5.4 Matrix Building upon the vector, a matrix is simply composed of columns of either all numeric or string vectors. That statement is not completely accurate as matrices can be row based, however, if we mentally orient ourselves to column based organizations, then the following data.frame will make sense. Matrices are constructed using a function as shown in the following example. # taking the vector 1:4 and distributing it by 2 rows and 2 columns m &lt;- matrix(1:4,2,2) Elements within the matrix have a reference schema similar to vectors, with the first integer in the square brackets is the row and the second the column [row,col]. ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 Just like a vector, a matrix can be used to compute operations on all elements simultaneously, apply a comparison and extract the variable(s) matching the condition … m_half &lt;- m / 2 w_point5 &lt;- which(m_half &gt; 1) m[w_point5] ## [1] 3 4 … or more sincintly. m[which(m/2 &gt; 1)] ## [1] 3 4 5.5 Data Frame Tables are one of the fundamental data structures encountered in data analysis, and what separates them from matrices is the mixed use of numerics and strings, and the orientation that data.frames are columns of vectors, with a row association. A table can be cinstructed with the data.frame() function as shown in the example. df &lt;- data.frame( let = LETTERS, pos = 1:length(LETTERS) ) ## let pos ## 1 A 1 ## 2 B 2 ## 3 C 3 ## 4 D 4 ## 5 E 5 ## ... Lets talk about the structure of what just happened in constructing the data.frame table. Note that we defined the column with let and pos referring to letter and position, respectively. Second, note the use of the single = to assign a vector to that column rather than the “out-of-function” assignment operator &lt;- – meaning that functions use the = assignment operator, while data structures use the &lt;- assignment operator. The printed output of the data.frame shows the two column headers and also prints out the row names, in this case the integer value. Now, that this table is organized by column with row assiciations, we can perform an evalutaion on one column and reterive the value(s) in the other. 5.6 Data Table A data.table is a package in R that provides an extension of data.frame. It is an optimized and efficient way of handling large datasets in R language. Data.table is widely used in data science as it provides fast and easy ways to analyze large datasets. It is built to handle large datasets with ease while still providing a simple and intuitive syntax. Some R packages build specifically for mass spectrometry utilize data.tables, however, its is easy to transform between object types and use the methods you are most comfortable with. PDF manual WEB tutorial CRAN GitHub The data.table object provides many advantages over the traditional data.frame. Some of the key advantages are as follows: Faster performance as compared to data.frame. Efficient memory usage. Provides an easy way to handle and manipulate large datasets. Provides a syntax similar to SQL for easy querying of data. library(data.table) tbl_let &lt;- data.table( let = LETTERS, pos = 1:length(LETTERS) ) ## let pos ## 1: A 1 ## 2: B 2 ## 3: C 3 ## 4: D 4 ## 5: E 5 ## ... ## 26: Z 26 ## let pos In addition, data.table also provides many functions for data manipulation and aggregation. Some of the commonly used functions are: .SD: Subset of Data.table. It is used to access the subset of data.table. .N: It is used to get the number of rows in a group. .SDcols: It is used to select columns to subset .SD. .GRP: It is used to get the group number of each row. 5.7 Tibbles A tibble is a modern data frame in R programming language. Tibble is a part of tidyverse package that provides an efficient and user-friendly way to work with data frames. Tibbles are similar to data frames, but they have better printing capabilities, and they are designed to never alter your data. Tibbles are created using the tibble() function. You can create a tibble by passing vectors, lists, or data frames to the tibble() function. Once created, you can manipulate the tibble using the dplyr package. library(tidyverse) tbl_let &lt;- tibble( let = LETTERS, pos = 1:length(LETTERS) ) ## # A tibble: 26 × 2 ## let pos ## &lt;chr&gt; &lt;int&gt; ## 1 A 1 ## 2 B 2 ## 3 C 3 ## 4 D 4 ## 5 E 5 ## 6 F 6 ## 7 G 7 ## 8 H 8 ## 9 I 9 ## 10 J 10 ## # … with 16 more rows ## # ℹ Use `print(n = ...)` to see more rows Tibbles have several advantages over data frames. They print only the first 10 rows and all the columns that fit on the screen. This makes it easier to view and work with large datasets. Tibbles also have better error messages, which makes debugging easier. Additionally, tibbles are more consistent in handling columns with different types of data. 5.8 Managing Objects 5.8.1 Examine the Contents You can use the str() function to peak inside any data object to see how it is structured. The contents of a data.frame: plant_data &lt;- data.frame( age_days = c(10, 20, 30, 40, 50, 60), height_inch = c(1.02, 1.10, 5.10, 6.00, 6.50, 6.90) ) str(plant_data) ## &#39;data.frame&#39;: 6 obs. of 2 variables: ## $ age_days : num 10 20 30 40 50 60 ## $ height_inch: num 1.02 1.1 5.1 6 6.5 6.9 The contents of a tibble is very similar: plant_data &lt;- data.table( age_days = c(10, 20, 30, 40, 50, 60), height_inch = c(1.02, 1.10, 5.10, 6.00, 6.50, 6.90) ) str(plant_data) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 6 obs. of 2 variables: ## $ age_days : num 10 20 30 40 50 60 ## $ height_inch: num 1.02 1.1 5.1 6 6.5 6.9 ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; The contents of a tibble is very similar: plant_data &lt;- tibble( age_days = c(10, 20, 30, 40, 50, 60), height_inch = c(1.02, 1.10, 5.10, 6.00, 6.50, 6.90) ) str(plant_data) ## tibble [6 × 2] (S3: tbl_df/tbl/data.frame) ## $ age_days : num [1:6] 10 20 30 40 50 60 ## $ height_inch: num [1:6] 1.02 1.1 5.1 6 6.5 6.9 The contents of a linear regression data object are quite different: # linear prediction of plant growth (eg. height) based on age linear_model &lt;- lm(data = plant_data, height_inch ~ age_days) linear_model ## ## Call: ## lm(formula = height_inch ~ age_days, data = plant_data) ## ## Coefficients: ## (Intercept) age_days ## -0.2133 0.1329 str(linear_model) List of 12 $ coefficients : Named num [1:2] -0.213 0.133 ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;(Intercept)&quot; &quot;age_days&quot; $ residuals : Named num [1:6] -0.0952 -1.3438 1.3276 0.899 0.0705 ... ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... $ effects : Named num [1:6] -10.868 5.558 1.296 0.602 -0.492 ... ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;(Intercept)&quot; &quot;age_days&quot; &quot;&quot; &quot;&quot; ... $ rank : int 2 $ fitted.values: Named num [1:6] 1.12 2.44 3.77 5.1 6.43 ... ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... 5.8.2 Converting Objects In R, we see that there are several data objects that can be used to store and manipulate data. Some of the commonly used data objects include data.frames, data.tables and tibbles. However, we don’t need to be stuck with any one object and can easily convert between these data objects using the as.data.frame, as.data.table and as_tibble functions. If we start out with a data.frame as shown above, we can convert that to either a data.table or a tibble very easily. library(tibble) library(data.table) Convert from a data.frame to a data.table: dt &lt;- as.data.table(df) str(dt) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 26 obs. of 2 variables: ## $ let: chr &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; ... ## $ pos: int 1 2 3 4 5 6 7 8 9 10 ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; Convert from a data.frame to a tibble: tbl &lt;- as_tibble(df) str(tbl) ## tibble [26 × 2] (S3: tbl_df/tbl/data.frame) ## $ let: chr [1:26] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; ... ## $ pos: int [1:26] 1 2 3 4 5 6 7 8 9 10 ... Convert from a data.table to a tibble: tbl &lt;- as_tibble(dt) str(tbl) ## tibble [26 × 2] (S3: tbl_df/tbl/data.frame) ## $ let: chr [1:26] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; ... ## $ pos: int [1:26] 1 2 3 4 5 6 7 8 9 10 ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; Exercises Create a new R Studio Project and name it 003_data_structures. Create a new R script, add your name and date at the top as comments. Construct the following vector and store as a variable. ## [1] &quot;red&quot; &quot;green&quot; &quot;blue&quot; Extract the 2nd element in the variable. ## [1] &quot;green&quot; Construct a numerical vector of length 5, containing the AREA of circles with integer RADIUS 1 to 5. Remember PEMDAS. ## [1] 3.141593 12.566372 28.274337 50.265488 78.539825 Extract all AREA greater than 50. ## [1] 50.26549 78.53982 Create a data.frame consisting of circles with integer RADIUS 1 to 5, and their AREA. ## radius area ## 1 1 3.141593 ## 2 2 12.566372 ## 3 3 28.274337 ## 4 4 50.265488 ## 5 5 78.539825 Extract all AREA greater than 50 from the data.frame. ## radius area ## 4 4 50.26549 ## 5 5 78.53982 More Exercises Create a new R Studio Project and name it 103_data_structures. Create a new R script, add your name and date at the top as comments. Exercise #1 – Working with Variables You are running an LC-MS experiment using a 60 min LC gradient 1.1 Create a variable called gradient_min to hold the length of the gradient in minutes. ## [1] 60 1.2 Using the gradient length variable you just created, convert it to seconds and assign it to a new variable with a meaningful name. ## [1] 3600 Exercise #2 – Working with Vectors Continuing from Exercise #1… 2.1 Imagine you conducted additional experiments, one with a 15 minute gradient and one with a 30 min gradient. Create a vector to hold all three gradient times in minutes, and assign it to a new variable. ## [1] 15 30 60 2.2 Convert the vector of gradient times to seconds. How does this conversion compare to how you did the conversion in Exercise 1? ## [1] 900 1800 3600 Exercise #3 – More Practice with Vectors 3.1 The following vector represents precursor m/z values for detected features from your experiment: prec_mz &lt;- c(968.4759, 812.1599, 887.9829, 338.5294, 510.2720, 775.3455, 409.2369, 944.0385, 584.7687, 1041.9523) How many values are there? ## [1] 10 What is the minimum value? The maximum? ## [1] 338.5294 ## [1] 1041.952 Exercise #4 – Vectors and Conditional Expressions 4.1 Using the above vector of precursor values, write a conditional expression to find the values with m/z &lt; 600. What is returned by this expression? A single value or multiple values? A number or something else? ## [1] FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE TRUE FALSE 4.2 Use this conditional expression to get the precursor values with m/z &lt; 600 ## [1] 338.5294 510.2720 409.2369 584.7687 4.3 Consider a new vector of data that contains the charge states of the same detected features from above: prec_z &lt;- c(2, 4, 2, 3, 2, 2, 2, 2, 2, 2) Write a conditional expression to find which detected features that have a charge state of 2. ## [1] TRUE FALSE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE 4.4 Write an expression to get the precursor m/z values for features having charge states of 2? ## [1] 968.4759 887.9829 510.2720 775.3455 409.2369 944.0385 584.7687 1041.9523 "],["tidyverse-1.html", "Chapter 6 Tidyverse 6.1 Core Packages 6.2 Importing 6.3 Wrangling 6.4 Data Types 6.5 Summarizing 6.6 Dplyr in more Detail 6.7 Dplyr Data Pipelines Exercises", " Chapter 6 Tidyverse The tidyverse is actually a collection of R packages designed for data analysis and visualization. It is an essential tool for data scientists and statisticians who work with large datasets. At the end of this chapter you should be able to Grasp the utility of the tidyverse. Understand how to construct a data pipeline. Composed a simple workflow. Figure 6.1: The tidyverse. The tidyverse packages are built around a common philosophy of data manipulation. The goal is to provide a consistent and intuitive syntax for data analysis that is easy to learn and use. The packages in the tidyverse include: 6.1.1 magrittr provides the pipe, %&gt;% used throughout the tidyverse. 6.1.2 tibble creates the main data object. 6.2.1 readr reading and writing data in various formats. 6.3.1 dplyr data manipulation. 6.3.2 tidyr transforming messy data into a tidy format. 6.3.3 purrr functional programming with vectors and lists. 6.4.1 stringr working with strings. 6.4.2 lubridate working with dates and date strings. 6.5.2 ggplot2 graphical plotting and data visualization. These packages work seamlessly together, allowing users to easily manipulate and visualize their data. The tidyverse also includes a set of conventions and best practices for data analysis, making it easy to follow a consistent workflow. Cheat-sheets ?? Consider the following workflow to read in data, calculate a linear regression and visualize the data using nine (9) of the underlying packages in the tidyverse. In this example, the goal is to read in climate data from Denver, Colorado and create a linear model relating the monthly amount of snowfall to the minimum temperature computed separately for each year in the data set. Additionally, plots of the data and the linear fit are produced to visualize the data. library(tidyverse) # readr, tibble: read in a table of comma separate values url &lt;- &quot;https://raw.githubusercontent.com/jeffsocal/ASMS_R_Basics/main/data/denver_climate.csv&quot; download.file(url, destfile = &quot;./data/denver_climate.csv&quot;) tbl_csv &lt;- read_csv(&quot;data/denver_climate.csv&quot;) # define a function to fit a linear regression model lm_func &lt;- function(data) { lm(snowfall ~ min_temp, data = data) } # readr, tibble, magrittr: using the data imported from above tbl_csv_lm &lt;- tbl_csv %&gt;% # dplyr group_by(year) %&gt;% # tidyr nest() %&gt;% # dplyr, purrr: apply the function to each nested data frame mutate(model = map(data, lm_func)) %&gt;% # dplyr, broom, purrr: extract the coefficients from each model mutate(tidy = map(model, broom::tidy)) %&gt;% # tidyr unnest(tidy) %&gt;% ungroup() %&gt;% # dplyr, stringr: clean-up the terms mutate(term = term %&gt;% str_replace_all(&quot;\\\\(|\\\\)&quot;, &quot;&quot;)) %&gt;% # dplyr: retain only specific columns select(year, term, estimate) %&gt;% # tidyr: convert from a long table to a wide table pivot_wider(names_from = &#39;term&#39;, values_from = &#39;estimate&#39;) %&gt;% # dplyr: rename the min_temp variable to slope rename(model_slope = min_temp) #ggplot2 ggplot(tbl_csv, aes(min_temp, snowfall)) + # represent the data as points geom_point() + # use the linear model data to plot regression lines geom_abline(data = tbl_csv_lm, aes(slope = model_slope, intercept = Intercept)) + # plot each year separately facet_wrap(~year) To get started with the tidyverse, you can install the package using the following command: install.packages(&quot;tidyverse&quot;) Once installed, users can load the package and begin using the individual packages within the tidyverse: library(tidyverse) Overall, the tidyverse is an essential tool for data analysis and visualization in R. Its user-friendly syntax and consistent conventions make it easy for data scientists and statisticians to work with large datasets. 6.1 Core Packages Two important packages in the tidyverse are tibble and magrittr. These core packages enable other data manipulation operations to work seamlessly, improving efficiency and ease of use when working with data in R. In practice, these packages are typically used “behind-the-scenes”: you may not realize you’re using them, but they provide the basis for working with data in the tidyverse. 6.1.1 magrittr The tidyverse package magrittr is a popular R package that provides a set of operators for chaining operations in a sequence, or pipeline. The package was developed by Stefan Milton Bache and Hadley Wickham. The main goal of magrittr is to make code more readable and easier to maintain by providing a chaining mechanism to link together individual data manipulation operations into data analysis pipelines. These pipelines are often much easier to construct and read compared to the same code written with base R. The pipe operator, %&gt;%, is the most famous operator provided by magrittr. It allows you to chain multiple operations without the need to use intermediate variables. The pipe operator takes an input, which could be the output of a previous function, and passes it as the first argument to the next function. This chaining of operations allows for more concise and readable code. Here is an example of how to use the pipe operator with magrittr: # create a vector of numbers numbers &lt;- c(1, 2, 3, 4, 5) # Compute the square root of the sum of the numbers # using base R sqrt(sum(numbers)) ## [1] 3.872983 # use the pipe operator to do the same thing # by chaining together the operations numbers %&gt;% sum() %&gt;% sqrt() ## [1] 3.872983 In this example, we create a vector of numbers and use the pipe operator to chain the sum() and sqrt() functions. The output of the sum() function is passed as the first argument to the sqrt() function. In this case, the base R version is actually quicker to type, but the order of operations are specified “inside-out” which is backwards from how we typically read. An advantage of pipe syntax is that the steps of the data processing proceed from left-to-right, just like how we read standard text. This become particularly helpful when performing more complex data analyses. Magrittr also provides other useful operators, such as the assignment pipe %&lt;&gt;%, which allows you to update a variable in place, and the tee operator %T&gt;%, which allows you to inspect the output of an operation without interrupting the chain. The New Pipe Operator: The R language has recently added a “native” pipe operator, |&gt;, that is part of base R. Newer R code may use |&gt; instead of %&gt;% but both do essentially the same thing. see this blog post for more information 6.1.2 tibble R tibble is a class of data frame in the R programming language. It is an improved alternative to the traditional data frame and is part of the tidyverse package. Tibbles are data frames with stricter requirements, and they provide a streamlined and more efficient way to work with data. One of the main advantages of tibbles is that they provide a cleaner and more consistent way to display data. Tibbles only show the first 10 rows and all the columns that fit on the screen, making it easier to work with large datasets. Additionally, tibbles automatically convert character vectors to factors, preventing common errors that can occur when working with data frames. Another important feature of tibbles is the way they handle column names. Tibbles will not allow spaces in column names, and they use backticks to reference columns with non-standard names. This makes it easier to work with datasets that have complex column names. Tibbles also provide a more consistent way to handle missing values. In data frames, missing values can be represented in different ways, such as NA, NaN, or NULL. Tibbles, on the other hand, only use NA to represent missing values, making it easier to work with missing data. 6.2 Importing 6.2.1 readr R readr is a package in the R language that is used to read structured data files into R. The package is an efficient and user-friendly toolkit that allows for the reading of different types of flat files such as CSV, TSV, and fixed-width files. It is part of the tidyverse collection of packages, which is popular among data scientists and statisticians. One of the key features of readr is its ability to quickly read data into R, making it an ideal package for data analysis and data cleaning. readr is designed to handle various types of data, including numeric, date, and character data. The package also has advanced features such as automatic guessing of column types, encoding detection, and parsing of dates and times. # read in data from various formats tbl_csv &lt;- &quot;data/some_data_set.csv&quot; %&gt;% read_csv() tbl_tsv &lt;- &quot;data/some_data_set.tsv&quot; %&gt;% read_tsv() # even read in compressed files tbl_csv &lt;- &quot;data/some_data_set.csv.zip&quot; %&gt;% read_csv() # read in data with some unconventional column separation by defining it literally tbl_weird &lt;- &quot;data/some_data_set.txt&quot; %&gt;% read_delim(delim = &quot;*&quot;) One of the best things about readr is its consistency in dealing with file formats, which allows for easy and fast data manipulation. The package provides a high level of control over the import process, allowing you to specify the location of the data file, the delimiter, and the encoding type. Additionally, readr can handle large datasets with ease, making it one of the most efficient packages for data handling. 6.3 Wrangling Data wrangling is the process of cleaning, transforming, and formatting raw data into a usable format for analysis. The steps involved in data wrangling include removing duplicates, dealing with missing or erroneous values, converting data types, and formatting data into a consistent structure. It also involves merging data from different sources, reshaping data, and transforming data for analysis. The objective of data wrangling is to create high-quality, structured data for further analysis and modeling. It requires technical skills, domain knowledge, and creativity. Without proper data wrangling, analysis and modeling may be compromised, leading to incorrect conclusions and decisions. This is where tidyverse functions become quite useful and we will go deeper into Data Wrangling in the subsiquent chapter. Given an example of wide data, where Arabidopsis thaliana plants are measured for height for three weeks post germination. library(tidyverse) tbl_wide &lt;- tibble( plant = LETTERS[1:3], condition = c(&#39;wet, cold&#39;, &#39;moist, cold&#39;, &#39;moist, hot&#39;), week_1 = c(0.3,0.2,0.4), week_2 = c(1.3,1.5,1.7), week_3 = c(3.4,4.1,5.2) ) tbl_wide ## # A tibble: 3 × 5 ## plant condition week_1 week_2 week_3 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A wet, cold 0.3 1.3 3.4 ## 2 B moist, cold 0.2 1.5 4.1 ## 3 C moist, hot 0.4 1.7 5.2 6.3.1 dplyr R dplyr is perhaps pne of the most powerful libraries in the tidyverse, providing a set of tools for data manipulation and transformation. It is designed to work seamlessly with data stored in data frames. The library comes with a set of functions that can be used to filter, arrange, group, mutate, and summarize data. These functions are optimized for speed and memory efficiency, allowing users to work with large datasets easily. Some of the most commonly used functions in dplyr are: filter: used to extract specific rows from a data frame based on certain conditions. tbl_wide %&gt;% filter(plant == &#39;A&#39;) ## # A tibble: 1 × 5 ## plant condition week_1 week_2 week_3 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A wet, cold 0.3 1.3 3.4 arrange: used to sort the rows of a data frame based on one or more columns. tbl_wide %&gt;% arrange(week_3) ## # A tibble: 3 × 5 ## plant condition week_1 week_2 week_3 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A wet, cold 0.3 1.3 3.4 ## 2 B moist, cold 0.2 1.5 4.1 ## 3 C moist, hot 0.4 1.7 5.2 select: used to select specific columns from a data frame. tbl_wide %&gt;% select(plant, week_3) ## # A tibble: 3 × 2 ## plant week_3 ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 3.4 ## 2 B 4.1 ## 3 C 5.2 mutate: used to add new columns to a data frame. tbl_wide %&gt;% mutate(week_4 = c(3.8, 4.6, 5.7)) ## # A tibble: 3 × 6 ## plant condition week_1 week_2 week_3 week_4 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A wet, cold 0.3 1.3 3.4 3.8 ## 2 B moist, cold 0.2 1.5 4.1 4.6 ## 3 C moist, hot 0.4 1.7 5.2 5.7 6.3.2 tidyr R tidyr is a package in R that helps to reshape data frames. It is an essential tool for data cleaning and analysis. Tidyr is used to convert data from wide to long format and vice versa, and it also helps to separate and unite columns. pivot_longer: used to reshape data from a column-based wide format to a row-based long format. tbl_long &lt;- tbl_wide %&gt;% pivot_longer(cols = matches(&#39;week&#39;), names_to = &#39;time&#39;, values_to = &#39;inches&#39;) tbl_long ## # A tibble: 9 × 4 ## plant condition time inches ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A wet, cold week_1 0.3 ## 2 A wet, cold week_2 1.3 ## 3 A wet, cold week_3 3.4 ## 4 B moist, cold week_1 0.2 ## 5 B moist, cold week_2 1.5 ## 6 B moist, cold week_3 4.1 ## 7 C moist, hot week_1 0.4 ## 8 C moist, hot week_2 1.7 ## 9 C moist, hot week_3 5.2 pivot_wider: used to reshape data from a row-based long format to a column-based wide format. tbl_long %&gt;% pivot_wider(names_from = &#39;time&#39;, values_from = &#39;inches&#39;) ## # A tibble: 3 × 5 ## plant condition week_1 week_2 week_3 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A wet, cold 0.3 1.3 3.4 ## 2 B moist, cold 0.2 1.5 4.1 ## 3 C moist, hot 0.4 1.7 5.2 The package tidyr also has functions to separate and unite columns. The “separate” function is used when you have a column that contains multiple variables. For example, if you have a column that contains both the first and last name of a person, you can separate them into two columns. The “unite” function is the opposite of separate. It is used when you want to combine two or more columns into one column. separate: used to separate a column with multiple values into two or more columns. tbl_long %&gt;% separate(condition, into = c(&#39;soil&#39;, &#39;temp&#39;)) ## # A tibble: 9 × 5 ## plant soil temp time inches ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A wet cold week_1 0.3 ## 2 A wet cold week_2 1.3 ## 3 A wet cold week_3 3.4 ## 4 B moist cold week_1 0.2 ## 5 B moist cold week_2 1.5 ## 6 B moist cold week_3 4.1 ## 7 C moist hot week_1 0.4 ## 8 C moist hot week_2 1.7 ## 9 C moist hot week_3 5.2 separate_rows: used to duplicate a row with multiple values from a given column. tbl_wide %&gt;% separate_rows(condition, sep = &#39;, &#39;) ## # A tibble: 6 × 5 ## plant condition week_1 week_2 week_3 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A wet 0.3 1.3 3.4 ## 2 A cold 0.3 1.3 3.4 ## 3 B moist 0.2 1.5 4.1 ## 4 B cold 0.2 1.5 4.1 ## 5 C moist 0.4 1.7 5.2 ## 6 C hot 0.4 1.7 5.2 6.3.3 purrr The purrr package is a functional programming toolkit for R that enables users to easily and rapidly apply a function to a set of inputs, returning a list or vector of outputs. It is designed to work seamlessly with the tidyverse ecosystem of packages, but can also be used with base R functions. The most important feature in purrr is its ability to replace loops with functions that save time and effort. The package has a collection of functions that allow you to work with functions that take one or more arguments. Some of these functions include map, map2, pmap, and imap. The map function is purrr’s flagship function and is used to apply a function to each element of a list or vector, returning a list of outputs. The map2 function applies a function to two lists or vectors in parallel, returning a list of outputs. The pmap function applies a function to an arbitrary number of lists or vectors in parallel, returning a list of outputs. The imap function is similar to map, but also provides the index of the current element in the input vector. Purrr also includes features such as the possibility of mapping over nested lists, using map and variants to iterate over grouped data, and using map and variants to modify data in place. numbers &lt;- list(1, 2, 3, 4, 5) # define a function to square a number square &lt;- function(x) { x ^ 2 } # use map to apply the function to each element of the list squared_numbers &lt;- map(numbers, square) # print the result squared_numbers ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 4 ## ## [[3]] ## [1] 9 ## ## [[4]] ## [1] 16 ## ## [[5]] ## [1] 25 6.3.4 glue R glue is a tidyverse package that provides a simple way to interpolate values into strings. It allows users to combine multiple strings or variables together into a single string with minimum efforts, simpler than using base R fuctions. The glue function can handle various types of inputs, including vectors, lists, and expressions. It also supports user-defined formats and allows users to specify separators between the values. One of the significant advantages of using glue is that it provides a more readable and concise way to create strings in R. It eliminates the need for multiple paste() or paste0() statements, which can be cumbersome and error-prone. For example, instead of writing: paste0(&quot;The value of x is: &quot;, x, &quot;, and the value of y is: &quot;, y) we can use R glue: glue(&quot;The value of x is: {x}, and the value of y is: {y}&quot;) This code will produce the same output, but it’s more readable and easier to modify. 6.4 Data Types 6.4.1 stringr The tidyverse package stringr provides a cohesive set of functions designed to make working with strings more efficient. It is especially useful when dealing with messy or unstructured data that needs to be cleaned and transformed into a more structured format. Several functions in stringr provides methods working with strings, for example: str_replace: replaces a pattern with another pattern in a string. str_replace(&quot;Hello World&quot;, &quot;W.+&quot;, &quot;Everyone&quot;) ## [1] &quot;Hello Everyone&quot; str_extract: extracts the first occurrence of a pattern from a string. str_extract(&quot;Hello World&quot;, &quot;W.+&quot;) ## [1] &quot;World&quot; str_split: splits a string into pieces based on a specified pattern. str_split(&quot;Hello World&quot;, &quot;\\\\s&quot;) ## [[1]] ## [1] &quot;Hello&quot; &quot;World&quot; 6.4.2 lubridate The tidyverse package lubridate helps with the handling of dates and times. The package has several functions that make it easier to work with dates and times, especially when dealing with data that has different formats. Some of the functions in lubridate package include: ymd - this is used to convert dates in the format of year, month, and day to the date format in R. For example, ymd(\"20220101\") will return the date in R format. dmy - this is used to convert dates in the format of day, month, and year to the date format in R. For example, dmy(\"01-01-2022\") will return the date in R format. hms - this is used to convert time in the format of hours, minutes, and seconds to the time format in R. For example, hms(\"12:30:15\") will return the time in R format. ymd_hms - this is used to convert dates and times in the format of year, month, day, hours, minutes, and seconds to the date and time format in R. For example, ymd_hms(\"2022-01-01 12:30:15\") will return the date and time in R format. There are also functions for extracting information from dates and times such as year(), month(), day(), hour(), minute(), and second(). 6.4.3 forcats R forcats is a tidyverse package that provides a set of tools for working with categorical data. It is designed to make it easier to work with factors in R, which are used to represent categorical data. The forcats package provides several functions that can be used to manipulate factors, including reordering levels, combining levels, and handling missing values. It also provides functions for working with ordered factors, which are used to represent data that has a natural ordering, such as age groups or ratings. One of the key benefits of using forcats is that it allows you to easily visualize and analyze categorical data. The package provides functions for creating categorical plots, such as bar charts and pie charts, as well as for calculating summary statistics for categorical data. In addition to its core functionality, forcats is also highly customizable. It provides a wide range of options for controlling the appearance of plots and for customizing the behavior of factor manipulation functions. 6.5 Summarizing 6.5.1 dplyr In the R tidyverse package, summarizing data is a common task performed on data frames. The dplyr package provides a set of functions that makes it easy to summarize data based on one or more variables. group_by: used to group rows of a data frame by one or more columns. summarize: used to summarize the data based on one or more aggregate functions. The summarise() function is used to perform simple summary statistics on data frames. It takes the name of the new variable as well as the summary function that should be used to calculate its value. For example, to calculate the mean and standard deviation of a variable named ‘x’ in a data frame named ‘df’, we can use the following code: tbl_long %&gt;% summarise(min = min(inches), max = max(inches)) ## # A tibble: 1 × 2 ## min max ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.2 5.2 The group_by() function is used to group data frames by one or more variables. This is useful when we want to summarize data by different categories. For example, to calculate the mean and standard deviation of ‘x’ by ‘group’, we can use the following code: tbl_long %&gt;% group_by(plant) %&gt;% summarise(min = min(inches), max = max(inches)) ## # A tibble: 3 × 3 ## plant min max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 0.3 3.4 ## 2 B 0.2 4.1 ## 3 C 0.4 5.2 The summarize_at() and summarize_all() functions are used to perform summary statistics on multiple variables at once. The summarize_at() function takes a list of variables to summarize, while the summarize_all() function summarizes all variables in the data frame. For example, to calculate the mean and standard deviation of all numeric variables in a data frame named ‘df’, we can use the following code: tbl_long %&gt;% summarise_all(list(max = max, min = min)) ## # A tibble: 1 × 8 ## plant_max condition_max time_max inches_max plant_min condition_min time_min inches_min ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 C wet, cold week_3 5.2 A moist, cold week_1 0.2 Summarizing data is an essential task that can be performed using several functions. These functions make it easy to calculate summary statistics based on one or more variables, group data frames by different categories, and summarize multiple variables at once. 6.5.2 ggplot2 The tidyverse package ggplot2, demonstrated at the onset of this chapter, is a data visualization package in R programming language that provides a flexible and powerful framework for creating graphs and charts. It is built on the grammar of graphics, which is a systematic way of mapping data to visual elements like points, lines, and bars. With ggplot2, you can create a wide range of graphs including scatterplots, bar charts, line charts, and more. The package offers a variety of customization options, such as color schemes, themes, and annotations, allowing you to create professional-looking visualizations with ease. One of the key benefits of ggplot2 is that it allows you to quickly explore and analyze your data visually. You can easily create multiple graphs with different variables and subsets of your data, and compare them side by side to identify patterns and trends. 6.6 Dplyr in more Detail Figure 6.2: dplyr hex logo. The dplyr package is the backbone of the tidyverse and defines the fundamental data operations that are needed to perform data analysis tasks. Specifically, dplyr formalizes these fundamental operations into a set of “verbs” or actions that are used to manipulate/transform/summarize data. These verbs are embodied as functions you can use to perform complex data analysis tasks. Fortunately, there is a relatively small set of functions that you need to learn and remember, and their names clearly reflect what they are used for. Ultimately, these dplyr functions help you focus on the question you’re trying to answer rather than the mechanism of how to answer the question. There are six fundamental verbs (functions) provided by the dplyr package: mutate: adds a new (or updates an existing) column to the input data, often based upon exisiting columns in the input data select: picks specific columns from the input data (i.e. filters to specific columns) filter: subsets the input data to specific rows, usually based upon a filtering condition arrange: reorders the input data based upon the data in specified columns group_by: creates subset groups of the input data using columns with group information in the input data summarize: compute summaries across multiple rows in your data; often used with group_by Figure 6.3: Overview of dplyr verbs. Importantly, the first input to a dplyr function is always a data table (e.g. a tibble) and the ouput is always a new data table that has been transformed by the function. For example: # add a new column to my_df mutate(my_df, ...) # subset my_df to specific columns select(my_df, ...) # dplyr functions always return a transformed data table # you typically want to save the results to a new variable # or update the existing version of the table my_df2 &lt;- mutate(my_df, ...) 6.6.1 mutate The mutate function is used when you want to add a new column or update an existing column in your data table. Typically, the new or updated column is based upon existing data already present in your data table and you use it as the basis for the new column you want to create/update. Some examples include: computing the log transformed version of existing data adding together the values from multiple columns subtracting off a baseline value from existing measurements The following code provides a common example of how mutate can be used: # Read the data set url &lt;- &quot;https://raw.githubusercontent.com/jeffsocal/ASMS_R_Basics/main/data/bacterial-metabolites_dose-simicillin_tidy.csv&quot; download.file(url, destfile = &quot;./data/bacterial-metabolites_dose-simicillin_tidy.csv&quot;) # read in and examine the data dat &lt;- read_csv(&quot;data/bacterial-metabolites_dose-simicillin_tidy.csv&quot;) ## Rows: 180 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Organism, Metabolite ## dbl (3): Dose_mg, Time_min, Abundance ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. dat ## # A tibble: 180 × 5 ## Organism Dose_mg Metabolite Time_min Abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli 0 glutamate 0 191666 ## 2 e coli 0 glutamate 10 2082823 ## 3 e coli 0 glutamate 20 1796910 ## 4 e coli 0 glutamate 50 846402 ## 5 e coli 0 glutamate 120 1782879 ## 6 staph aureus 0 glutamate 0 125210 ## 7 staph aureus 0 glutamate 10 132156 ## 8 staph aureus 0 glutamate 20 114445 ## 9 staph aureus 0 glutamate 50 117150 ## 10 staph aureus 0 glutamate 120 137747 ## # ℹ 170 more rows # Add a new column that is the log10 value of the Abundance column dat &lt;- mutate(dat, log10_Abundance = log10(Abundance)) dat ## # A tibble: 180 × 6 ## Organism Dose_mg Metabolite Time_min Abundance log10_Abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli 0 glutamate 0 191666 5.28 ## 2 e coli 0 glutamate 10 2082823 6.32 ## 3 e coli 0 glutamate 20 1796910 6.25 ## 4 e coli 0 glutamate 50 846402 5.93 ## 5 e coli 0 glutamate 120 1782879 6.25 ## 6 staph aureus 0 glutamate 0 125210 5.10 ## 7 staph aureus 0 glutamate 10 132156 5.12 ## 8 staph aureus 0 glutamate 20 114445 5.06 ## 9 staph aureus 0 glutamate 50 117150 5.07 ## 10 staph aureus 0 glutamate 120 137747 5.14 ## # ℹ 170 more rows In this example, dat is data table and used as input to the mutate function. Inside the mutate function, the new column to create is given a name, log10_Abundance, followed by an = and the expression that will be used to compute the data in the new column. Importantly, this expression uses existing data in our input data table specified by its column name, Abundance. Note that there are no quotation marks around Abundance; when using dplyr functions, you don’t put quotes around the name of columns in the input data. 6.6.2 select The select function is used when you want to focus on specific columns in your data table. Sometimes, a data table may contain many columns of data, but you may only need a small subset of them for a given analysis task. In this case, using select removes the unnecessary columns from the data table and allows you to more easily focus on what you need. # Focus on just the Organism, Time_min and Abundance columns dat_slct &lt;- select(dat, Organism, Time_min, Abundance) dat_slct ## # A tibble: 180 × 3 ## Organism Time_min Abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli 0 191666 ## 2 e coli 10 2082823 ## 3 e coli 20 1796910 ## 4 e coli 50 846402 ## 5 e coli 120 1782879 ## 6 staph aureus 0 125210 ## 7 staph aureus 10 132156 ## 8 staph aureus 20 114445 ## 9 staph aureus 50 117150 ## 10 staph aureus 120 137747 ## # ℹ 170 more rows Here, the select function just takes the names of the columns you want to subset to as input. Like in the mutate example above, you do not put quotes around the names of the columns. 6.6.3 filter The filter function is used when you need to get or remove specific rows from your data table, almost always as specified by a conditional expression. The filter function uses the supplied condition expression, evaluates the condition on each row in your data table, and only gives back rows for which the condition is true. Keeping versus removing rows depends on how the conditional expression is constructed. For example, if you want to keep rows based on a given condition, you might use the == operator, while if you want to remove rows, you might use the != operator. Some examples where filter is used include: removing negative or missing values (NA) from your data table getting only the “disease” samples from your data table subsetting the data to m/z values greater than 1000 # Get just the e coli data dat_flt &lt;- filter(dat, Organism == &quot;e coli&quot;) dat_flt ## # A tibble: 60 × 6 ## Organism Dose_mg Metabolite Time_min Abundance log10_Abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli 0 glutamate 0 191666 5.28 ## 2 e coli 0 glutamate 10 2082823 6.32 ## 3 e coli 0 glutamate 20 1796910 6.25 ## 4 e coli 0 glutamate 50 846402 5.93 ## 5 e coli 0 glutamate 120 1782879 6.25 ## 6 e coli 10 glutamate 0 1445 3.16 ## 7 e coli 10 glutamate 10 1017467 6.01 ## 8 e coli 10 glutamate 20 14456533 7.16 ## 9 e coli 10 glutamate 50 813321 5.91 ## 10 e coli 10 glutamate 120 29825 4.47 ## # ℹ 50 more rows # Remove any data where the Abundance is less than 10 dat_flt &lt;- filter(dat, Abundance &gt;= 10) dat_flt ## # A tibble: 176 × 6 ## Organism Dose_mg Metabolite Time_min Abundance log10_Abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli 0 glutamate 0 191666 5.28 ## 2 e coli 0 glutamate 10 2082823 6.32 ## 3 e coli 0 glutamate 20 1796910 6.25 ## 4 e coli 0 glutamate 50 846402 5.93 ## 5 e coli 0 glutamate 120 1782879 6.25 ## 6 staph aureus 0 glutamate 0 125210 5.10 ## 7 staph aureus 0 glutamate 10 132156 5.12 ## 8 staph aureus 0 glutamate 20 114445 5.06 ## 9 staph aureus 0 glutamate 50 117150 5.07 ## 10 staph aureus 0 glutamate 120 137747 5.14 ## # ℹ 166 more rows In the first example, we use the conditional expression Organism == \"e coli\" as the filtering criteria. Here, Organism in the name of a column in our input data (note: no quotes around the column name) and the conditional expression asks the question “is Organism equal to ‘e coli’?” for every row in the data table. If the answer is yes, that row is kept in the output; otherwise it is removed. The second example is similar but uses the &gt;= operator to ask questions about the Abundance values, only keeping those rows where the Abundance is greater than or equal to 10. 6.6.4 arrange The arrange function is used to reorder the rows in your data table, most often, by sorting based upon one or more of the columns. Some examples include: sorting your table from low to high values sorting your table first by a group column, then by values within each group # sort the table by Abundance (low to high) dat &lt;- arrange(dat, Abundance) dat ## # A tibble: 180 × 6 ## Organism Dose_mg Metabolite Time_min Abundance log10_Abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 staph aureus 20 succinic acid 120 1 0 ## 2 staph aureus 20 phosphatidylcholine 120 1 0 ## 3 p aeruginosa 20 succinic acid 120 6 0.778 ## 4 e coli 20 phosphatidylcholine 120 9 0.954 ## 5 p aeruginosa 20 phosphatidylcholine 120 10 1 ## 6 p aeruginosa 0 lysine 0 15 1.18 ## 7 e coli 20 succinic acid 120 16 1.20 ## 8 p aeruginosa 10 glutamate 0 23 1.36 ## 9 p aeruginosa 20 succinic acid 50 80 1.90 ## 10 staph aureus 0 lysine 20 109 2.04 ## # ℹ 170 more rows # sort the table by Abundance (high to low) # desc is a function that says to use descending order dat &lt;- arrange(dat, desc(Abundance)) dat ## # A tibble: 180 × 6 ## Organism Dose_mg Metabolite Time_min Abundance log10_Abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 p aeruginosa 10 phosphatidylcholine 0 326068174 8.51 ## 2 e coli 0 phosphatidylcholine 10 196542536 8.29 ## 3 e coli 20 phosphatidylcholine 0 183210138 8.26 ## 4 e coli 0 phosphatidylcholine 120 175507910 8.24 ## 5 p aeruginosa 0 phosphatidylcholine 50 173720561 8.24 ## 6 p aeruginosa 0 phosphatidylcholine 20 156286340 8.19 ## 7 p aeruginosa 0 phosphatidylcholine 10 144491775 8.16 ## 8 e coli 10 phosphatidylcholine 0 128103670 8.11 ## 9 e coli 0 phosphatidylcholine 20 121918323 8.09 ## 10 p aeruginosa 0 phosphatidylcholine 120 120335903 8.08 ## # ℹ 170 more rows # sort the table by Organism, then by Abundance (high to low) dat &lt;- arrange(dat, Organism, desc(Abundance)) dat ## # A tibble: 180 × 6 ## Organism Dose_mg Metabolite Time_min Abundance log10_Abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli 0 phosphatidylcholine 10 196542536 8.29 ## 2 e coli 20 phosphatidylcholine 0 183210138 8.26 ## 3 e coli 0 phosphatidylcholine 120 175507910 8.24 ## 4 e coli 10 phosphatidylcholine 0 128103670 8.11 ## 5 e coli 0 phosphatidylcholine 20 121918323 8.09 ## 6 e coli 0 phosphatidylcholine 50 110535732 8.04 ## 7 e coli 0 phosphatidylcholine 0 84588958 7.93 ## 8 e coli 20 lysine 50 21482392 7.33 ## 9 e coli 10 succinic acid 20 20374016 7.31 ## 10 e coli 10 glutamate 20 14456533 7.16 ## # ℹ 170 more rows In these examples, column names in the input data table are used to sort the table. When you need to perform sorting within multiple groups, you just supply the column names in the order you want to perform the sorting. By default, sorting is performed from “low to high”: for numbers, numerical sorting is used, and for characters, alphabetical sorting is used. The desc function can be used to reverse the sorting from “high to low”. 6.6.5 group_by + summarize While group_by can be used with the other dplyr functions, it is most commonly used in combination with summarize in order to compute summarizations or aggregations of data within subgroups of your data. By itself, group_by doesn’t outwardly do anything to your data table. Rather, it tells R to get ready to work on subsets of your data individually for each subgroup. As such, group_by is always used in combination with another dplyr function. You can think of the group_by function as slicing apart your input data table into individual smaller tables based on a supplied grouping column so that you can perform further data manipulations on each of these sub-tables individually. Some examples of when you might use group_by + summarize include: computing the average abundance for each analyte in your experiment finding the max retention time for each peptide across runs computing intensity CVs for each analyte (measured with replicates) # group the data by Organism dat_grp &lt;- group_by(dat, Organism) dat_grp ## # A tibble: 180 × 6 ## # Groups: Organism [3] ## Organism Dose_mg Metabolite Time_min Abundance log10_Abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli 0 phosphatidylcholine 10 196542536 8.29 ## 2 e coli 20 phosphatidylcholine 0 183210138 8.26 ## 3 e coli 0 phosphatidylcholine 120 175507910 8.24 ## 4 e coli 10 phosphatidylcholine 0 128103670 8.11 ## 5 e coli 0 phosphatidylcholine 20 121918323 8.09 ## 6 e coli 0 phosphatidylcholine 50 110535732 8.04 ## 7 e coli 0 phosphatidylcholine 0 84588958 7.93 ## 8 e coli 20 lysine 50 21482392 7.33 ## 9 e coli 10 succinic acid 20 20374016 7.31 ## 10 e coli 10 glutamate 20 14456533 7.16 ## # ℹ 170 more rows # compute the median abundance for each organism dat_smry &lt;- summarize(dat_grp, median_Abundance = median(Abundance)) dat_smry ## # A tibble: 3 × 2 ## Organism median_Abundance ## &lt;chr&gt; &lt;dbl&gt; ## 1 e coli 190909 ## 2 p aeruginosa 242584. ## 3 staph aureus 57954 In this first example, the data table was first grouped by Organism, then summarize was used on this grouped data table to compute the median Abundance value for each group. The expression, median_Abundance = median(Abundance) is very similar to the type of expression used in the mutate example above. Here, we first supply the name of the new column, median_Abundance, that will be created to hold the summarized median Abundance values computed for each Organism. Then we specify the expression to be performed on each Organism group, in this case, using the median function. Because the input table has data for 3 different organisms, the resulting table returned from summarize has 3 rows, one row for each organsim. Furthermore, the output table contains the new column with the summarized abundance values. It is also possible to group by more than one column, as shown below: # You can also group by multiple columns dat_grp &lt;- group_by(dat, Organism, Dose_mg) dat_smry &lt;- summarize(dat_grp, median_Abundance = median(Abundance)) ## `summarise()` has grouped output by &#39;Organism&#39;. You can override using the `.groups` ## argument. dat_smry ## # A tibble: 9 × 3 ## # Groups: Organism [3] ## Organism Dose_mg median_Abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli 0 1541934. ## 2 e coli 10 201385 ## 3 e coli 20 90985 ## 4 p aeruginosa 0 886166 ## 5 p aeruginosa 10 214703 ## 6 p aeruginosa 20 84444. ## 7 staph aureus 0 120692. ## 8 staph aureus 10 13202. ## 9 staph aureus 20 11184 Here, you simply supply additional grouping column names in the group_by function. In this case, we use both Organism and Dose_mg to create the subgroups. Interestingly, the summarize code is exactly the same as before, but since the input grouped data has been group by two columns, the summarized output is different and based upon this new grouping. Now, the results show the median Abundance values for each Organism and Dose_mg pair. Since there are 3 organisms and 3 dose levels in the input data, the output summarized data has 3 x 3 = 9 rows. 6.7 Dplyr Data Pipelines Building on the dplyr fundamentals covered above, the next step is learn how to these data manipulation verbs can be effectively used to create data analysis pipelines. First, we need to revisit the pipe operator from above, %&gt;%. Recall that the pipe operator takes an input (on the left side) and passes it as the first argument to a function (on the right side. In practice, the pipe operator just allows us to write R code in a slightly different way, but this new way is really useful for making data analysis pipelines. The following figure shows specifically what the pipe operator does: Figure 6.4: Base R syntax vs. pipeline syntax. Both of these lines of code do exactly the same thing, but are written slightly differently. The pipe syntax allows you to pull out the first argument of a function, move it to the front before the %&gt;%, and then specify the function with the rest of needed arguments. Next, we can combine this syntax with one of the fundamental properties of the dplyr functions from above: the first input to all dplyr verb functions is a data table, and the ouput is another data table as transformed by the verb function. Figure 6.5: dplyr verb function used with the pipe operator. This pattern can be though of as a data manipulation template. Relating this back to the specific dplyr functions from above, this template can be applied as follows: Figure 6.6: Pipeline template applied with different dplyr verbs. Because we know both the input and output for a dplyr verb is a data frame, the pipeline operator allows us to chain together multiple data manipulations into a data analysis pipeline: Figure 6.7: dplyr verbs chained together in a dplyr pipeline. Let’s now make this more concrete with specific examples. Using the metabolite data used in the examples above, imagine you need to compute the mean log10 abundance for metabolite and dose, separately for each organism. Furthermore, you’ve found that the data at the last timepoint (120 min) are not reliable and should be removed from the calculations. Also, since you are most interested in the results with the highest mean abundance values, it would be useful to sort the output in descending mean abundance order. Fortunatley, it is fairly straightforward to translate this task to a dplyr pipeline. As a beginner, a great place to start is to translate the problem into step-by-step pieces that you intend to perform: Read the data into R Filter out data from the 120 min time point Compute the log10 abundance values Group the data by organism, does, and metabolite Summarize the data by computing the3 mean log10 abundance values for each group Arrange the resulting data by mean abundance in descending order With this workflow in mind, each step can be translated into a dplyr pipeline: # Make the analysis pipeline dat_smry &lt;- dat %&gt;% # 2. Filter out data from the 120 min time point filter(Time_min != 120) %&gt;% # 3. Compute the log10 abundance values mutate(log10_Abundance = log10(Abundance)) %&gt;% # 4. Group the data by organism, dose, and metabolite group_by(Organism, Dose_mg, Metabolite) %&gt;% # 5. Summarize the data by computing the3 mean log10 abundance values for each group summarize(mean_log10_Abundance = mean(log10_Abundance)) %&gt;% # 6. Arrange the resulting data by mean abundance in descending order arrange(desc(mean_log10_Abundance)) %&gt;% # optional but advised step to ungroup the data ungroup() ## `summarise()` has grouped output by &#39;Organism&#39;, &#39;Dose_mg&#39;. You can override using the ## `.groups` argument. dat_smry ## # A tibble: 36 × 4 ## Organism Dose_mg Metabolite mean_log10_Abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 p aeruginosa 0 phosphatidylcholine 8.14 ## 2 e coli 0 phosphatidylcholine 8.09 ## 3 staph aureus 0 phosphatidylcholine 7.11 ## 4 e coli 10 phosphatidylcholine 6.43 ## 5 p aeruginosa 10 phosphatidylcholine 6.42 ## 6 p aeruginosa 0 succinic acid 6.24 ## 7 p aeruginosa 10 succinic acid 6.21 ## 8 e coli 0 succinic acid 6.10 ## 9 e coli 10 succinic acid 5.98 ## 10 e coli 0 glutamate 5.95 ## # ℹ 26 more rows Exercises Create a new R Studio Project and name it 004_tidyverse. Create a new R script, add your name and date at the top as comments. Download the bacterial-metabolites_dose-simicillin_tidy.csv data set. url &lt;- &quot;https://raw.githubusercontent.com/jeffsocal/ASMS_R_Basics/main/data/bacterial-metabolites_dose-simicillin_tidy.csv&quot; download.file(url, destfile = &quot;./data/bacterial-metabolites_dose-simicillin_tidy.csv&quot;) Read in the bacterial-metabolites_dose-simicillin_tidy.csv data set. library(tidyverse) dat &lt;- read_csv(&quot;data/bacterial-metabolites_dose-simicillin_tidy.csv&quot;) ## Rows: 180 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Organism, Metabolite ## dbl (3): Dose_mg, Time_min, Abundance ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. How many organisms, metabolites, dose levels, and time points are in the data? How many rows are in the data table? What is the overall study design? ## [1] 180 ## [1] TRUE Which metabolite has the highest overall mean abundance? ## # A tibble: 4 × 2 ## Metabolite mean_abundance ## &lt;chr&gt; &lt;dbl&gt; ## 1 phosphatidylcholine 50221415. ## 2 succinic acid 1855206. ## 3 glutamate 1817320. ## 4 lysine 1349963. Does this metabolite have the highest mean abundance for each organism, or is there differences between organisms? ## `summarise()` has grouped output by &#39;Organism&#39;. You can override using the `.groups` ## argument. ## # A tibble: 12 × 3 ## # Groups: Organism [3] ## Organism Metabolite mean_abundance ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 e coli phosphatidylcholine 67749754. ## 2 e coli glutamate 2323045. ## 3 e coli succinic acid 1906850. ## 4 e coli lysine 1519827. ## 5 p aeruginosa phosphatidylcholine 77078132. ## 6 p aeruginosa succinic acid 3525432. ## 7 p aeruginosa glutamate 2902178 ## 8 p aeruginosa lysine 2336119. ## 9 staph aureus phosphatidylcholine 5836359. ## 10 staph aureus glutamate 226736. ## 11 staph aureus lysine 193944. ## 12 staph aureus succinic acid 133337. Is there an overall trend of mean abundance values vs. time point? What about abundance vs. dose? ## # A tibble: 5 × 2 ## Time_min mean_abundance ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 26639946. ## 2 10 11726915. ## 3 20 11929517. ## 4 50 9389061. ## 5 120 9369441. ## # A tibble: 3 × 2 ## Dose_mg mean_abundance ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 24454975. ## 2 10 9900030. ## 3 20 7077924. Using the example code at the beginning of this Chapter (using the Dever climate data), compute a linear fit of log10 abundance vs. time point for each metabolite and plot the results. "],["data-wrangling.html", "Chapter 7 Data Wrangling 7.1 Tidy Data 7.2 Example Walkthrough Exercises", " Chapter 7 Data Wrangling In data analysis, having known and expected patterns in the data is important. Wrangling is the act of getting messy real world data organized for analysis. Figure 7.1: Visualizing messy data. At the end of this chapter you should be able to Understand concept of tidy data. Wrangle some messy data into a tidier form. 7.1 Tidy Data Tidy data is a concept in data science and statistics that refers to a specific way of organizing data into a tabular format that is easy to work with. The concept of tidy data was introduced by Hadley Wickham in his paper “Tidy Data” published in the Journal of Statistical Software in 2014. In tidy data, each row represents a single observation or record, and each column represents a single variable or attribute. Additionally, there should be no duplicate columns or rows, and each cell should contain a single value. More specifically, tidy data should have the following properties: Each variable has its own column: In tidy data, each variable or attribute should have its own column. This makes it easy to compare and analyze different variables. Each observation has its own row: In tidy data, each observation or record should have its own row. This makes it easy to perform calculations on individual observations or groups of observations. There should be no duplicate columns or rows: In tidy data, there should be no duplicate columns or rows. This ensures that the data is organized in a clear and concise manner. Each cell should contain a single value: In tidy data, each cell should contain a single value. This makes it easy to perform calculations and manipulate the data. By organizing data in a tidy format, it becomes easier to analyze and visualize the data using tools like R and Python. Additionally, tidy data is more robust and less prone to errors than other types of data formats, making it easier to work with and share with others. 1.2.1 Online Cheat-sheets Tidy data visualized: Figure 7.2: Visualizing tidy data concepts. 7.2 Example Walkthrough We have a processed MS data set of metabolites measured for different bacteria at different time-points with different dosages of an antibiotic. Figure 7.3: Checking our messy data table. Having a look at the data, we can see definitely that it is not tidy. Our goal will be to: * Read the data (in wide, untidy format) * In a single dplyr pipe create a tidy with culture, dose_mg, metabolite, time_min, abundance as our columns. read_ Download the data … url &lt;- &quot;https://raw.githubusercontent.com/jeffsocal/ASMS_R_Basics/main/data/bacterial-metabolites_dose-simicillin_wide.csv&quot; download.file(url, destfile = &quot;./data/bacterial-metabolites_dose-simicillin_wide.csv&quot;) Let’s start with the read-in. library(tidyverse) tbl_met &lt;- read_csv(&quot;data/bacterial-metabolites_dose-simicillin_wide.csv&quot;) tbl_met ## # A tibble: 9 × 21 ## Culture glutamate_Time_0min glutamate_Time_10min glutamate_Time_20min glutamate_Time_50min ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli … 191666 2082823 1796910 846402 ## 2 staph a… 125210 132156 114445 117150 ## 3 p aerug… 978126 397012 267599 424183 ## 4 e coli … 1445 1017467 14456533 813321 ## 5 staph a… 152 128683 1360154 102188 ## 6 p aerug… 23 1612622 18282386 199374 ## 7 e coli … 885 42356 11758951 22498 ## 8 staph a… 136 4276 1162855 4841 ## 9 p aerug… 3098 68146 19333030 106262 ## # ℹ 16 more variables: glutamate_Time_2hr &lt;dbl&gt;, lysine_Time_0min &lt;dbl&gt;, ## # lysine_Time_10min &lt;dbl&gt;, lysine_Time_20min &lt;dbl&gt;, lysine_Time_50min &lt;dbl&gt;, ## # lysine_Time_2hr &lt;dbl&gt;, `succinic acid_Time_0min` &lt;dbl&gt;, ## # `succinic acid_Time_10min` &lt;dbl&gt;, `succinic acid_Time_20min` &lt;dbl&gt;, ## # `succinic acid_Time_50min` &lt;dbl&gt;, `succinic acid_Time_2hr` &lt;dbl&gt;, ## # phosphatidylcholine_Time_0min &lt;dbl&gt;, phosphatidylcholine_Time_10min &lt;dbl&gt;, ## # phosphatidylcholine_Time_20min &lt;dbl&gt;, phosphatidylcholine_Time_50min &lt;dbl&gt;, … separate Let’s separate Culture into two new variables: Species and Dose. Figure 7.4: Separating columns in a tibble. tbl_met %&gt;% separate(Culture, c(&quot;culture&quot;, &quot;dose_mg&quot;), sep = &quot; dose:&quot;) ## # A tibble: 9 × 22 ## culture dose_mg glutamate_Time_0min glutamate_Time_10min glutamate_Time_20min ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli 0mg 191666 2082823 1796910 ## 2 staph aureus 0mg 125210 132156 114445 ## 3 p aeruginosa 0mg 978126 397012 267599 ## 4 e coli 10mg 1445 1017467 14456533 ## 5 staph aureus 10mg 152 128683 1360154 ## 6 p aeruginosa 10mg 23 1612622 18282386 ## 7 e coli 20mg 885 42356 11758951 ## 8 staph aureus 20mg 136 4276 1162855 ## 9 p aeruginosa 20mg 3098 68146 19333030 ## # ℹ 17 more variables: glutamate_Time_50min &lt;dbl&gt;, glutamate_Time_2hr &lt;dbl&gt;, ## # lysine_Time_0min &lt;dbl&gt;, lysine_Time_10min &lt;dbl&gt;, lysine_Time_20min &lt;dbl&gt;, ## # lysine_Time_50min &lt;dbl&gt;, lysine_Time_2hr &lt;dbl&gt;, `succinic acid_Time_0min` &lt;dbl&gt;, ## # `succinic acid_Time_10min` &lt;dbl&gt;, `succinic acid_Time_20min` &lt;dbl&gt;, ## # `succinic acid_Time_50min` &lt;dbl&gt;, `succinic acid_Time_2hr` &lt;dbl&gt;, ## # phosphatidylcholine_Time_0min &lt;dbl&gt;, phosphatidylcholine_Time_10min &lt;dbl&gt;, ## # phosphatidylcholine_Time_20min &lt;dbl&gt;, phosphatidylcholine_Time_50min &lt;dbl&gt;, … mutate Still some issues here. We managed a separation but out dose column isn’t numeric! We need to remove the “mg”. Figure 7.5: Mutate:Making new columns or changing existing ones. tbl_met %&gt;% separate(Culture, c(&quot;culture&quot;, &quot;dose_mg&quot;), sep = &quot; dose:&quot;) %&gt;% mutate(Dose = str_remove(dose_mg, &quot;mg&quot;)) ## # A tibble: 9 × 23 ## culture dose_mg glutamate_Time_0min glutamate_Time_10min glutamate_Time_20min ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli 0mg 191666 2082823 1796910 ## 2 staph aureus 0mg 125210 132156 114445 ## 3 p aeruginosa 0mg 978126 397012 267599 ## 4 e coli 10mg 1445 1017467 14456533 ## 5 staph aureus 10mg 152 128683 1360154 ## 6 p aeruginosa 10mg 23 1612622 18282386 ## 7 e coli 20mg 885 42356 11758951 ## 8 staph aureus 20mg 136 4276 1162855 ## 9 p aeruginosa 20mg 3098 68146 19333030 ## # ℹ 18 more variables: glutamate_Time_50min &lt;dbl&gt;, glutamate_Time_2hr &lt;dbl&gt;, ## # lysine_Time_0min &lt;dbl&gt;, lysine_Time_10min &lt;dbl&gt;, lysine_Time_20min &lt;dbl&gt;, ## # lysine_Time_50min &lt;dbl&gt;, lysine_Time_2hr &lt;dbl&gt;, `succinic acid_Time_0min` &lt;dbl&gt;, ## # `succinic acid_Time_10min` &lt;dbl&gt;, `succinic acid_Time_20min` &lt;dbl&gt;, ## # `succinic acid_Time_50min` &lt;dbl&gt;, `succinic acid_Time_2hr` &lt;dbl&gt;, ## # phosphatidylcholine_Time_0min &lt;dbl&gt;, phosphatidylcholine_Time_10min &lt;dbl&gt;, ## # phosphatidylcholine_Time_20min &lt;dbl&gt;, phosphatidylcholine_Time_50min &lt;dbl&gt;, … Ok, the “mg” is removed but if we check the tibble, Dose is still chr, so it’s still computed as text by R! Let’s make it numeric. tbl_met %&gt;% separate(Culture, c(&quot;culture&quot;, &quot;dose_mg&quot;), sep = &quot; dose:&quot;) %&gt;% mutate(dose_mg = str_remove(dose_mg, &quot;mg&quot;)) %&gt;% mutate(dose_mg = as.numeric(dose_mg)) ## # A tibble: 9 × 22 ## culture dose_mg glutamate_Time_0min glutamate_Time_10min glutamate_Time_20min ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli 0 191666 2082823 1796910 ## 2 staph aureus 0 125210 132156 114445 ## 3 p aeruginosa 0 978126 397012 267599 ## 4 e coli 10 1445 1017467 14456533 ## 5 staph aureus 10 152 128683 1360154 ## 6 p aeruginosa 10 23 1612622 18282386 ## 7 e coli 20 885 42356 11758951 ## 8 staph aureus 20 136 4276 1162855 ## 9 p aeruginosa 20 3098 68146 19333030 ## # ℹ 17 more variables: glutamate_Time_50min &lt;dbl&gt;, glutamate_Time_2hr &lt;dbl&gt;, ## # lysine_Time_0min &lt;dbl&gt;, lysine_Time_10min &lt;dbl&gt;, lysine_Time_20min &lt;dbl&gt;, ## # lysine_Time_50min &lt;dbl&gt;, lysine_Time_2hr &lt;dbl&gt;, `succinic acid_Time_0min` &lt;dbl&gt;, ## # `succinic acid_Time_10min` &lt;dbl&gt;, `succinic acid_Time_20min` &lt;dbl&gt;, ## # `succinic acid_Time_50min` &lt;dbl&gt;, `succinic acid_Time_2hr` &lt;dbl&gt;, ## # phosphatidylcholine_Time_0min &lt;dbl&gt;, phosphatidylcholine_Time_10min &lt;dbl&gt;, ## # phosphatidylcholine_Time_20min &lt;dbl&gt;, phosphatidylcholine_Time_50min &lt;dbl&gt;, … pivot_ Now, we have a bigger problem… multiple pieces of information encoded into each column! We need this tibble to be long rather than wide to by tidy. We can use pivot_longer from tidyr. Figure 7.6: Pivoting longer from wide data. tbl_met %&gt;% separate(Culture, c(&quot;culture&quot;, &quot;dose_mg&quot;), sep = &quot; dose:&quot;) %&gt;% mutate(dose_mg = str_remove(dose_mg, &quot;mg&quot;)) %&gt;% mutate(dose_mg = as.numeric(dose_mg)) %&gt;% pivot_longer(cols = 3:22, names_to = &quot;metabolite_time&quot;, values_to = &quot;abundance&quot;) ## # A tibble: 180 × 4 ## culture dose_mg metabolite_time abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 e coli 0 glutamate_Time_0min 191666 ## 2 e coli 0 glutamate_Time_10min 2082823 ## 3 e coli 0 glutamate_Time_20min 1796910 ## 4 e coli 0 glutamate_Time_50min 846402 ## 5 e coli 0 glutamate_Time_2hr 1782879 ## 6 e coli 0 lysine_Time_0min 1418 ## 7 e coli 0 lysine_Time_10min 2004 ## 8 e coli 0 lysine_Time_20min 1498 ## 9 e coli 0 lysine_Time_50min 1356 ## 10 e coli 0 lysine_Time_2hr 962 ## # ℹ 170 more rows Another happy table…but STILL has multiple data encoded in a single column! tbl_met %&gt;% separate(Culture, c(&quot;culture&quot;, &quot;dose_mg&quot;), sep = &quot; dose:&quot;) %&gt;% mutate(dose_mg = str_remove(dose_mg, &quot;mg&quot;)) %&gt;% mutate(dose_mg = as.numeric(dose_mg)) %&gt;% pivot_longer(cols = 3:22, names_to = &quot;metabolite_time&quot;, values_to = &quot;abundance&quot;) %&gt;% separate(metabolite_time, c(&quot;metabolite&quot;, &quot;time_min&quot;), sep=&quot;_&quot;) ## # A tibble: 180 × 5 ## culture dose_mg metabolite time_min abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 e coli 0 glutamate Time 191666 ## 2 e coli 0 glutamate Time 2082823 ## 3 e coli 0 glutamate Time 1796910 ## 4 e coli 0 glutamate Time 846402 ## 5 e coli 0 glutamate Time 1782879 ## 6 e coli 0 lysine Time 1418 ## 7 e coli 0 lysine Time 2004 ## 8 e coli 0 lysine Time 1498 ## 9 e coli 0 lysine Time 1356 ## 10 e coli 0 lysine Time 962 ## # ℹ 170 more rows Almost there, but notice how metabolite_time has three items when we separate by _? glutamate_Time_0min -&gt; glutamateTime0min We only provided two column names, though. Perhaps we can rethink our separator. tbl_met %&gt;% separate(Culture, c(&quot;culture&quot;, &quot;dose_mg&quot;), sep = &quot; dose:&quot;) %&gt;% mutate(dose_mg = str_remove(dose_mg, &quot;mg&quot;)) %&gt;% mutate(dose_mg = as.numeric(dose_mg)) %&gt;% pivot_longer(cols = 3:22, names_to = &quot;metabolite_time&quot;, values_to = &quot;abundance&quot;) %&gt;% separate(metabolite_time, c(&quot;metabolite&quot;, &quot;time_min&quot;), sep=&quot;_Time_&quot;) ## # A tibble: 180 × 5 ## culture dose_mg metabolite time_min abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 e coli 0 glutamate 0min 191666 ## 2 e coli 0 glutamate 10min 2082823 ## 3 e coli 0 glutamate 20min 1796910 ## 4 e coli 0 glutamate 50min 846402 ## 5 e coli 0 glutamate 2hr 1782879 ## 6 e coli 0 lysine 0min 1418 ## 7 e coli 0 lysine 10min 2004 ## 8 e coli 0 lysine 20min 1498 ## 9 e coli 0 lysine 50min 1356 ## 10 e coli 0 lysine 2hr 962 ## # ℹ 170 more rows Ok a new problem has appeared: mixed time units in our time_min column. We have both “min” and “hr”. We need to standardize to understand these as time units. We can mutate with case_when, which does specific things conditionally on particular observations in the tibble. This is a dplyr way of doing if statements. We will also advance it up by doing the “character” to “numeric” conversion in one go! tbl_met_tidy &lt;- tbl_met %&gt;% separate(Culture, c(&quot;culture&quot;, &quot;dose_mg&quot;), sep = &quot; dose:&quot;) %&gt;% mutate(dose_mg = str_remove(dose_mg, &quot;mg&quot;)) %&gt;% mutate(dose_mg = as.numeric(dose_mg)) %&gt;% pivot_longer(cols = 3:22, names_to = &quot;metabolite_time&quot;, values_to = &quot;abundance&quot;) %&gt;% separate(metabolite_time, c(&quot;metabolite&quot;, &quot;time_min&quot;), sep=&quot;_Time_&quot;) %&gt;% mutate( time_min = case_when( str_detect(time_min, &quot;min&quot;) ~ parse_number(time_min), str_detect(time_min, &quot;hr&quot;) ~ parse_number(time_min) * 60 ) ) tbl_met_tidy ## # A tibble: 180 × 5 ## culture dose_mg metabolite time_min abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli 0 glutamate 0 191666 ## 2 e coli 0 glutamate 10 2082823 ## 3 e coli 0 glutamate 20 1796910 ## 4 e coli 0 glutamate 50 846402 ## 5 e coli 0 glutamate 120 1782879 ## 6 e coli 0 lysine 0 1418 ## 7 e coli 0 lysine 10 2004 ## 8 e coli 0 lysine 20 1498 ## 9 e coli 0 lysine 50 1356 ## 10 e coli 0 lysine 120 962 ## # ℹ 170 more rows Now we can examine our tibble and confirm that once and for all, it is tidy. Let’s save it for later use. We can also pipe in our saving. write_ tbl_met %&gt;% separate(Culture, c(&quot;culture&quot;, &quot;dose_mg&quot;), sep = &quot; dose:&quot;) %&gt;% mutate(dose_mg = str_remove(dose_mg, &quot;mg&quot;)) %&gt;% mutate(dose_mg = as.numeric(dose_mg)) %&gt;% pivot_longer(cols = 3:22, names_to = &quot;metabolite_time&quot;, values_to = &quot;abundance&quot;) %&gt;% separate(metabolite_time, c(&quot;metabolite&quot;, &quot;time_min&quot;), sep=&quot;_Time_&quot;) %&gt;% mutate( time_min = case_when( str_detect(time_min, &quot;min&quot;) ~ parse_number(time_min), str_detect(time_min, &quot;hr&quot;) ~ parse_number(time_min) * 60 ) ) %&gt;% write_csv(&quot;data/tidyed-met-df.csv&quot;) Non-pipe One thing you can notice is how the dplyr verbs add up. We can continually pipe one data frame through a series of wrangling steps until we are satisfied. Operating in this way organizes processes in a way that is easy to manage. We could also do this: # read the data no_pipe_tbl_met &lt;- read_csv(&quot;data/bacterial-metabolites_dose-simicillin_wide.csv&quot;) no_pipe_tbl_met &lt;- separate(no_pipe_tbl_met, Culture, c(&quot;culture&quot;, &quot;dose_mg&quot;), sep = &quot; dose:&quot;) no_pipe_tbl_met &lt;- mutate(no_pipe_tbl_met, dose_mg = str_remove(dose_mg, &quot;mg&quot;)) no_pipe_tbl_met &lt;- mutate(no_pipe_tbl_met, dose_mg = as.numeric(dose_mg)) no_pipe_tbl_met &lt;- pivot_longer(no_pipe_tbl_met, cols = 3:22, names_to = &quot;metabolite_time&quot;, values_to = &quot;abundance&quot;) no_pipe_tbl_met &lt;- separate(no_pipe_tbl_met, metabolite_time, c(&quot;metabolite&quot;, &quot;time_min&quot;), sep=&quot;_Time_&quot;) no_pipe_tbl_met_tidy &lt;- mutate(no_pipe_tbl_met, time_min = case_when( str_detect(time_min, &quot;min&quot;) ~ parse_number(time_min), str_detect(time_min, &quot;hr&quot;) ~ parse_number(time_min) * 60 ) ) no_pipe_tbl_met_tidy ## # A tibble: 180 × 5 ## culture dose_mg metabolite time_min abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli 0 glutamate 0 191666 ## 2 e coli 0 glutamate 10 2082823 ## 3 e coli 0 glutamate 20 1796910 ## 4 e coli 0 glutamate 50 846402 ## 5 e coli 0 glutamate 120 1782879 ## 6 e coli 0 lysine 0 1418 ## 7 e coli 0 lysine 10 2004 ## 8 e coli 0 lysine 20 1498 ## 9 e coli 0 lysine 50 1356 ## 10 e coli 0 lysine 120 962 ## # ℹ 170 more rows summarize Now we have a tidy tibble, we can summarize very rapidly. tbl_met_tidy %&gt;% group_by(culture, metabolite) %&gt;% summarize(mean = mean(abundance)) ## # A tibble: 12 × 3 ## # Groups: culture [3] ## culture metabolite mean ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 e coli glutamate 2323045. ## 2 e coli lysine 1519827. ## 3 e coli phosphatidylcholine 67749754. ## 4 e coli succinic acid 1906850. ## 5 p aeruginosa glutamate 2902178 ## 6 p aeruginosa lysine 2336119. ## 7 p aeruginosa phosphatidylcholine 77078132. ## 8 p aeruginosa succinic acid 3525432. ## 9 staph aureus glutamate 226736. ## 10 staph aureus lysine 193944. ## 11 staph aureus phosphatidylcholine 5836359. ## 12 staph aureus succinic acid 133337. What other questions can we ask??? Exercises Create a new R Studio Project and name it 005_data_wrangling. Create a new R script, add your name and date at the top as comments. Locate and/or download a Tidyverse cheat-sheet and refer to it as needed. Download the data. url &lt;- &quot;https://raw.githubusercontent.com/jeffsocal/ASMS_R_Basics/main/data/bacterial-metabolites_dose-simicillin_messy.csv&quot; download.file(url, destfile = &quot;./data/bacterial-metabolites_dose-simicillin_messy.csv&quot;) Read in the messy bacteria data and store it as a variable. ## # A tibble: 9 × 12 ## Culture User glutamate_runtime_0hr glutamate_runtime_24hr glutamate_runtime_2day ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli dose--0-m… Mass… 191666 2082823 1796910 ## 2 staph aureus dos… Mass… 125210 132156 114445 ## 3 p aeruginosa dos… Mass… 978126 397012 267599 ## 4 e coli dose--10-… Mass… 1445 1017467 14456533 ## 5 staph aureus dos… Mass… 152 128683 1360154 ## 6 p aeruginosa dos… Mass… 23 1612622 18282386 ## 7 e coli dose--20-… Mass… 885 42356 11758951 ## 8 staph aureus dos… Mass… 136 4276 1162855 ## 9 p aeruginosa dos… Mass… 3098 68146 19333030 ## # ℹ 7 more variables: glutamate_runtime_3day &lt;dbl&gt;, glutamate_runtime_5day &lt;dbl&gt;, ## # lysine_runtime_0hr &lt;dbl&gt;, lysine_runtime_24hr &lt;dbl&gt;, lysine_runtime_2day &lt;dbl&gt;, ## # lysine_runtime_3day &lt;dbl&gt;, lysine_runtime_5day &lt;dbl&gt; In all proceeding exercises, pipe results from previous exercise into current exercise creating a single lone pipe for data processing Separate Culture column containing culture and dose into culture and dose_mg_ml columns. ## # A tibble: 9 × 13 ## culture dose_mg_ml User glutamate_runtime_0hr glutamate_runtime_24hr ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli 0-mg/ml Mass Spectrometrist 191666 2082823 ## 2 staph aureus 0-mg/ml Mass Spectrometrist 125210 132156 ## 3 p aeruginosa 0-mg/ml Mass Spectrometrist 978126 397012 ## 4 e coli 10-mg/ml Mass Spectrometrist 1445 1017467 ## 5 staph aureus 10-mg/ml Mass Spectrometrist 152 128683 ## 6 p aeruginosa 10-mg/ml Mass Spectrometrist 23 1612622 ## 7 e coli 20-mg/ml Mass Spectrometrist 885 42356 ## 8 staph aureus 20-mg/ml Mass Spectrometrist 136 4276 ## 9 p aeruginosa 20-mg/ml Mass Spectrometrist 3098 68146 ## # ℹ 8 more variables: glutamate_runtime_2day &lt;dbl&gt;, glutamate_runtime_3day &lt;dbl&gt;, ## # glutamate_runtime_5day &lt;dbl&gt;, lysine_runtime_0hr &lt;dbl&gt;, lysine_runtime_24hr &lt;dbl&gt;, ## # lysine_runtime_2day &lt;dbl&gt;, lysine_runtime_3day &lt;dbl&gt;, lysine_runtime_5day &lt;dbl&gt; Make dose_mg_ml column numeric by removing the text and change the column data type from character to numeric. ## # A tibble: 9 × 13 ## culture dose_mg_ml User glutamate_runtime_0hr glutamate_runtime_24hr ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli 0 Mass Spectrometrist 191666 2082823 ## 2 staph aureus 0 Mass Spectrometrist 125210 132156 ## 3 p aeruginosa 0 Mass Spectrometrist 978126 397012 ## 4 e coli 10 Mass Spectrometrist 1445 1017467 ## 5 staph aureus 10 Mass Spectrometrist 152 128683 ## 6 p aeruginosa 10 Mass Spectrometrist 23 1612622 ## 7 e coli 20 Mass Spectrometrist 885 42356 ## 8 staph aureus 20 Mass Spectrometrist 136 4276 ## 9 p aeruginosa 20 Mass Spectrometrist 3098 68146 ## # ℹ 8 more variables: glutamate_runtime_2day &lt;dbl&gt;, glutamate_runtime_3day &lt;dbl&gt;, ## # glutamate_runtime_5day &lt;dbl&gt;, lysine_runtime_0hr &lt;dbl&gt;, lysine_runtime_24hr &lt;dbl&gt;, ## # lysine_runtime_2day &lt;dbl&gt;, lysine_runtime_3day &lt;dbl&gt;, lysine_runtime_5day &lt;dbl&gt; Pivot the table from wide to long creating metabolite, time_hr &amp; abundance columns. ## # A tibble: 90 × 6 ## culture dose_mg_ml User metabolite time_hr abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 e coli 0 Mass Spectrometrist glutamate 0hr 191666 ## 2 e coli 0 Mass Spectrometrist glutamate 24hr 2082823 ## 3 e coli 0 Mass Spectrometrist glutamate 2day 1796910 ## 4 e coli 0 Mass Spectrometrist glutamate 3day 846402 ## 5 e coli 0 Mass Spectrometrist glutamate 5day 1782879 ## 6 e coli 0 Mass Spectrometrist lysine 0hr 1418 ## 7 e coli 0 Mass Spectrometrist lysine 24hr 2004 ## 8 e coli 0 Mass Spectrometrist lysine 2day 1498 ## 9 e coli 0 Mass Spectrometrist lysine 3day 1356 ## 10 e coli 0 Mass Spectrometrist lysine 5day 961 ## # ℹ 80 more rows Make sure time_hr contains just hours and not a mixture of days and hours. ## # A tibble: 90 × 6 ## culture dose_mg_ml User metabolite time_hr abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli 0 Mass Spectrometrist glutamate 0 191666 ## 2 e coli 0 Mass Spectrometrist glutamate 24 2082823 ## 3 e coli 0 Mass Spectrometrist glutamate 48 1796910 ## 4 e coli 0 Mass Spectrometrist glutamate 72 846402 ## 5 e coli 0 Mass Spectrometrist glutamate 120 1782879 ## 6 e coli 0 Mass Spectrometrist lysine 0 1418 ## 7 e coli 0 Mass Spectrometrist lysine 24 2004 ## 8 e coli 0 Mass Spectrometrist lysine 48 1498 ## 9 e coli 0 Mass Spectrometrist lysine 72 1356 ## 10 e coli 0 Mass Spectrometrist lysine 120 961 ## # ℹ 80 more rows Remove the User column. See cheatsheet here: data-wrangling-cheatsheet or consult internet. ## # A tibble: 90 × 5 ## culture dose_mg_ml metabolite time_hr abundance ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 e coli 0 glutamate 0 191666 ## 2 e coli 0 glutamate 24 2082823 ## 3 e coli 0 glutamate 48 1796910 ## 4 e coli 0 glutamate 72 846402 ## 5 e coli 0 glutamate 120 1782879 ## 6 e coli 0 lysine 0 1418 ## 7 e coli 0 lysine 24 2004 ## 8 e coli 0 lysine 48 1498 ## 9 e coli 0 lysine 72 1356 ## 10 e coli 0 lysine 120 961 ## # ℹ 80 more rows "],["data-visualization.html", "Chapter 8 Data Visualization 8.1 Base: plot 8.2 GGPlot2 8.3 Alternatives Exercises", " Chapter 8 Data Visualization Visualizing your data is crucial because it helps you understand the patterns, trends, and relationships within the data. A well-designed visualization can make complex data easy to understand and convey insights that would be hard to discern from raw data. Anscombe’s quartet is a classic example that demonstrates the importance of visualizing your data. This quartet comprises four datasets with nearly identical simple descriptive statistics. However, when graphed, they have very different distributions and appear very different from one another. This example shows that relying solely on summary statistics to understand data can be misleading and inadequate. Table 8.1: Summary stats for Anscombe’s quartet. set mean_x var_x mean_y var_y intercept slope r.squared A 9 11 7.500909 4.127269 3.000091 0.5000909 0.6665425 B 9 11 7.500909 4.127629 3.000909 0.5000000 0.6662420 C 9 11 7.500000 4.122620 3.002454 0.4997273 0.6663240 D 9 11 7.500000 4.126740 3.000000 0.5000000 0.6663856 In data analysis, creating a plot to convey a message or demonstrate a result is a common endpoint. To achieve this, this book utilizes the GGPlot2 package, which is part of the Tidyverse. This package complements the data pipelining demonstrated in the previous chapters, making it a perfect choice for creating a wide range of plots, from simple scatter plots to complex heat maps, making it ideal for data visualization. At the end of this chapter you should be able to Understand the need for visualizations. Create some simple plots of points, lines and bars. Manipulate how a plot looks. 8.1 Base: plot R comes standard with the fairly basic plotting function plot(). While this function forms the basis for all plotting interactions in R, it can be greatly extended with additional packages. Three such packages widely used are lattice, GGplot2, Plotly. This chapter will dive into GGplot2 which is integrated with the tidyverse, and is great for static publication quality plots. The other two will briefly be covered as suitable alternatives. plot(sample(1:20),sample(1:20)) df &lt;- data.frame( x1 = sample(1:20), y1 = sample(1:20) ) df %&gt;% plot() 8.2 GGPlot2 The motivation behind GGplot is based on the grammar of graphics such that “the idea that you can build every graph from the same few components” Ideally this accomplishes dual goals of allowing you to quickly construct plots for initial analyses and checking for oddities (as explained above) and following the logical process of the plot construction. 8.2.1 Syntax To graph in GGPlot there are a few core embodiments that need to be considered. data geom aesthetics coordinate a table of numeric and/or categorical values a geometric object or visual representation, that can be layered how variables in the data are mapped to visual properties orientation of the data points data.frame or tibble points, lines, bars, boxs, etc. eg. x = col_a, y = col_b eg. cartesian (x,y), polar # the basic structure ggplot(data, aes(x,y)) + geom_point() + coord_cartiesian() # combined with dplyr makes for a readable process data %&gt;% ggplot(aes(x,y)) + geom_point() In this example the ggplot() function contains the two components, the data table data and mapping function aes(). Since GGPlot follows a layered modality, the ggplot() function “sets” the canvas and passes the data table data and mapping function aes() to all the functions that follow with the + operator. library(tidyverse) Lets create some data … set.seed(5) n_peaks &lt;- 300 tbl_mz &lt;- tibble( mz = sample(3500:20000/10, n_peaks), int = rlnorm(meanlog = 5, sdlog = 2, n_peaks), class = sample(c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;), n_peaks, replace = TRUE) ) 8.2.2 Basic Data Plotting 1.2.1 Online Cheat-sheets Points and Lines Points and lines graphing is a simple way of representing data in a two-dimensional space. In this graph, we use points to represent individual data values, and lines to connect them. The x-axis usually represents the independent variable while the y-axis represents the dependent variable - or in other words, what y was observed while measuring x. To plot a point, we use an ordered pair of values (x, y) that correspond to the position of the point on the graph. For example, the point (2, 5) would be plotted 2 units to the right on the x-axis and 5 units up on the y-axis. We can also connect points with lines to show a trend or pattern in the data. These lines can be straight or curved, depending on the nature of the data. A straight line can be drawn to connect two points or to represent a linear relationship between the variables. p01 &lt;- tbl_mz %&gt;% ggplot(aes(mz, int)) + geom_point() p02 &lt;- tbl_mz %&gt;% ggplot(aes(mz, int)) + geom_line() p03 &lt;- tbl_mz %&gt;% ggplot(aes(mz, int)) + geom_point(color=&#39;red&#39;) + geom_line(color=&#39;black&#39;) p04 &lt;- tbl_mz %&gt;% ggplot(aes(mz, int)) + geom_line(color=&#39;black&#39;) + geom_point(color=&#39;red&#39;) It’s important to take note of the difference between plots p03 and p04. While both plots showcase a similar data representation, a closer look reveals a notable difference. Specifically, in the latter plot (p04), we can see that the red points appear under the black line. This occurs because the points were layered first, and then the lines were layered over them. This is a crucial distinction to make as it highlights the importance of the order in which layers are applied in the plot. Segments Line segments are an important concept in geometry and are used in various applications. A line segment is a part of a line that is bounded by two distinct end points. It is also a default representation of centroided mass spectra. In this case the segment will start and end on the same x (mz), while the y (int) component will end at 0. p05 &lt;- tbl_mz %&gt;% ggplot(aes(mz, int)) + geom_segment(aes(xend = mz, yend = 0)) Bar Chart When it comes to representing categorical data, bar charts are considered to be the most effective visualization tool. Bar charts are simple, yet powerful, and can be used to display data in a clear and concise way. They are easy to read and understand, and are a popular choice among data analysts, researchers, and business professionals. Whether you’re trying to visualize sales data, survey results, or demographic information, bar charts are a great option to consider. So, if you’re looking for a way to represent categorical data, consider using a bar chart for the most accurate and comprehensive representation. p06 &lt;- tbl_mz %&gt;% ggplot(aes(class)) + geom_bar() Pie Chart You maybe considering a pie chart, which is a circular diagram divided into sectors, with each sector representing a proportion of the whole. It is commonly used to display percentages, where the sum of the sectors equals 100%. There is no specific geom to build a pie-chart with ggplot2. The trick is to build a barplot and use coord_polar() to make it circular. However, interpreting pie charts can be challenging since humans are not very skilled at reading angles. For instance, it is often difficult to determine which group is the largest and to arrange them by value. As a result, it is advisable to refrain from using pie charts. p07 &lt;- tbl_mz %&gt;% ggplot(aes(class, fill=class)) + geom_bar() p08 &lt;- tbl_mz %&gt;% ggplot(aes(class, fill=class)) + geom_bar() + coord_flip() p09 &lt;- tbl_mz %&gt;% ggplot(aes(1, fill=class)) + geom_bar(position = &#39;fill&#39;) + coord_polar(theta = &#39;y&#39;) Note how difficult it is in the pie chart to tell (by eye) which is the smallest. 8.2.3 Data Distributions In statistics, a distribution refers to the way in which a set of data is spread out or dispersed. It describes the pattern of values that a variable can take and how frequently each value occurs. A distribution can be characterized by its shape, center, and spread, and can be represented graphically using tools such as histograms, box plots, and density plots. Histograms A histogram is a graphical representation of the distribution of a dataset. It is a type of bar chart that displays the frequency of data values falling into specified intervals or ranges of values, known as bins. The x-axis of the histogram represents the bins or intervals, and the y-axis represents the frequency or count of values falling into each bin. Histograms are widely used to summarize large datasets and identify patterns or trends and to visualize the shape of a distribution, whether it is symmetric or skewed, and whether it has any outliers or gaps in the data. They can also be used to compare the distributions of two or more datasets, by plotting them on the same graph with different colors or patterns. p10 &lt;- tbl_mz %&gt;% ggplot(aes(log10(int))) + geom_histogram(binwidth = 1) Density A density plot is a graphical representation of the distribution of a dataset. It is formed by smoothing the data values and representing them as a continuous probability density function. The density plot is a variation of the histogram that provides a smoother representation of the data, eliminating the need for binning. It is particularly useful when the data is continuous and the sample size is large. The density plot can be used to identify the shape of the distribution, the presence of multiple modes, and the presence of outliers. Again, it can also be used to compare the distributions of two or more datasets by overlaying them on the same plot. p11 &lt;- tbl_mz %&gt;% ggplot(aes(log10(int))) + geom_density() Box-Plot One of the most commonly used types of plots in GGplot2 is the box plot. A box plot is used to display the distribution of a continuous variable. It shows the median, interquartile range, and any outliers present in the data. Box plots are useful in scientific analysis because they allow us to quickly see the distribution of a variable and identify any potential outliers. They are particularly useful when comparing the distribution of a variable across different groups or categories. For example, we may use a box plot to compare the distribution of values across different class levels. To create a box plot in GGplot2, we use the geom_boxplot() function. We specify the variable we want to plot on the y-axis and any grouping variables on the x-axis. p12 &lt;- tbl_mz %&gt;% ggplot(aes(class, log10(int))) + geom_boxplot() 8.2.4 Extended Syntax One way to enhance the functionality of ggplots is by using additional modifiers. These modifiers can help you to create more intricate and detailed visualizations that better represent your data. By tweaking the parameters of your ggplots, you can create visualizations that are more informative, aesthetically pleasing, and tailored to your specific needs. Whether you want to adjust the color scheme, add annotations, or modify the axis labels, additional modifiers can help you to achieve your desired outcome. Colors We saw a bit how to adjust colors in the previous plots. The two color arguments to consider are color, which modifies the point, line and edge color, and fill, which modifies the internal color of a shape for plots such as geom_bar and geom_histogram. p13 &lt;- tbl_mz %&gt;% ggplot(aes(log10(int))) + geom_histogram(color = &#39;blue&#39;, fill = &#39;purple&#39;, binwidth = 1) p14 &lt;- tbl_mz %&gt;% ggplot(aes(log10(int))) + geom_density(color = &#39;red&#39;, fill = &#39;orange&#39;) Colors can also take on a transparency called alpha, which allows one layer to show through when two or more are plotted together. p15 &lt;- tbl_mz %&gt;% ggplot(aes(log10(int))) + geom_density(aes(fill = class)) p16 &lt;- tbl_mz %&gt;% ggplot(aes(log10(int))) + geom_density(aes(fill = class), alpha = .25) Scales GGplot2 is a popular data visualization package in R that allows users to create stunning and insightful visualizations. One of the key features of GGplot2 is its ability to handle scales, which are critical for displaying data accurately and effectively. In this document, we will explore how to use scales in GGplot2, specifically for log10 and manual scales. Log10 Scale Logarithmic scales are useful when the data spans several orders of magnitude. GGplot2 makes it easy to create log10 scales using the scale_y_log10() and scale_x_log10() functions. Using the same examples from above, yet instead of applying the log10() function directly to the variable, we can apply it to the scale instead. p17 &lt;- tbl_mz %&gt;% ggplot(aes(int)) + geom_histogram(aes(fill = class), position = &#39;identity&#39;, alpha = .5, binwidth = 1) + scale_x_log10() p18 &lt;- tbl_mz %&gt;% ggplot(aes(int)) + geom_density(aes(fill = class), alpha = .25) + scale_x_log10() This results in a plot where the y-axis is scaled logarithmically, making it easier to see the differences between the different car classes. Note, that when we specified the binwidth = 1 in the geom_histogram(), GGplot2 applied that to the log10 space specified from the scale_x_log10(). Manual Scales Sometimes, we may want to manually define the scale for our plots. For example, we may want to create a plot where the y-axis only shows values between 0 and 10. We can do this using the scale_y_continuous() function in GGplot2. Here is an example of how to use the scale_y_continuous() function to manually define the y-axis scale: p19 &lt;- tbl_mz %&gt;% ggplot(aes(mz, int)) + geom_segment(aes(xend = mz, yend = 0)) + scale_y_continuous(n.breaks = 13) + scale_x_continuous(n.breaks = 5) In this example, we added a manual scale to both the x- and y-axis using the scale_x_continuous() and scale_y_continuous() functions, respectively, and specifying the number of breaks n.breaks = and the limits limits =. Faceting Faceting is a powerful feature in ggplot2 that allows us to split a single plot into multiple small plots based on a categorical variable. It enables us to visualize complex data patterns and relationships in a more understandable way. There are two types of faceting in ggplot2: facet_wrap and facet_grid. facet_wrap facet_wrap creates a grid of plots by wrapping the facets from left-to-right and top-to-bottom in the plot. Each facet is displayed in a separate panel, and the panels are arranged in rows and columns based on the levels of the specified categorical variable. p20 &lt;- p17 + facet_wrap(. ~ class) p21 &lt;- p17 + facet_wrap(. ~ class, scales = &#39;free&#39;) In this example, we are reused ggplot object p17 and created two additional plots. The facet_wrap function is used to split the plot into multiple panels based on the categorical variable class using the tilde . ~ class. In this case, the dot . prior to the tilde ~ tells ggplot to consider only a single variable, class as we had defined it. You can think of the tilde as a type of function this 'by' that or y ~ x. This becomes more important in the facet_grid() function. Notice in the p21 plot we set the scales free, allowing each facet to dictate x and y plot scales. facet_grid facet_grid creates a grid of plots by specifying one or more categorical variables that define the rows and columns of the grid. It allows us to create more complex faceted plots than facet_wrap. In this example we will randomly add a new varaible called group that will allow us to create the y direction of the facet. tbl_new &lt;- tbl_mz %&gt;% mutate(group = sample(c(&#39;positive&#39;, &#39;negative&#39;), n_peaks, replace = TRUE)) p22 &lt;- tbl_new %&gt;% ggplot(aes(int)) + geom_histogram(binwidth = 1) + scale_x_log10() + facet_grid(group ~ class) To illistrate the difference between facet_wrap() and facet_grid() consider what happens when a set of data is missing. Note in p24 it is not immediately intuitive in facet_wrap() that c-negative is missing, where as in facet_grid(), the layout highlights this realization. tbl_new &lt;- tbl_mz %&gt;% mutate(group = sample(c(&#39;positive&#39;, &#39;negative&#39;), n_peaks, replace = TRUE)) w &lt;- which(tbl_new$class == &#39;C&#39; &amp; tbl_new$group == &#39;negative&#39;) p23 &lt;- tbl_new[-w, ] %&gt;% ggplot(aes(int)) + geom_histogram(binwidth = 1) + scale_x_log10() p24 &lt;- p23 + facet_wrap(group ~ class) p25 &lt;- p23 + facet_grid(group ~ class) Labels Adding text to plots to enhance the visualization is an important aspect of data analysis. Labels help to identify the variables being plotted and provide context for the audience. When adding labels to a plot, it is important to consider the type of plot being used and the information being displayed. For example, in a scatter plot, labels can be added to each point to indicate the value of each variable. In a line chart, labels can be added to the x and y axes to indicate the units of measurement. p26 &lt;- tbl_mz %&gt;% ggplot(aes(mz, int)) + geom_segment(aes(xend = mz, yend = 0)) + geom_text(data = tbl_mz %&gt;% slice_max(int, n = 10), aes(label = mz), vjust = 0) p27 &lt;- tbl_mz %&gt;% ggplot(aes(mz, int)) + geom_segment(aes(xend = mz, yend = 0)) + geom_label(data = tbl_mz %&gt;% slice_max(int, n = 10), aes(label = mz), vjust = 0) A handy package not in the tidyverse, is ggrepel, which allows for the labels to rearrange themselves such that none of them overlap. library(ggrepel) p28 &lt;- tbl_mz %&gt;% ggplot(aes(mz, int)) + geom_segment(aes(xend = mz, yend = 0)) + geom_text_repel(data = tbl_mz %&gt;% slice_max(int, n = 10), aes(label = mz), box.padding = .5, color = &#39;dodgerblue&#39;) p29 &lt;- tbl_mz %&gt;% ggplot(aes(mz, int)) + geom_segment(aes(xend = mz, yend = 0)) + geom_label_repel(data = tbl_mz %&gt;% slice_max(int, n = 10), aes(label = mz), box.padding = .5, color = &#39;dodgerblue&#39;) Annotations Annotations add explanatory text or labels to a plot, providing additional information to the reader. Adding annotations to a plot can help convey the message behind the data and make the plot more understandable. p30 &lt;- p12 + ggplot2::annotate(&#39;text&#39;, x = Inf, y = Inf, label = &#39;Top-Right&#39;, hjust = 1, vjust = 1) + ggplot2::annotate(&quot;text&quot;, x = -Inf, y = Inf, label = &quot;Top-left&quot;, hjust = 0, vjust = 1) + ggplot2::annotate(&#39;text&#39;, x = Inf, y = -Inf, label = &#39;Bottom-Right&#39;, hjust = 1, vjust = -0.5) + ggplot2::annotate(&quot;text&quot;, x = -Inf, y = -Inf, label = &quot;Bottom-left&quot;, hjust = 0, vjust = -0.5) Style Creating a plotting style can help you to quickly improve the appearance of your plots and make them more consistent with your brand. When working with data visualization, it’s important to keep in mind that the appearance of your plots can significantly impact the way your audience interprets your data. GGplot2 themes and colors offer an easy way to create professional-looking visualizations that will make your data stand out. Themes To apply a theme to your plot, you simply need to call the theme() function and specify the name of the theme you want to use. Some of the most popular themes include: theme_gray(): A simple, gray background with white gridlines. theme_dark(): A simple, gray background with white gridlines. theme_classic(): A classic black and white theme with no gridlines. theme_minimal(): A minimalistic theme with no background or gridlines. theme_bw(): A black and white theme with gray gridlines. You can also create your own custom themes by modifying various theme elements. For example, you can change the background color, font, and size of the plot elements. To do this, you can use the element_*() functions. For example, the element_text() function allows you to modify the font size, color, and family of your text. Another great feature of GGplot2 themes is that they allow you to maintain consistency across multiple visualizations. If you’re creating a series of plots, applying the same theme to each one will give your work a more polished and professional look. p31 &lt;- p12 + theme_gray() # default p32 &lt;- p12 + theme_dark() p33 &lt;- p12 + theme_light() p34 &lt;- p12 + theme_classic() p35 &lt;- p12 + theme_minimal() p36 &lt;- p12 + theme_bw() Colors In addition to applying a theme to your layout, you can should also consider the color scheme. GGplot2 is a powerful data visualization package in R that allows users to create beautiful and informative graphs. The package is highly customizable, and one of its most important features is the ability to customize colors using Brewer and manual color scales. Brewer Color Scales The Brewer color scales in GGplot2 are color palettes that have been specifically designed to be distinguishable by people with color vision deficiencies. These color scales are useful when creating visualizations where color is used to convey information. The Brewer palettes are particularly useful because they are carefully curated to ensure that the colors are distinguishable from one another, even for individuals with color vision deficiencies. This makes them a great option for creating informative data visualizations. To use Brewer color scales in GGplot2, you can simply specify the name of the color scale as an argument to the scale_color_brewer() or scale_fill_brewer() functions. Other popular Brewer color scales include Blues, Greens, Oranges, and Purples. By using these scales, you can create beautiful visualizations that are both aesthetically pleasing and informative. Manual Color Scales In addition to the Brewer color scales, GGplot2 also allows users to specify custom color scales using the scale_color_manual() or scale_fill_manual() functions. These functions take a vector of colors as an argument, which can be specified using names, hex codes, or RGB values. Manual color scales are particularly useful when you want to use specific colors that are not included in the Brewer palettes. For example, if you are creating a visualization for a company and you want to use the company’s brand colors, you can specify the colors using a manual color scale. p37 &lt;- p31 p38 &lt;- p31 + scale_fill_brewer(palette = &#39;Set1&#39;) p39 &lt;- p31 + scale_fill_brewer(palette = &#39;Blues&#39;) p40 &lt;- p31 + scale_fill_manual(values = c(&quot;#d97828&quot;, &quot;#83992a&quot;, &quot;#995d81&quot;,&quot;#44709d&quot;)) 8.3 Alternatives 8.3.1 lattice The Lattice package is an R package that is used for plotting graphs, and is based on the grid graphics system. The package provides a high-level interface to grid graphics, which makes it easy to create complex visualizations with an emphasis on multivariate data. It is designed to meet most typical graphics needs with minimal tuning, but can also be easily extended to handle most nonstandard requirements. Trellis Graphics, originally developed for S and S-PLUS at the Bell Labs, is a framework for data visualization developed by R. A. Becker, W. S. Cleveland, et al, extending ideas presented in Cleveland’s 1993 book Visualizing Data. The Lattice API is based on the original design in S, but extends it in many ways. Various types of lattice plots available for data visualization. Among the different types of plots, univariate plots stand out as they utilize only a single variable for plotting. The different options available for univariate plots include bar plots, box-and-whisker plots, kernel density estimates, dot plots, histograms, quantile plots, and one-dimensional scatter plots. Bivariate plots involve plotting two variables against each other. Examples of bivariate plots include scatterplots and quantile plots. These types of plots are useful in analyzing the relationship between two variables and can provide valuable insights into the data. Trivariate plots, as the name implies, involve plotting three variables and provide a more complex visualization of the data. Options for trivariate plots include level plots, contour plots, three-dimensional scatter plots, and three-dimensional surface plots. These types of plots can be particularly helpful in analyzing complex data sets and identifying patterns in the data that may not be immediately apparent. library(lattice) p51 &lt;- xyplot(int ~ mz, data = tbl_mz, main = &quot;Scatter Plot&quot;) p52 &lt;- xyplot(int ~ mz, data = tbl_mz, type=&#39;a&#39;, main = &quot;Line Plot&quot;) p53 &lt;- histogram(~ log10(int) | class, data = tbl_mz, main = &quot;Histogram&quot;) p54 &lt;- densityplot(~ log10(int) | class, data = tbl_mz, main = &quot;Density Plot&quot;) 8.3.2 plotly Plotly is an open-source data visualization library that allows you to create interactive visualizations in R plotly.com/r/. It offers a wide range of graphs and charts, including line plots, scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, plots with multiple-axes, 3D plots, and more. The package is built on top of htmlwidgets, which means that you can easily embed your visualizations in web applications or other HTML documents. Plotly also nativity supports many data science languages such as R, Python, Julia, Java-script, MATLAB and F#. library(plotly) plot_ly(tbl_mz, x = ~mz, y = ~int, color = ~class, mode = &quot;markers&quot;, type = &quot;scatter&quot;) %&gt;% layout(title = &#39;Scatter Plot&#39;) Exercises Create a new R Studio Project and name it 006_data_visualizing. Create a new R script, add your name and date at the top as comments. Locate and/or download a GGplot2 cheat-sheet and refer to it as needed. If not already done, download Bacterial Metabolite Data (tidy) to use as an example data file. url &lt;- &quot;https://raw.githubusercontent.com/jeffsocal/ASMS_R_Basics/main/data/bacterial-metabolites_dose-simicillin_tidy.csv&quot; download.file(url, destfile = &quot;./data/bacterial-metabolites_dose-simicillin_tidy.csv&quot;) Read in the dataset .csv using the tidyverse set of packages. Create a plot of metabolite Abundance by Time_min … … facet by Organism and Metabolite… … adjust the y-axis to log10, color by Dose_mg, and add a 50% transparent line … … change the theme to something publishable, add a title, modify the x- and y-axis label, modify the legend title, adjust the y-axis ticks to show the actually measured time values, and pick a color scheme that highlights the dose value… "],["sharing.html", "Chapter 9 Sharing 9.1 Reproducibility and Data Science Notebooks 9.2 Quarto 9.3 Packrat 9.4 GitHub 9.5 Docker 9.6 R Packages 9.7 R Shiny Applications Exercises", " Chapter 9 Sharing This topic covers a variety of ways to share R code with others. The goal is to make your work accessible and reproducible for others. Sharing can take many forms, including sharing your RStudio project, creating a distilled version of your analysis that others can follow, developing a web-based application for others to use, or finding ways to contain and disseminate reproducible analyses. By sharing your work, you enable others to learn from and build upon your research, making it more impactful and useful for the wider community. At the end of this chapter you should be able to Understand the options for sharing analyses. Understand why reproducible data analysis is important Understand the basic of Quarto and how to use it for data reports 9.1 Reproducibility and Data Science Notebooks The ability to reproduce or replicate published scientific findings is a critical component to the scientific process. However, over the last decade, the ongoing “reproducibility/replication crisis” has emerged with the problematic concern that many research findings cannot be reproduced. In the context of data analysis, irreproducibility can occur for a varitey of reasons including: raw data not provided analysis files or scripts not provided analysis methods not adequately described wrong analysis methods used software, analysis scripting bugs In recent years, publication standard have increased to help combat the reproducibility crisis. For example, many journals now require that raw data be included in the publication process. As scientists, it is also important for us to keep reproducibility in mind as we perform data analysis tasks. Not only does this help us to conduct better science, reproducible analysis methods can also make our jobs easier by clarifying the analysis process and explicitly capturing all steps that were performed. Reproducible data analysis aims to ensure that someone else is able to repeat the analysis and get the same results. This “someone else” could also be you in the future! A reproducible analysis should provide the why behind the methods used (i.e. good documentation) and the how/what was done (i.e. input data and analysis code). Each analysis step should be explicit and clear, and provide the foundation for others to understand and evaluate what was done. It’s important to note that reproducibility does not imply correctness. You can still do the wrong thing (i.e. apply incorrect methods or have a bug in your code) in a reproducible way. However, knowing exactly what was done is critical to finding and correcting mistakes. Reproducible methods also enables you to more easily share your work and allow others to help find and fix problems. Reproducibility in data analysis exists at many different levels, including: reproducibility of your own processes and code does your code capture all steps of the analysis? if you make changes to your code, could you revisit older versions? reproducibility of external code (software versions, packages, libraries) is your analysis reproducible if you upgrade your software? could you access older software versions if you needed to? reproducibility of your compute environment what happens if you upgrade your computer? is your analysis affected by different operating systems or settings? reproducibility of your data could the analysis data change in the future, knowingly or unknowingly? could you detect a change in the data if one occurred? Whether or not an analysis is fully reproducible isn’t a binary assessment. There is a continuum along the reproducibility scale, and not all projects require the same level of reproducibility. A high degree of reproducibility can require a significant amount of time and cost, so one must balance this effort with the needs of the project. Fortunately, a basic but highly effective level of reproducibility in data analysis tasks can be achieved by following a few guidelines: Capture all steps of an analysis in code/scripts/analysis notebooks, starting from reading input data through to the final output of results. Avoid any manual editing or transformation of data. Provide good documentation about the why behind the analysis and the choices you made Provide how/what you did through your analysis code To help data analysts follow these guidelines, data science notebooks have emerged as a standard tool used for conducting and communicating analysis work. There are a variety of notebook formats to choose from, including Jupyter Notebooks, RMarkdown and Quarto, but they all generally use a literate programming style where code and documentation about the code live in the same document. These notebooks typically utilize two type of “languages” Plain text, formatted text, or a Markup language (e.g. Markdown) to capture the documentation A programming language to capture the code underlying the analysis After an analysis notebook has been created, the document can be “compiled” to create a human readble document that combines both the documentation, the code, and the output from the code. The compiled document can exist in a variety of forms (e.g. PDFs or HTML files) that can be easily shared with others who can see exactly how the analysis was performed. 9.2 Quarto Quarto is a modern “open-source scientific and technical publishing system” created by Posit. For many years, RMarkdown (also developed and supported by Posit/RStudio) was the default data science notebook platform for R users. However, development for RMarkdown was stopped several years ago and efforts are now focused on Quarto, which also supports other languages including Python. Quarto is an extremely power system that not only allows one to make data science notebooks, but also books, websites, reports and presentations. See the Quarto Gallery for examples of what Quarto can do. Installing Quarto: Before you can use Quarto in RStudio, you need to first install it. Follow the instructions on the Quarto Get Started page. In a Quarto document, you write text documentation and R code in the same text file with a .qmd file extension. RStudio makes it easy to work with Quarto documents, as summarized in the following workflow: Create a new .qmd file: File --&gt; New File --&gt; Quarto Document... Write/edit the .qmd file using either the source or visual editor Once finished, render the file to an output document, such as a PDF or HTML file You can think of editing a .qmd file just like editing an R script, but with added emphasis on also including well organized text documentation about your methods and accompanying R code. In older versions of RStudio, the only way to edit notebooks (which were RMarkdown notebooks at the time) was to use the source editor, the same one used to edit R scripts. However, RStudio has recently gained the ability to edit .qmd documents using a visual editor. In the source editor, everything, including the text documentation, is written with code. This meant that you had to also learn the markup language Markdown and some HTML to effectively write the text documentation. However, with the addition of the visual editor to RStudio, you can now write text documentation in a text-editor-like manner (e.g. like in Microsoft Word), removing the need to learn additional computer languages. There are three main components to a .qmd document: A YAML header that defines the documention options and parameters Text blocks where you document your work, analysis, and code Code blocks where write R code to do the actual analysis work Each of these will be described in more detail in the following sections. Figure 9.1: Basic components of a Quarto document. 9.2.1 YAML Headers The YAML header is place at the top of a .qmd file and defines global properties of the document (e.g. title, author, date, output format). While there are a lot of possible options you can specify in the YAML, beginners can just copy and paste the same default template (given below) which covers good default settings for scientific reports. --- title: &quot;REPORT TITLE&quot; author: &quot;NAME&quot; date: &quot;today&quot; toc: true format: html: code-tools: true code-fold: true self-contained: true execute: echo: true warning: false --- Figure 9.2: Quarto YAML header example. In the example YAML header above, many of the options are self explanatory: title, author and date are used to specific exactly what they say. In the date option, you can supply a formatted date string (e.g. 2024-06-01), or use the shortcut “today” to insert today’s date (as of the time of rendering the document). The toc option indicate whether or not you want a table of contents included in the documjent (use false if you don’t). The format options are a bit more complex, but essentially say that we want HTML output along with some additional options about how code is displayed in the document. Finally, the execute options specify how the code blocks are handled. In this case, echo: true means we want to include the printing of code in our output document, and warning: false means we want to suppress the printing of R warnings in our ouptut document. The YAML header content is sandwiched between --- at the top and bottom, with options specified in the middle. These options take the form of a defined keyword (e.g. title), followed by a : and the value that specifies the option. The options supplied in the header are global, meaning they apply to all the following blocks in the document. For example, it’s possible to define default figure size in the YAML header. While you can individually override figure size in each code block, the figure size defined in the YAML will be used by default. There are a lot of options you can configure in the YAML header. But for basic usage, the above header is a great place to start. 9.2.2 Text Blocks After the YAML header is added, the next step is to intertwine text and code blocks needed to perform the analysis task. There is no limit to the number or length of the text and code blocks: use as many as you need to properly communicate your work. As a shortcut in the visual editor, you can type / on a blank line to bring on an insert palette which provides a list of block types to add to your document, including not only text and code blocks, but also things like bulleted lists, tables and links. With the visual editor, you can style text using the formatting bar in the editor pane. Furthermore, many standard text editing keyboard shortcuts are also available to use (e.g. CTRL/CMD+B to bold text). When writing text documentation, it is also good practice to make use of heading levels to organize your document by defining sections for your content. By default, the document title (supplied in the YAML header) has the top heading level of 1. Therefore, additional sections in your document should start a level heading 2 and higher. For example, in a report, you may start with a Summary section (heading level 2) which provides an overview of your analysis, information about the input data, and an overview of the methods used. This could be followed with an Analysis section (heading level 2) that goes into the analysis details and code. If you have a more complex analysis, you may further divide the Analysis section into subgroups (heading levels 3) to describe individual components of the analysis. Finally, you may add a Conclusions section (heading level 2) to summarize the main results and findings. How you organize your document is completely up to you, but following standard writing procedures that you use in other contexts (e.g. when writing publications or reports in school) is recommended. As an added benefit, level heading sections are used to automatically generate a table of contents, which can be particularly useful for longer documents. Figure 9.3: Visual editor for Quarto documents. 9.2.3 Code Blocks Code blocks are where you enter R code to perform the actual analysis work. You can easily insert an R code block by typing / on a empty line in the .qmd file. This will bring up a block selection interface where you can select different types of block and objects to insert, including an R code block (first item in the interface by default). Once you add an R code block, you will see a new section added to the document with {r} at the top and a shaded background. This indicates you now have an R code block, and you can enter your R code into this shaded area. The green right facing triangle in the upper right side of the code block allows you to execute the code inside the block in the console. This is useful if you want to test out the code you’ve written. Figure 9.4: R Code blocks can be inserted using the / menu When adding code to your document, you don’t need, nor is it suggested, to enter all your analysis code into a single block. Rather, it usually makes sense to break-up your analysis code into smaller pieces, interspersed with text blocks documenting your work. This makes it easier for you to develope your analysis, and for someone else to follow what you did. Like a standard R script, R code blocks are executed from top to bottom in the document, and any defined variables are accessible to all blocks that follow. Unlike text blocks, which directly display the entered text into the output document, the output from code blocks depends on the options specified either in the YAML header (global options) or individually for each R code block. For example, you can control whether or not R code, the output from the code, and warning/error messages are shown in the output document. In the YAML header shown above, the options that are part of the execute: section indicate that R code should be output (echo: true) and warning messages should not (warning: false). Because these options are specified in the YAML header, all R code blocks will follow these options unless different options are specified within an R code block. Code block level options can be specified at the top of a code block, by typing #| followed by an option string. Multiple options can be specified by entering them on multiple lines. Some useful code block options include: fig-width:, fig-height: control the width and height of a plot fig-cap: add a figure caption echo: whether or not to display code warning: whether or not to display warning or error messages eval: whether or not to actually execute the code when rendering (useful if you just want to output code) An example of using code block options and how they affect output is shown in the figure below. Figure 9.5: Example code block options and associated output. 9.2.4 Rendering After you have finished writing a .qmd file, the final step is to render it into an output document. This can be easily accomplished by clicking the Render button in the tool bar of the Quarto editor pane inside of RStudio. The format of the output is indicated by the options set in the YAML header. For reports, PDFs and HTML files are useful output options. In practice, HTML output is highly suggested because HTML allows for interactivity in the document (like for webpages) that isn’t possible with PDF files. For example, HTML output allows you to have a floating table of contents within your document, and with more advanced R coding, you can also include interactive plots in your output. Furthermore, since web browsers are ubiquitous, HTML files can be opened by anyone and on any platform. 9.3 Packrat Packrat is an R package that provides a way to manage R package dependencies for projects. It is a powerful tool for reproducible research, as it allows you to create a local library of packages specific to a project that can be shared with collaborators or moved to another machine. With Packrat, packages used in a project are kept at a specific version, ensuring that the same results can be obtained regardless of the version of the package used. Initiating To initiate a Packrat project, you need to run the packrat::init() function in your R console. This will create a packrat directory in your project folder, which will contain all the necessary files and information for Packrat to manage the package dependencies for your project. library(packrat) packrat::init() Installing To install a package into the project-specific library, you can use the packrat::install.packages() function. Packrat will automatically detect package dependencies and install them as well. packrat::install.packages(&quot;dplyr&quot;) Loading To load a package from the project library, you simply use the library() function as usual. Packrat will ensure that the correct versions are used. library(dplyr) Updating To update a package in the project library, you can use the packrat::update.packages() function. Packrat will update the package and all its dependencies. packrat::update.packages(&quot;dplyr&quot;) Overall, Packrat is a valuable tool for reproducible research, as it allows you to manage package dependencies for your projects and ensure that the same results can be obtained regardless of the version of the package used. 9.4 GitHub GitHub is an online platform that provides version control and collaboration features for software development projects. It is widely used by developers to store and manage their code repositories, track changes made to code over time, and collaborate with others on projects. It is a powerful tool that simplifies the process of managing code and makes it easier for developers to work together. A key benefit of RStudio IDE is that it has built-in support for version control systems like GitHub, which makes it easy to manage and share code with others. Sharing To use GitHub within RStudio IDE, you need to first create a GitHub account and set up a repository. Once you have created a repository, you can follow these steps to use it within RStudio IDE: Open RStudio IDE and navigate to the “New Project” tab. Select “Version Control” and then “Git”. Enter the URL of your GitHub repository and choose a project directory. Click “Create Project” to create a new RStudio project that is linked to your GitHub repository. Sharing Sharing code with others using GitHub and RStudio IDE is a straightforward process. Once you have set up your GitHub repository and linked it to your RStudio project, you can follow these steps to share code with others: Make changes to your code in RStudio IDE. Commit your changes to the local Git repository using the “Commit” button in the “Git” tab. Push your changes to your GitHub repository using the “Push” button in the “Git” tab. Share the URL of your GitHub repository with others so they can access your code. 9.5 Docker Docker is an open-source platform that allows developers to easily create, deploy, and run applications in containers. Containers are lightweight, portable, and self-contained environments that can run isolated applications. Docker helps to simplify the process of software development, testing, and deployment by providing a consistent environment that runs the same way on any machine, independent of the host operating system. For more information check out the main Docker website in addition to the Rocker R Project. 9.6 R Packages In R, packages are collections of R functions, data, and compiled code that can be easily shared and reused with others. They are an essential part of the R ecosystem and are used for a variety of purposes, such as data analysis, visualization, and statistical modeling. Creating a package in R is a straightforward process, and RStudio IDE provides several tools to simplify the package development process. Packages are a way of organizing your code and data into a single, self-contained unit that can be easily shared and distributed with other R users. Creating To create a package in RStudio, follow these simple steps: Create a new R Project. Go to “File” -&gt; “New Project” -&gt; “New Directory” -&gt; “R Package” Choose a name for the package, such as my_new_rpackage and a directory location where it will be saved. Once the project is created, RStudio will generate a basic package structure with the following files: DESCRIPTION: This file contains information about the package, such as its name, version, author, and dependencies. NAMESPACE: This file defines the package’s API, i.e., the set of functions and objects that are intended for public use. R/: This directory contains the package’s R source code files. man/: This directory contains the package’s documentation files. Now it’s time to write some code. You can start by creating a simple function that outputs “Hello ASMS”. Here’s an example: #&#39; Hello ASMS Function #&#39; #&#39; This function prints &quot;Hello ASMS&quot; to the console. #&#39; #&#39; @return A character vector with the message &quot;Hello ASMS&quot;. #&#39; @export say_hello &lt;- function() { return(&quot;Hello ASMS&quot;) } Save the function in a new R script file called “hello_world.R” and place it in the package’s R/ directory. Build the package by running “Build” -&gt; “Build &amp; Reload” from the “Build” tab. This will compile the package code and create a binary package file (.tar.gz) in the “build/” directory. Finally, install the package by running “Install and Restart” from the “Build” tab. This will install the package on your local machine, making it available for use. Using Once the package is installed, you can load it into your R session using the library() function. Here’s an example: library(my_new_rpackage) say_hello() This will output “Hello ASMS” to the console. 9.7 R Shiny Applications R Shiny is an R package that allows users to create interactive web applications using R. With R Shiny, users can create and customize web-based dashboards, data visualization tools, and other interactive applications that can be easily shared with others. The benefits of using R Shiny include creating powerful data-driven web applications with ease and providing a user-friendly interface for data analysis. R Shiny is widely used in various industries, including finance, healthcare, and e-commerce. Creating Creating an R Shiny application is relatively easy, and it can be done in the RStudio IDE. Here are the steps to follow: Open RStudio and create a new R script file. Install the ‘shiny’ R package by running the following command: install.packages(\"shiny\") Load the ‘shiny’ package by running the following command: library(shiny) Create a new Shiny application by running the following command: shinyApp(ui = ui, server = server) The ‘ui’ argument should contain the user interface (UI) code for the application, while the ‘server’ argument should contain the server-side code for the application. Write the UI code and server-side code for your application, and save the file with a ‘.R’ extension. Run the application by clicking on the ‘Run App’ button in the RStudio IDE, or by running the following command: runApp(\"path/to/your/app.R\") Example Here’s an example of an R Shiny application that allows users to plot points on a graph: library(shiny) # Define UI for application ui &lt;- fluidPage( titlePanel(&quot;Plotting Points&quot;), sidebarLayout( sidebarPanel( numericInput(&quot;x&quot;, &quot;X Coordinate:&quot;, 0), numericInput(&quot;y&quot;, &quot;Y Coordinate:&quot;, 0), actionButton(&quot;plot&quot;, &quot;Plot Point&quot;) ), mainPanel( plotOutput(&quot;plot&quot;) ) ) ) # Define server logic server &lt;- function(input, output) { coords &lt;- reactiveValues(x = numeric(), y = numeric()) observeEvent(input$plot, { coords$x &lt;- c(coords$x, input$x) coords$y &lt;- c(coords$y, input$y) }) output$plot &lt;- renderPlot({ plot(coords$x, coords$y, xlim = c(0, 10), ylim = c(0, 10), pch = 19, col = &quot;blue&quot;) }) } # Run the application shinyApp(ui = ui, server = server) In this example, the UI code defines a sidebar panel with input fields for the X and Y coordinates of a point, as well as a button to plot the point. The main panel contains a plot that displays all of the points that have been plotted by the user. The server-side code defines a reactive variable called ‘coords’ which stores the X and Y coordinates of each plotted point. When the user clicks the ‘Plot Point’ button, an observer function is triggered that adds the new point to the ‘coords’ variable. The renderPlot function then plots all of the points on the graph. Check out the R Shiny web page for more information. Exercises Create a new R Studio Project and name it 007_quarto. Create a new Quarto document, add your name, a title, and date Replace the default YAML header with the one specified in this chapter and render the document. Confirm the HTML output is successfully created. Review the document and study the structure. Which parts are text blocks and which are code blocks? How are headings being used in the text blocks? Try executing and editing the R code blocks using the green right-facing triangle. What happens when you execute a code block? Insert a new R code block and add a ggplot code example from the Data Visualization section (make sure you load the required data first). What happens when you execute the code? Change the options in the YAML header under execute: to not include R code. Re-render the document to confirm. Add code block options to the ggplot code you added in 4 to change the figure with and height. "],["mass-spectrometry.html", "Chapter 10 Mass Spectrometry 10.1 Commercial 10.2 Open Access Data 10.3 R Mass 10.4 R MS Data 10.5 R MS Analysis", " Chapter 10 Mass Spectrometry Mass spectrometry experiments produce a vast amount of data that comes in various formats. Mass spectrometry data is composed of raw data (m/z peak data) from different types of proprietary instrument vendor formats, open-access formats, and data processing and interpretation workflow formats such as XML, CSV, tab delimited, and Excel. To handle such a diverse set of data, researchers can rely on multiple R packages that help access, manipulate, and process the data. These packages are designed to perform tasks such as filtering, normalization, and quality control, which are essential to produce reliable results. Furthermore, the ability to analyze and interpret mass spectrometry data is essential for many fields, including proteomics, metabolomics, and lipidomics. Researchers use mass spectrometry data to identify and quantify proteins, metabolites, and lipids in biological samples. The field of mass spectrometry generates a considerable amount of data in various formats that require processing and interpretation. With the help of a slew of R packages, researchers can efficiently manage and analyze this data to provide accurate and reliable results. At the end of this chapter you should be able to Understand commercially proprietary raw data. How to convert raw data to an open access format. Be familiar with some R packages specific to mass spectrometry. 10.1 Commercial Accessing mass spectrometry data has some barriers due to vendors proprietary file formats. In recent years, vendors have been providing software development kits (SDKs) that allow programmers to provide direct access to the raw data. This is essential when developing fundamental workflows that require such data and also can enable analysis tools to bypass sometimes time consuming file format conversions to open formats. There are some means to access such raw data through R, allowing end-to-end pipelines to be developed directly from vendor data. RAW (Thermo) rawrr This package provides access to proprietary Thermo Fisher Scientific Orbitrap instrument data as a stand-alone R package or serves as MsRawFileReaderBackend for the Bioconductor Spectra package. rawrr wraps the functionality of the RawFileReader .NET assembly. Test files are provided by the tartare ExperimentData package. PDF manual WEB tutorial GitHub Bioconductor Kockmann, Tobias, and Christian Panse. “The rawrr R package: direct access to Orbitrap data and beyond.” Journal of Proteome Research 20.4 (2021): 2028-2034. rawReadeR This package is an R/C++ API to the MSFileReader.dll. rawReadeR allows for m/z profiles and scan header data to be extracted from .RAW files without the need for manual extraction using Xcalibur or conversion to a more universal format (.mzML, .mZXML, etc…) GitHub .d (Bruker) timsTOF data TimsR Bruker has released an SDK that has now been ported to python and R. It provides a simple way to get data out of results collected with your Bruker timsTOF Pro from R. This definitely ain’t no rocket science, but is pretty useful! The data is reported in stored in data.table objects, that are the only thing R has to actually work meaningfully with big data sets. GitHub WIFF (Sciex) None known at the moment. D (Agilent) Convert from raw using vendor software. 10.2 Open Access Data Mass spectrometry (MS) is a powerful analytical method that can be used to determine the mass-to-charge ratio (m/z) of ions in a sample. Mass spectrometry data is generated from MS experiments and can be stored in various formats, including mzXML, mzML, and mzH5. 10.2.1 ProteoWizard An open-source software suite that provides a collection of open-source, cross-platform software libraries and tools for extracting raw mass spectrometry data from from various instrument vendor formats and converting it to the formats listed below. _Kessner, D., Chambers, M., Burke, R., Agus, D., &amp; Mallick, P. (2008). ProteoWizard: open source software for rapid proteomics tools development. Bioinformatics, 24(21), 2534–2536. proteowizard.sourceforge.io/ 10.2.2 mzXML mzXML is an open XML-based format for encoding MS data. It was developed by the Seattle Proteome Center and is widely used in the mass spectrometry community. mzXML files contain raw MS data, as well as metadata describing the instrument parameters used to acquire the data. mzXML files can be processed using a variety of software tools, such as the Trans-Proteomic Pipeline and ProteoWizard. The mzXML format has been shown to be effective in handling data from a wide range of instruments. It has a simple structure that makes it easy to parse and process, making it an attractive choice for many researchers. The format is also relatively lightweight, which makes it easy to transfer and store large amounts of data. Pedrioli, P.G., Eng, J.K., Hubley, R., Vogelzang, M., Deutsch, E.W., Raught, B., Pratt, B., Nilsson, E., Angeletti, R.H., Apweiler, R. and Cheung, K., 2004. A common open representation of mass spectrometry data and its application to proteomics research. Nature biotechnology, 22(11), pp.1459-1466. 10.2.3 mzML The mzML format is another open XML-based format for MS data, developed by the Proteomics Standards Initiative. It is designed to be more flexible than mzXML and includes more detailed metadata. mzML files can be processed using software tools such as OpenMS and mzR. The mzML format allows for more detailed and comprehensive data storage than mzXML. This is because mzML has a more complex structure, which enables the storage of a wider range of experimental metadata. The format is also more flexible, which means that it can be easily adapted to different types of experiments and instruments. Martens, L., Chambers, M., Sturm, M., Kessner, D., Levander, F., Shofstahl, J., Tang, W.H., Römpp, A., Neumann, S., Pizarro, A.D. and Montecchi-Palazzi, L., 2011. mzML—a community standard for mass spectrometry data. Molecular &amp; cellular proteomics, 10(1), p.R110. 000133. 10.2.4 mzMLb Recently proposed as a new file format based on HDF5 and NetCDF4 standards, mzMLb is faster and more flexible than existing approaches while preserving the XML encoding of metadata. Additionally, it is optimized for both read/write speed and storage efficiency. The format has a reference implementation provided within the ProteoWizard toolkit. Bhamber, Ranjeet S., et al. “mzMLb: A future-proof raw mass spectrometry data format based on standards-compliant mzML and optimized for speed and storage requirements.” Journal of proteome research 20.1 (2020): 172-183. 10.3 R Mass Molecular mass is the sum of the atomic masses of all the atoms in a molecule. It is an important parameter used in various fields of chemistry. The molecular mass of a molecule is usually expressed in atomic mass units (amu) or daltons (Da). Mass spectrometry based measurements require a charge and are expressed as mass-to-charge (m/z) or as Thompsons (Th). Isotopic probabilities are also important in determining molecular mass. Isotopes are atoms of the same element that have different numbers of neutrons in their nuclei. Isotopes of an element have different atomic masses. The isotopic probability of an element is the probability that a given isotope of that element will occur in nature. For example, carbon (C) has two stable isotopes, carbon-12 (12C) and carbon-13 (13C), with atomic masses of 12.000 amu and 13.003 amu, respectively. The isotopic probability of carbon-12 is 98.9%, while that of carbon-13 is 1.1%. Therefore, the average atomic mass of carbon in nature is 12.011 amu. There are several measures of a molecular mass that can be reported. The average mass is that of the weighted average of all isotopes and likely to be reported for small molecules. While the nominal mass of a molecule is defined as the sum of the integer masses of the most abundant isotopes in a molecule. The monoisotopic mass is commonly considered the as the sum of the exact masses of the lightest isotopes, and this value is considered in all peptide-based proteomics applications. 10.3.1 BRAIN Baffling Recursive Algorithm for Isotope distributioN calculations R BRAIN is an isotopic abundance calculator implemented in R programming language and is especially useful for chemists and researchers who deal with complex molecules and need to calculate their isotopic composition accurately. In addition, it has a handy function for calculating the mass directly from an amino acid sequence. Bioconductor Dittwald, Piotr, et al. “BRAIN: a universal tool for high-throughput calculations of the isotopic distribution for mass spectrometry.” Analytical chemistry 85.4 (2013): 1991-1994. Installation BiocManager::install(&quot;BRAIN&quot;) Useage library(BRAIN) # Human insulin amino acid sequence str_seq &lt;- &quot;MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGGGPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN&quot; # get a list-object of atoms lst_atm &lt;- getAtomsFromSeq(str_seq) Calculate the average mass. calculateAverageMass(lst_atm) ## [1] 11980.82 Calculate the monoisotopic mass. calculateMonoisotopicMass(lst_atm) ## [1] 11973.02 Calculate the isotopic abundances (probabilities) mass. lst_isotopes &lt;- useBRAIN(lst_atm, nrPeaks = 20) plot(lst_isotopes$masses, lst_isotopes$isoDistr, xlab=&#39;Mass&#39;, ylab=&#39;Probability&#39;, type = &#39;h&#39;) Calculate the isotopic abundances (probabilities) mass for a metabolite [C100H200S2Cl5]. lst_atm &lt;- list(C=100, H=200, S=2, Cl=5) lst_isotopes &lt;- useBRAIN(lst_atm, nrPeaks = 13) plot(x = lst_isotopes$masses, y = lst_isotopes$isoDistr / max(lst_isotopes$isoDistr) * 100, xlab=&#39;Mass&#39;, ylab=&#39;Relative abundance&#39;, type = &#39;h&#39;) 10.3.2 enviPat Isotope Pattern, Profile and Centroid Calculation for Mass Spectrometry A method for calculating theoretical isotope patterns in mass spectrometry. This method uses a treelike structure to derive sets of subisotopologues for each element in a molecule. By doing so, it allows for early pruning of low-probability isotopologues and the detection of the most probable isotopologue. The method was validated in a large-scale benchmark simulation. CRAN Loos, Martin, et al. “Accelerated isotope fine structure calculation using pruned transition trees.” Analytical chemistry 87.11 (2015): 5738-5744. Installation install.packages(&quot;enviPat&quot;) Use library(enviPat) data(&quot;isotopes&quot;) pattern &lt;- isopattern( isotopes, &quot;C100H200S2Cl5&quot;, threshold=0.1, plotit=TRUE, charge=FALSE, emass=0.00054858, algo=1 ) 10.4 R MS Data 10.4.1 RforMassSpectrometry The RforMassSpectrometry initiative is a collaborative project aimed at developing efficient, documented, and flexible R software for analyzing high throughput mass spectrometry assays. The project formalizes the long-time collaborative development efforts of its core members under the RforMassSpectrometry organization to facilitate dissemination and accessibility of their work. WEB tutorial Packages The suite of packages developed by the initiative is available on GitHub under the RforMassSpectrometry organization. This platform serves as the main development and collaboration area of the project. Once the packages have reached a satisfactory level of maturity and stability, they are submitted to the Bioconductor project to integrate with existing and broader infrastructure for high throughput biology data. The RforMassSpectrometry initiative focuses on co-developing and maintaining interoperable software to tackle a wide range of needs in computational mass spectrometry using the R language and environment. The goal is not to address all possible needs, but rather to provide a set of software that work together efficiently, rather than maximizing the number of packages. Packages MsExperiment The MsExperiment package provides the infrastructure to store and manage all aspects related to a complete proteomics or metabolomics mass spectrometry experiment. It relies on the other RforMassSpectrometry core packages for the data crunching. Spectra The Spectra package provides base classes and processing methods for raw mass spectrometry data. It is designed with efficiency, both in terms of memory footprint and processing time in mind, and can manage data in different types of formats. QFeatures The QFeatures package offers the infrastructure to manage and process quantitative features for high-throughput mass spectrometry assays, including proteomics and metabolomics experiments. PSMatch The PSMatch package allows to read, process and analyse peptide-spectrum matches. Chromatograms The Chromatograms package provides base classes and processing methods for chromatographic data. It is designed with efficiency, both in terms of memory footprint and processing time in mind, and can manage data in different types of formats. MsCoreUtils The MsCoreUtils package defines low-level functions for mass spectrometry data processing and is independent of any high-level data structures. MetaboCoreUtils The MetaboCoreUtils package defines low-level functions for common operations in metabolomics and is independent of any high-level data structures. 10.4.2 MSnbase Often, the full access to raw data provides a complete sense of control. However, there are times when attempting to achieve mundane tasks can be extremely tedious and repetitive. This is where the concept of abstraction comes in. It is unnecessary to be aware of every detail that is exposed by mzR to work with and manipulate raw data. This is where MSnbase and the MSnExp data structure come in. These tools provide a smoother and more efficient approach to handle and annotate raw data. While MSnbase only provides rudimentary access to the data, there are many other algorithms that add additional functionality such as xcms, which provides powerful visualization tools for mass spectrometry data. WEB tutorial PDF manual Bioconductor Gatto, Laurent, and Kathryn S. Lilley. “MSnbase-an R/Bioconductor package for isobaric tagged mass spectrometry data visualization, processing and quantitation.” Bioinformatics 28.2 (2012): 288-289. Note dependencies for MSnbase are not available for Mac M1 cpus. 10.4.3 mzR The mzR package is a powerful tool for analyzing large raw data files in proteomics research. It offers a direct interface to the proteowizard code base, which is a widely-used software framework for mass spectrometry data analysis. By leveraging a substantial proportion of pwiz’s C/C++ code, mzR is able to provide fast and efficient parsing of these complex data files. WEB tutorial PDF manual Bioconductor Chambers, Matthew C., et al. “A cross-platform toolkit for mass spectrometry and proteomics.” Nature biotechnology 30.10 (2012): 918-920. Read in an 10.2.3 mzML converted LCMS data file and check its contents. # download the file url &lt;- &quot;https://raw.githubusercontent.com/jeffsocal/ASMS_R_Basics/main/data/small.mzML&quot; download.file(url, destfile = &quot;./data/small.mzML&quot;) library(mzR) ms_dat &lt;- openMSfile(&quot;./data/small.mzML&quot;) ms_dat ## Mass Spectrometry file handle. ## Filename: small.mzML ## Number of scans: 48 Extract out a specific spectrum. library(tidyverse) tbl_spec &lt;- peaks(ms_dat, 36) %&gt;% as_tibble() tbl_spec ## # A tibble: 19,800 × 2 ## mz intensity ## &lt;dbl&gt; &lt;dbl&gt; ## 1 200. 0 ## 2 200. 0 ## 3 200. 0 ## 4 200. 0 ## 5 200. 0 ## 6 201. 0 ## 7 201. 0 ## 8 201. 0 ## 9 201. 0 ## 10 201. 0 ## # ℹ 19,790 more rows tbl_spec %&gt;% ggplot(aes(mz, intensity)) + geom_segment(aes(xend = mz, yend = 0)) In addition to its parsing capabilities, mzR includes a range of useful functions for working with mass spectrometry data. This package supports a wide variety of file formats, and provides functions for reading, writing, and manipulating data in these formats. Furthermore, mzR includes features for working with tandem mass spectrometry (MS/MS) data, including functions for spectral processing and peak picking. 10.4.4 xcms XCMS is an R package that provides a collection of functions for processing and analyzing mass spectrometry (MS) data. This package is designed to extract meaningful information from large data sets generated by MS-based experiments, such as metabolomics and proteomics studies. The web tutorials offer an excellent starting point. Note dependencies for xcms are not available for Mac M1 cpus. WEB tutorial PDF manual Bioconductor Smith, Colin A., et al. “XCMS: processing mass spectrometry data for metabolite profiling using nonlinear peak alignment, matching, and identification.” Analytical chemistry 78.3 (2006): 779-787. Benton, H. Paul, et al. “XCMS2: processing tandem mass spectrometry data for metabolite identification and structural characterization.” Analytical chemistry 80.16 (2008): 6382-6389. XCMS offers a wide range of functionalities that are necessary for preprocessing and analyzing raw MS data. Some of the key features of XCMS include: Data preprocessing: The package offers a variety of preprocessing functions, such as peak detection, retention time correction, and noise filtering. Feature detection: XCMS provides functions for detecting and aligning features across multiple samples, which is essential for comparative analysis. Statistical analysis: The package also includes functions for statistical analysis, such as principal component analysis (PCA), hierarchical clustering, and differential expression analysis. Visualization: XCMS provides several visualization tools for exploring and interpreting MS data, including heatmaps, scatter plots, and volcano plots. Read in an 10.2.3 mzML converted LCMS data file and check its contents. library(xcms) ms_dat &lt;- readMSData(&quot;~/lcms_data/example_001.mzML&quot;) Use a peak-detection algorithm to find chromatographic peaks for each precursor cwp &lt;- CentWaveParam(snthresh = 5, noise = 100, ppm = 10, peakwidth = c(3, 30)) ms_dat &lt;- findChromPeaks(ms_dat, param = cwp) One of the main advantages of XCMS is its flexibility and scalability. This package is highly customizable, allowing users to tailor the analysis pipeline to their specific needs. In addition, XCMS is designed to handle large data sets, making it suitable for high-throughput experiments. Currently, spectra data representation, handling and processing is being re-implemented as part of the 10.4.1 RforMassSpectrometry initiative aiming at increasing the performance of methods and simplifying their use. 10.5 R MS Analysis Finally, there are numerous R packages that implement methods, common and advanced, in statistical post analysis of quantitative data. Explored here are only a few of the available packages. 10.5.1 MSstats The MSstats package is an R package designed for the analysis of label-free mass spectrometry data. It provides a wide range of statistical tools for the analysis of protein abundance data, including normalization, missing value imputation, quality control, and differential expression analysis. MSstats provides a powerful and flexible way to analyze mass spectrometry data, making it an essential tool for researchers in the field. WEB tutorial PDF manual Bioconductor Choi, Meena, et al. “MSstats: an R package for statistical analysis of quantitative mass spectrometry-based proteomic experiments.” Bioinformatics 30.17 (2014): 2524-2526. 10.5.2 Tidyproteomics The tidyproteomics R package is a tool that provides a set of functions to preprocess and analyze proteomics data using the tidy data framework. This package is built on top of the tidyverse and Bioconductor packages, which are widely used in the R community for data manipulation and analysis. WEB manual GitHub WEB App Some of the main features of the tidyproteomics package include: Data preprocessing functions for common tasks such as filtering, normalization, and imputation. Functions for quality assessment and visualization of proteomics data. Integration with other Bioconductor packages for downstream analysis such as differential expression analysis and pathway analysis. Figure 10.1: tidyproteomics workflow Installation To install the tidyproteomics package, you will need to install GitHub and Bioconductor repositories: install.packages(&quot;devtools&quot;) devtools::install_github(&quot;jeffsocal/tidyproteomics&quot;) install.packages(&quot;BiocManager&quot;) BiocManager::install(c(&quot;limma&quot;,&quot;qvalue&quot;,&quot;fgsea&quot;,&quot;Biostrings&quot;)) Loading Data To load your data into tidyproteomics, you can use the following code: # Load the tidyproteomics package library(tidyproteomics) # Import data data_proteins &lt;- &quot;path_to_data.xlsx&quot; %&gt;% import(&quot;ProteomeDiscoverer&quot;, &quot;proteins&quot;) Data Summaries Currently, tidyproteomics implements two summary quantitative visualizations. The first is a simple grouped bar chart that displays individual and grouped proteins, as well as all and unique peptides. The match-between-runs is shown as a margin above the MS2 evidenced identifications. In recent literature, a summary of protein quantitation has been visualized as a rank-based dot plot. This plot can be extended to highlight statistical differences via an unbiased all-pair-wise comparison, which gives an anticipated view on how to guide downstream analyses. p01 &lt;- data_proteins %&gt;% plot_counts() p02 &lt;- data_proteins %&gt;% plot_quantrank() Summary Stats {-} Summarizing proteomics data is vital to understanding the bigger picture and conveying summary stats that set the tone for the larger analysis. The results of each summary can be directed to via the destination option to “print” on screen, “save” to a file or “return” as a tibble. data_proteins &lt;- data_proteins %&gt;% # save a table of simple summary stats summary(&quot;sample&quot;, destination = &quot;save&quot;) %&gt;% # save a report on contamination summary(contamination = &quot;CRAP&quot;, destination = &quot;save&quot;) %&gt;% # remove contamination subset(!description %like% &quot;^CRAP&quot;) Normalization and Imputation Quantitative proteomics requires accurate normalization, which can be difficult to implement. The normalize() function in the tidyproteomics package is a wrapper for various normalization methods, while select_normalization() automatically selects the best method based on a weighted score. Both functions allow for downstream analyses such as expression() and enrichment(). The package attempts to apply each function universally to peptide and protein values using the identifier variable to identify the thing being measured. data_proteins &lt;- data_proteins %&gt;% # normalize via several methods, best method will be automatically selected normalize(.method = c(&quot;median&quot;,&quot;linear&quot;,&quot;limma&quot;,&quot;randomforest&quot;)) %&gt;% # impute with a minimum value (this is a knock-out) impute(base::min) # plot visualizations comparing normalization methods p03 &lt;- data_proteins %&gt;% plot_normalization() p04 &lt;- data_proteins %&gt;% plot_variation_cv() p06 &lt;- data_proteins %&gt;% plot_dynamic_range() # plot visualizations of unbiased clustering p07 &lt;- data_proteins %&gt;% plot_heatmap() p08 &lt;- data_proteins %&gt;% plot_pca() Expression Analysis data_proteins &lt;- data_proteins %&gt;% # calculate the expression between experiment: ko and control: wt expression(kndw/ctrl) %&gt;% # plot the expression analysis plot_volcano(kndw/ctrl, destination = &quot;png&quot;, significance_column = &quot;p_value&quot;) %&gt;% plot_proportion(kndw/ctrl, destination = &quot;png&quot;) Overall, the tidyproteomics package provides a useful set of tools for preprocessing and analyzing proteomics data using the tidy data framework in R. There are several more workable examples in the online documentation. 10.5.3 AssayR Wills, Jimi, Joy Edwards-Hicks, and Andrew J. Finch. “AssayR: a simple mass spectrometry software tool for targeted metabolic and stable isotope tracer analyses.” Analytical chemistry 89.18 (2017): 9616-9619. WEB tutorial WEB manual AssayR is an R package that tailors peak detection for each metabolite in high resolution wide-scan liquid chromatography-mass spectrometry data sets. It integrates peak areas for all isotopologues and outputs extracted ion chromatograms, stacked bar charts, and a .csv data file. AssayR provides easy and robust targeted metabolite and stable isotope analyses on wide-scan data sets from high resolution mass spectrometers, and is shown to provide more accurate and robust quantitation than XCMS. 10.5.4 DEqMS Zhu, Yafeng, et al. “DEqMS: a method for accurate variance estimation in differential protein expression analysis.” Molecular &amp; Cellular Proteomics 19.6 (2020): 1047-1057. PDF manual Bioconductor DEqMS is built on top of Limma, which assumes the same prior variance for all genes. However, protein abundance estimates in proteomics depend on the number of peptides/PSMs quantified, with proteins quantified by multiple peptides or PSMs being more accurately measured. To achieve better accuracy, the DEqMS package estimates different prior variances for proteins quantified by different numbers of PSMs/peptides. This package is suitable for analyzing both label-free and labelled proteomics data. 10.5.5 MS-EmpiRe Ammar, Constantin, et al. “MS-EmpiRe Utilizes Peptide-level Noise Distributions for Ultra-sensitive Detection of Differentially Expressed Proteins [S].” Molecular &amp; Cellular Proteomics 18.9 (2019): 1880-1892. GitHub MS-EmpiRe is a new method for mass spectrometry based proteomics that explicitly accounts for the noise underlying peptide fold changes. It derives data set-specific, intensity-dependent empirical error fold change distributions, which are used for individual weighing of peptide fold changes to detect differentially expressed proteins (DEPs). MS-EmpiRe doubles the number of correctly identified DEPs compared to state-of-the-art tools and can be applied to any common quantitative proteomics setup. 10.5.6 msgrob2 Goeminne L, Gevaert K, Clement L (2016). “Peptide-level Robust Ridge Regression Improves Estimation, Sensitivity, and Specificity in Data-dependent Quantitative Label-free Shotgun Proteomics.” Molecular &amp; Cellular Proteomics, 15(2), 657-668. PDF manual Bioconductor msgrob2 is an R package that provides functions to perform robust estimation in linear models with missing data. With the help of the Expectation-Maximization (EM) algorithm, the package estimates the parameters of the linear model and imputes the missing data. Additionally, the package offers robust methods for estimating the covariance matrix, including the Minimum Covariance Determinant (MCD) estimator and the S-estimator. The msgrob2 package is particularly useful in situations where data is missing from a linear model. The EM algorithm implemented in the package is a powerful tool for imputing missing data, and the robust covariance estimators allow for a better understanding of the data. The package is designed to provide efficient and accurate results when working with incomplete data, making it an essential tool for researchers and data analysts. 10.5.7 StatsPro Yang, Yin, et al. “StatsPro: Systematic integration and evaluation of statistical approaches for detecting differential expression in label-free quantitative proteomics.” Journal of Proteomics 250 (2022): 104386. WEB tutorial PDF manual GitHub Quantitative label-free mass spectrometry is a powerful technology for profiling proteins, but choosing an appropriate statistical procedure for detecting differentially expressed proteins remains a challenge. This study presents 12 common testing algorithms and 6 P-value combination methods, along with a user-friendly web tool called StatsPro to help proteomics scientists investigate their influence on Differentially Expressed Protein detection. The authors utilize two case studies demonstrate the tool’s practicability. "],["putting-it-all-together.html", "Chapter 11 Putting It All Together 11.1 Test Scenario 1 11.2 Test Scenario 2 11.3 Some Pro Coding Tricks", " Chapter 11 Putting It All Together Over the past few days, you have embarked on an enlightening journey into the world of R programming, a language renowned for its prowess in data analysis and visualization. This expedition has been as rewarding as it has been challenging, with each new concept building upon the last, culminating in a cohesive and comprehensive understanding of how to wield R effectively. The initial steps involved mastering the basics of R. Understanding the syntax, the various data structures, and the fundamental functions were the foundation stones. These basics, though seemingly simple, are crucial as they form the bedrock upon which more complex operations are built. Every line of code, every function call, and every variable assignment reinforced the importance of a solid grasp of these foundational elements. As you progressed, the focus shifted to data wrangling, a critical aspect of data analysis. Here, you learned how to manipulate data frames, clean datasets, and transform data into a usable format. Functions like dplyr and tidyr became invaluable allies, enabling you to filter, select, and arrange data with precision and efficiency. Data wrangling is akin to preparing a canvas before painting; it ensures that the data is in its optimal form, ready to reveal its insights. With clean, well-structured data in hand, you moved on to the art of creating succinct graphs and plots. Visualization is a powerful tool in data analysis, transforming raw numbers into visual stories that are easy to comprehend. Using libraries like ggplot2, you explored various ways to represent data visually, from simple bar charts and histograms to more complex scatter plots and line graphs. Each plot was an exercise in clarity, aiming to convey information as effectively as possible. All these skills coalesced in the organized notebook, a repository of knowledge and analysis. This notebook is not just a collection of code snippets and plots; it is a narrative that answers complex questions methodically. Each section flows logically into the next, ensuring that the analysis is easy to follow and reproduce. The notebook stands as a testament to the structured approach you have taken—beginning with data importation, followed by cleaning, analysis, visualization, and finally, interpretation of results. This process of learning and application has underscored the importance of organization and clarity. By putting everything you have learned together, you can now approach complex questions with confidence, armed with the ability to analyze data thoroughly and present your findings in a manner that is both understandable and reproducible. The journey through R has not just been about acquiring a new skill; it has been about learning how to think critically and systematically about data, ensuring that every analysis is as insightful as it is rigorous. Nothing to learn here, just practice Download the data. Approach the scientific question. Document your analysis and results in a notebook. 11.1 Test Scenario 1 Proteomics Optimization The LCMS Proteomics lab manager is trying to determine the optimal conditions for yield and reproducibility. The Data Set url &lt;- &quot;https://raw.githubusercontent.com/jeffsocal/ASMS_R_Basics/main/data/HelaDilution_Skyline-MSstats_peptides.csv.zip&quot; download.file(url, destfile = &quot;./data/HelaDilution_Skyline-MSstats_peptides.csv.zip&quot;) Other Observations The Proteomics Bioinformatician suggests some dilutions may yield different identifications. not all identifications are real, ie, some are MBR (Match Between Run). Plan Your Approach Create a new notebook. Examine the data file. Tidy if necessary. Provide some basic summary accounting of the data. Get into the analysis. 11.2 Test Scenario 2 Metabolomics Biomarker The Company has measured the quantitative abundance of several metabolites between healthy patients and patients with breast cancer. Your task is to determine if there are any potential biomarkers. The Data Set url &lt;- &quot;https://www.metabolomicsworkbench.org/studydownload/ST000918_AN001504_Results.txt&quot; download.file(url, destfile = &quot;./data/ST000918_AN001504_Results.txt&quot;) Other Observations The data is in a matrix format and has been post processed to normalize and impute missing values. You will likely need to adjust for multiple hypothesis testing. Plan Your Approach Create a new notebook. Examine the data file. Tidy if necessary. Provide some basic summary accounting of the data. Get into the analysis. 11.3 Some Pro Coding Tricks Tidy Columns One way to avoid the backtick in referencing columns is to rename them with underscores for spaces. tbl &lt;- tbl |&gt; # rename all the columns with a functing using the &#39;dplyr&#39; and &#39;stringr&#39; packages rename_all(function(x){tolower(x) |&gt; str_replace_all(&quot;\\\\s+&quot;, &quot;_&quot;)}) Nested Summary Stats One way to compute a summary statistic inside a table is with nesting. tbl &lt;- tbl |&gt; # group by the column(s) that you want to summarize across group_by(column) |&gt; # &quot;nest&quot; the remaining columns as a nested table, using the &#39;tidyr&#39; package nest() |&gt; mutate( # compute the summary stats using map from the `purrr` package stats = data |&gt; map(some_function), # extract the summary stat values using the &#39;broom&#39; package stats = stats |&gt; map(tidy)) |&gt; # unnest and ungroup unnest(stats) |&gt; ungroup() "],["answers-to-exercises.html", "Answers to Exercises 4 R Syntax 5 R Objects 5 R Objects MORE 6 Tidyverse 7 Data Wrangling 8 Data Visualization", " Answers to Exercises 4 R Syntax # 3. Calculate the sum of 2 and 3. 2 + 3 # 4. Evaluate if 0.5 is equal to 1 divided by 2. 0.5 == 1 / 2 # 5. Define a variable that is is 98.6 degrees in Fahrenheit. fahrenheit_temp &lt;- 101 # 6. Construct an if-else statement to determine if the temperature indicates a fever (temperature greater than or equal to 100). If it does print &quot;The temperature indicates a fever.&quot; If the temperature is less than 100, print &quot;The temperature does not indicate a fever.&quot; (hint: to print &quot;ok&quot;, the function is print(&quot;ok&quot;)) if (fahrenheit_temp &gt;= 100){ print(&quot;The temperature indicates a fever.&quot;) }else{ print(&quot;The temperature does not indicate a fever.&quot;) } # 8. Create a function to test if a temperature is a fever called `fever_checker` the prints above, but can be reused fever_checker &lt;- function(fahrenheit_temp) { if (fahrenheit_temp &gt;= 100){ print(&quot;The temperature indicates a fever.&quot;) }else{ print(&quot;The temperature does not indicate a fever.&quot;) } } # 9. Use similar logic to print if a temperature is the homeostatic range for human beings (97.7–99.5). homeostatic_range_check &lt;- function(fahrenheit_temp){ if (fahrenheit_temp &gt;= 97.7 &amp;&amp; fahrenheit_temp &lt;= 99.5){ print(&quot;In homeostatic range!&quot;) } else{ print(&quot;Outside homeostatic range!&quot;) } } # 10. Advanced exercise... add TRUE / FALSE returns (i.e. return TRUE, return FALSE) to the functions and create a function that combines both called `temperature_check`. That gives us info on the what our temperature means (i.e., is it homeostatic, is it a fever, are we possibly dead (temperature way too low??) # ... 5 R Objects # John Doe # 2023-06-02 # Institution Inc. # # Data Object Exercises # 1. Construct the following vector and store as a variable. str_gbu &lt;- c(&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;) # 2. Extract the 2nd element in the variable. str_gbu[2] # 3. Construct a numerical vector of length 5, containing the AREA of circles # with integer RADIUS 1 to 5. Remember PEMDAS. area &lt;- (1:5) ^ 2 * pi # 4. Extract all AREA greater than 50. area[which(area &gt; 50)] # 5. Create a data.frame consisting of circles with integer RADIUS 1 to 5, and their AREA. radius &lt;- 1:5 df &lt;- data.frame( radius = radius, area = (radius) ^ 2 * pi ) df # 6. Extract all AREA greater than 50 from the data.frame. w &lt;- which(df$area &gt; 50) df[w,] 5 R Objects MORE # John Doe # 2023-06-02 # Institution Inc. # # More Data Object Exercises # Exercise #1 -- Working with Variables You are running an LC-MS experiment # using a 60 min LC gradient # 1.1 Create a variable called gradient_min to hold the length of the gradient # in minutes. gradient_min &lt;- 60 # 1.2 Using the gradient length variable you just created, convert it to seconds # and assign it to a new variable with a meaningful name. gradient_sec &lt;- gradient_min * 60 # Exercise #2 -- Working with Vectors # Continuing from Exercise #1... # 2.1 Imagine you conducted additional experiments, one with a 15 minute gradient # and one with a 30 min gradient. Create a vector to hold all three gradient # times in minutes, and assign it to a new variable. gradients_min &lt;- c(15, 30, 60) # 2.2 Convert the vector of gradient times to seconds. How does this conversion # compare to how you did the conversion in Exercise 1? gradients_sec &lt;- gradients_min * 60 # Exercise #3 -- More Practice with Vectors # 3.1 The following vector represents precursor m/z values for detected features # from your experiment: prec_mz &lt;- c(968.4759, 812.1599, 887.9829, 338.5294, 510.2720, 775.3455, 409.2369, 944.0385, 584.7687, 1041.9523) # - How many values are there? length(prec_mz) # - What is the minimum value? The maximum? min(prec_mz) max(prec_mz) # Exercise #4 -- Vectors and Conditional Expressions # 4.1 Using the above vector of precursor values, write a conditional expression # to find the values with m/z \\&lt; 600. What is returned by this expression? A # single value or multiple values? A number or something else? prec_mz &lt; 600 # 4.2 Use this conditional expression to get the precursor values with m/z \\&lt; 600 prec_mz[prec_mz &lt; 600] # 4.3 Consider a new vector of data that contains the charge states of the same # detected features from above: prec_z &lt;- c(2, 4, 2, 3, 2, 2, 2, 2, 2, 2) # - Write a conditional expression to find which detected features that have # a charge state of 2. prec_z == 2 # 4.4 Write an expression to get the precursor m/z values for features having # charge states of 2? prec_mz[prec_z == 2] 6 Tidyverse Read in the bacterial-metabolites_dose-simicillin_tidy.csv data set. url &lt;- &quot;https://raw.githubusercontent.com/jeffsocal/ASMS_R_Basics/main/data/bacterial-metabolites_dose-simicillin_tidy.csv&quot; download.file(url, destfile = &quot;./data/bacterial-metabolites_dose-simicillin_tidy.csv&quot;) library(tidyverse) dat &lt;- read_csv(&quot;data/bacterial-metabolites_dose-simicillin_tidy.csv&quot;) ## Rows: 180 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Organism, Metabolite ## dbl (3): Dose_mg, Time_min, Abundance ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. How many organisms, metabolites, dose levels, and time points are in the data? How many rows are in the data table? What is the overall study design? orgs &lt;- unique(dat$Organism) n_orgs &lt;- length(unique(orgs)) metabs &lt;- unique(dat$Metabolite) n_metabs &lt;- length(unique(metabs)) doses &lt;- unique(dat$Dose_mg) n_doses &lt;- length(unique(doses)) time_pts &lt;- unique(dat$Time_min) n_time_pts &lt;- length(unique(time_pts)) nrow(dat) ## [1] 180 nrow(dat) == n_orgs * n_metabs * n_doses * n_time_pts ## [1] TRUE Which metabolite has the highest overall mean abundance? dat %&gt;% group_by(Metabolite) %&gt;% summarize(mean_abundance = mean(Abundance)) %&gt;% arrange(desc(mean_abundance)) ## # A tibble: 4 × 2 ## Metabolite mean_abundance ## &lt;chr&gt; &lt;dbl&gt; ## 1 phosphatidylcholine 50221415. ## 2 succinic acid 1855206. ## 3 glutamate 1817320. ## 4 lysine 1349963. Does this metabolite have the highest mean abundance for each organism, or is there differences between organisms? dat %&gt;% group_by(Organism, Metabolite) %&gt;% summarize(mean_abundance = mean(Abundance)) %&gt;% arrange(Organism, desc(mean_abundance)) ## `summarise()` has grouped output by &#39;Organism&#39;. You can override using the `.groups` ## argument. ## # A tibble: 12 × 3 ## # Groups: Organism [3] ## Organism Metabolite mean_abundance ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 e coli phosphatidylcholine 67749754. ## 2 e coli glutamate 2323045. ## 3 e coli succinic acid 1906850. ## 4 e coli lysine 1519827. ## 5 p aeruginosa phosphatidylcholine 77078132. ## 6 p aeruginosa succinic acid 3525432. ## 7 p aeruginosa glutamate 2902178 ## 8 p aeruginosa lysine 2336119. ## 9 staph aureus phosphatidylcholine 5836359. ## 10 staph aureus glutamate 226736. ## 11 staph aureus lysine 193944. ## 12 staph aureus succinic acid 133337. Is there an overall trend of mean abundance values vs. time point? What about abundance vs. dose? dat %&gt;% group_by(Time_min) %&gt;% summarize(mean_abundance = mean(Abundance)) ## # A tibble: 5 × 2 ## Time_min mean_abundance ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 26639946. ## 2 10 11726915. ## 3 20 11929517. ## 4 50 9389061. ## 5 120 9369441. dat %&gt;% group_by(Dose_mg) %&gt;% summarize(mean_abundance = mean(Abundance)) ## # A tibble: 3 × 2 ## Dose_mg mean_abundance ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 24454975. ## 2 10 9900030. ## 3 20 7077924. Using the example code at the beginning of this Chapter (using the Dever climate data), compute a linear fit of log10 abundance vs. time point for each metabolite and plot the results. dat &lt;- dat %&gt;% mutate(log10_abundance = log10(Abundance)) lm_func &lt;- function(data) { lm(log10_abundance ~ Time_min, data = data) } dat_lm &lt;- dat %&gt;% # dplyr group_by(Metabolite) %&gt;% # tidyr nest() %&gt;% # dplyr, purrr: apply the function to each nested data frame mutate(model = map(data, lm_func)) %&gt;% # dplyr, broom, purrr: extract the coefficients from each model mutate(tidy = map(model, broom::tidy)) %&gt;% # tidyr unnest(tidy) %&gt;% ungroup() %&gt;% # dplyr, stringr: clean-up the terms mutate(term = term %&gt;% str_replace_all(&quot;\\\\(|\\\\)&quot;, &quot;&quot;)) %&gt;% # dplyr: retain only specific columns select(Metabolite, term, estimate) %&gt;% # tidyr: convert from a long table to a wide table pivot_wider(names_from = &#39;term&#39;, values_from = &#39;estimate&#39;) %&gt;% # dplyr: create a new column with the model slope (better name), just copy Time_min mutate(model_slope = Time_min) #ggplot2 ggplot(dat, aes(Time_min, log10_abundance)) + # represent the data as points geom_point() + # use the linear model data to plot regression lines geom_abline(data = dat_lm, aes(slope = model_slope, intercept = Intercept)) + # plot each year separately facet_wrap(~Metabolite) 7 Data Wrangling # John Doe # 2023-06-02 # Institution Inc. # # Data Wrangling Exercises # 1. Download the data. url &lt;- &quot;https://raw.githubusercontent.com/jeffsocal/ASMS_R_Basics/main/data/bacterial-Metabolites_dose-simicillin_messy.xlsx&quot; download.file(url, destfile = &quot;./data/bacterial-Metabolites_dose-simicillin_messy.xlsx&quot;) # 2. Read in the messy bacteria data and store it as a variable. library(tidyverse) library(readxl) tbl_bac &lt;- &quot;data/bacterial-Metabolites_dose-simicillin_messy.xlsx&quot; %&gt;% read_excel(col_names = TRUE) # *In all proceeding exercises, pipe results from previous exercise into current # exercise creating a single lone pipe for data processing* # 3. Separate `Culture` column containing culture and dose into `culture` and # `dose_mg_ml` columns. tbl_bac %&gt;% separate(Culture, c(&quot;culture&quot;, &quot;dose_mg_ml&quot;), sep = &quot; dose--&quot;) # 4. Make `dose_mg_ml` column numeric by removing the text and change the column # data type from character to numeric. tbl_bac %&gt;% separate(Culture, c(&quot;culture&quot;, &quot;dose_mg_ml&quot;), sep = &quot; dose--&quot;) %&gt;% mutate(dose_mg_ml = gsub(&quot;-mg/ml&quot;,&quot;&quot;, dose_mg_ml)) %&gt;% mutate(dose_mg_ml = as.numeric(dose_mg_ml)) # 5. Pivot the table from wide to long creating `metabolite`, `time_hr` &amp; `abundance` columns. tbl_bac %&gt;% separate(Culture, c(&quot;culture&quot;, &quot;dose_mg_ml&quot;), sep = &quot; dose--&quot;) %&gt;% mutate(dose_mg_ml = gsub(&quot;-mg/ml&quot;,&quot;&quot;, dose_mg_ml)) %&gt;% mutate(dose_mg_ml = as.numeric(dose_mg_ml)) %&gt;% pivot_longer(cols = 4:13, names_to = &quot;metabolite_time&quot;, values_to = &quot;abundance&quot;) %&gt;% separate(metabolite_time, c(&quot;metabolite&quot;,&quot;time_hr&quot;), sep=&quot;_runtime_&quot;) # 6. Make sure `time_hr` contains just hours and not a mixture of days and hours. tbl_bac %&gt;% separate(Culture, c(&quot;culture&quot;, &quot;dose_mg_ml&quot;), sep = &quot; dose--&quot;) %&gt;% mutate(dose_mg_ml = gsub(&quot;-mg/ml&quot;,&quot;&quot;, dose_mg_ml)) %&gt;% mutate(dose_mg_ml = as.numeric(dose_mg_ml)) %&gt;% pivot_longer(cols = 4:13, names_to = &quot;metabolite_time&quot;, values_to = &quot;abundance&quot;) %&gt;% separate(metabolite_time, c(&quot;metabolite&quot;,&quot;time_hr&quot;), sep=&quot;_runtime_&quot;) %&gt;% mutate( time_hr = case_when( grepl(&quot;hr&quot;, time_hr, ignore.case = TRUE) ~ as.numeric(gsub(&quot;hr&quot;, &quot;&quot;, time_hr)), grepl(&quot;day&quot;, time_hr, ignore.case = TRUE) ~ as.numeric(gsub(&quot;day&quot;, &quot;&quot;, time_hr)) * 24 ) ) # 7. Remove the `User` column. See the cheat-sheet here: # https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf tbl_bac %&gt;% separate(Culture, c(&quot;culture&quot;, &quot;dose_mg_ml&quot;), sep = &quot; dose--&quot;) %&gt;% mutate(dose_mg_ml = gsub(&quot;-mg/ml&quot;,&quot;&quot;, dose_mg_ml)) %&gt;% mutate(dose_mg_ml = as.numeric(dose_mg_ml)) %&gt;% pivot_longer(cols = 4:13, names_to = &quot;metabolite_time&quot;, values_to = &quot;abundance&quot;) %&gt;% separate(metabolite_time, c(&quot;metabolite&quot;,&quot;time_hr&quot;), sep=&quot;_runtime_&quot;) %&gt;% mutate( time_hr = case_when( grepl(&quot;hr&quot;, time_hr, ignore.case = TRUE) ~ as.numeric(gsub(&quot;hr&quot;, &quot;&quot;, time_hr)), grepl(&quot;day&quot;, time_hr, ignore.case = TRUE) ~ as.numeric(gsub(&quot;day&quot;, &quot;&quot;, time_hr)) * 24 ) ) %&gt;% select(-User) 8 Data Visualization # John Doe # 2023-06-02 # Institution Inc. # # Data Visualization Exercises # 1. If not already done, download *Bacterial Metabolite Data (tidy)* to use as # an example data file. url &lt;- &quot;https://raw.githubusercontent.com/jeffsocal/ASMS_R_Basics/main/data/bacterial-Metabolites_dose-simicillin_tidy.csv&quot; download.file(url, destfile = &quot;./data/bacterial-Metabolites_dose-simicillin_tidy.csv&quot;) # 2. Read in the dataset .csv using the `tidyverse` set of packages. library(tidyverse) tbl_bac &lt;- &quot;./data/bacterial-Metabolites_dose-simicillin_tidy.csv&quot; %&gt;% read_csv() # 3. Create a Metabolite `Abundance` by `Time_min` ... tbl_bac %&gt;% ggplot(aes(Time_min, Abundance)) + geom_point() # 4. ... facet by `Organism` and `Metabolite`... tbl_bac %&gt;% ggplot(aes(Time_min, Abundance)) + geom_point() + facet_grid(Metabolite ~ Organism) # 4. ... adjust the y-axis to log10, color by `Dose_mg`, and add a 50% transparent line ... tbl_bac %&gt;% mutate(Dose_mg = Dose_mg %&gt;% as.factor()) %&gt;% ggplot(aes(Time_min, Abundance)) + geom_point(aes(color = Dose_mg)) + geom_line(aes(color = Dose_mg), alpha = .5) + facet_grid(Metabolite ~ Organism) + scale_y_log10() # 5. ... change the theme to something publishable, add a title, modify the x- # and y-axis label, modify the legend title, adjust the y-axis ticks to show # the actually measured time values, and pick a color scheme that highlights # the dose value... tbl_bac %&gt;% mutate(Dose_mg = Dose_mg %&gt;% as.factor()) %&gt;% ggplot(aes(Time_min, Abundance)) + geom_point(aes(color = Dose_mg)) + geom_line(aes(color = Dose_mg), alpha = .5) + facet_grid(Metabolite ~ Organism) + scale_color_manual(values = c(&quot;grey&quot;, &quot;orange&quot;, &quot;red&quot;)) + scale_y_log10() + scale_x_continuous(breaks = unique(tbl_bac$Time_min)) + labs(title = &#39;Bacterial Metabolite monitoring by LCMS in response to antibiotic&#39;, subtitle = &#39;Conditions: metered dose of similicillin&#39;, x = &quot;Time (min)&quot;, y = &quot;LCMS Abundance&quot;, color = &quot;Dose (mg)&quot;) + theme_classic() "]]
